{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mandelbrot Set and the roofline model\nIn the last tutorial we got a brief overview of the architecture of a GPU. In this tutorial We will elborate a bit more on Grid configuration and compute the mandelbrot set on the GPU.\n\n\n## Grid configuration\n\nWhen we launch a CUDA kernel we use the syntax `@cuda blocks=x threads=y f(a, b, c...)` where `f` is the function and `a, b, c...` are its arguments.\n\nThe `blocks` and `threads` options to `@cuda` are used to specify the **grid** configuration of the kernel which is being launched. \n\n`blocks` specifies the dimension and the size of the grid of blocks. This can be one, two or three dimensional. The reason for having multiple dimensions is to make it easier to express algorithms which index over two or three dimensional spaces. \n\n`threads` specifies the *cooperative thread array* (CTA). Threads in the same CTA have access to better coordination and communication utilities. CTA's can two or three dimensional for convenient indexing.\n\n\n// TODO: Diagrams\n\nThere are restrictions related to the grid given below.\n* Maximum x-dimension of a grid of thread blocks : $2^{31} - 1$\n* Maximum y-, or z-dimension of a grid of thread blocks : $65535 (2^{16} - 1)$\n* Maximum x- or y-dimension of a CTA: 1024\n* Maximum z-dimension of a CTA: 64\n* Maximum number of threads per block: 1024 \n\nNow let's go back to our SAXPY example and verify the flexibility in choosing grid configurations."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using CUDA\n\nfunction gpu_axpy!(A, X, Y)\n    # set tid to thread rank\n    tid = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n    tid > length(X) && return \n    Y[tid] = A*X[tid] + Y[tid]\n    return\nend\n\nN = 2^20\ngpu_v1 = CUDA.rand(N)\ngpu_v2 = CUDA.rand(N)\ngpu_v3 = copy(gpu_v2)\n\nα = 0.48\ngpu_v2 .+= α * gpu_v1\n\nfunction verify_grid(args, result, numthreads, numblocks = cld(N, numthreads))\n    u = copy(args[3])\n\n    @cuda threads=numthreads blocks=numblocks gpu_axpy!(args...)\n    println(\"Explicit kernel launch with threads=$numthreads and blocks=$numblocks is correct: \"\n    ,result == args[3])\n\n    args[3] = u\n    return \nend\n\nargs = [α, gpu_v1, gpu_v3]\n\nverify_grid(args, gpu_v2, 1024)\nverify_grid(args, gpu_v2, 1)\nverify_grid(args, gpu_v2, 33)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Occupancy\n\nThe above exercise shows the flexibility in deciding grid configuration. However this raises an important question of how the configuration affects performance ? The best way to determine is to actually measure what works best in a given scenario however that may prove to be cumbersome for many examples. \n\nWhile each SM(streaming multiprocessor) might be executing 100's of threads from the perspective of the programmer, the GPU Hardware however deals with `warps`. Each warp is a set of fixed number of threads(32 on NVIDIA hardware). Scheduling and issuing instructions is done at a per warp basis rather than a per thread basis. There is atleast one *warp scheduler* inside each SM whose job it is to keep the SM as busy as possible. Switching between warps(context switch) is extremely fast and essential to hide latencies such as global memory access. Whenever a warp stalls another warp is immediately switched to.\n\nComing back to out original question of how to determine the optimal grid configuration. One possible solution is to use the occupancy heurestic. $\\mbox{occupancy} = \\frac{\\mbox{active warps}}{\\mbox{maximum number of active warps}}$.\nSince each SM has a finite amount of resources. As the number of resources per thread increases, fewer of them can be concurrently executed. Occupancy can limited by register usage, shared memory and block size.\n\nThe `launch_configuration` function analyses the kernel's resource usage and suggests a configuration that maximises occupancy."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@show kernel_args = cudaconvert((α, gpu_v1, gpu_v3)) # Convert to GPU friendly types\n@show kernel_tt = Tuple{Core.Typeof.(kernel_args)...}\nkernel = cufunction(gpu_axpy!, kernel_tt)\nkernel_config = launch_configuration(kernel.fun, shmem = 0)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our example the configurator returns 20 blocks and 1024 threads. The occupancy API does not understand what our kernel is doing, it can only see the input types and the function definition. It's our job to figure out if the suggested configuration will work or not. It's best to keep the suggested block size in account while deciding the launch config. Launch configurat\n\nFor this example it's perhaps best to set the block size to 1024 and determine the grid size based on that.\n\n# Mandelbrot Set\n\nA popular example of mathematical visualization is the Mandelbrot set. Mathematically it is defined as the set of [complex numbers](https://simple.wikipedia.org/wiki/Complex_number) $c$ such $f_c(z) = z^2 + c$ is bounded.\n\nIn other words:\n\n- $Z_0 = 0$\n- $Z_{n + 1} = {Z_n}^2 + c$\n- $c$ is in the mandelbrot set if the value of $Z_{n}$ is bounded.\nIt can be mathematically shown $|Z_n| \\leq 2.0 $ for bounded points."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Images\n\nimg = rand(Bool, 10, 10)\nGray.(img)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "dims = (3000, 3000)\n\nmset = Array{Bool, 2}(undef, dims)\n\nfunction mandelbrot_cpu(mset::AbstractArray, dims, iterations)\n    origin = CartesianIndex(div.(dims, (2, 2), RoundUp))\n    for ind in CartesianIndices(mset)\n        # Compute coordinates for true canvas\n        coordinates = Tuple(ind - origin) ./ 1000.\n        c = ComplexF32(coordinates[1]im + coordinates[2])\n        mset[ind] = mandelbrot(c, iterations)\n    end\nend\n\nfunction mandelbrot(c, iterations)\n    z = ComplexF32(0, 0)\n    for i in 1:iterations\n        z = z^2 + c\n        abs(z) > 2.0 && return false\n    end\n    return true\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "mandelbrot_cpu(mset, dims, 32)\nGray.(mset)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This black and white image is no fun. To add color let's map a color to the number iterations it took $z$ to become greater than two."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "mset_color = Array{UInt8}(undef, dims)\nfunction mandelbrot(c, iterations)\n    z = ComplexF32(0, 0)\n    for i in 1:iterations\n        z = z^2 + c\n        abs2(z) > 4.0 && return i % UInt8\n    end\n    return zero(UInt8) \nend\nmandelbrot_cpu(mset_color, dims, 32)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "cmap = colormap(\"RdBu\", 32 + 1)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "map(x -> cmap[x + 1], mset_color)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our task is to move this computation to the GPU. The tricky part with moving to the GPU is that the idexing gets tricky. We can use 1-D indexing then figure out inside the kernel what our 2-D index is or use 2-D index from the get go."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "mset_gpu = CuArray{UInt8}(undef, dims)\nfunction mandelbrot_gpu(mset::AbstractArray, dims, iterations)\n    ind = CartesianIndex((blockIdx().y - 1)*blockDim().y + threadIdx().y,\n                        (blockIdx().x - 1)*blockDim().x + threadIdx().x)\n    # Check if index is valid, if not then exit\n    !(ind in CartesianIndices(dims)) && return\n    origin = CartesianIndex(div.(dims, (2, 2), RoundUp))\n    \n    # Scale the 3000x3000 image to -1.5 to 1.5\n    coordinates = Tuple(ind - origin) ./ 1000.\n    c = ComplexF32(coordinates[1]im + coordinates[2]) # x + yi\n    mset[ind] = mandelbrot(c, iterations)\n    return\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "blkdim = (16, 16)\n@cuda blocks=cld.(dims, blkdim) threads=blkdim mandelbrot_gpu(mset_gpu, dims, 32)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# copy back to host and display the same image\nmap(x -> cmap[x + 1], Array(mset_gpu))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "//TODO: Do I want to cover the roofline model here ? Not sure. \n\n## Roofline Model\n\nBoth axpy and the mandelbrot set examples are similar in the fact that both are embarassigly parallel applications. There is no thread cooperation making both simple and optimistic examples of parallelism. Both of these however are contrast in their arithmetic intensities. \n\nArithmetic intensity of a kernel is defined as the ratio between floating point operations to total data movement. Total data movement refers to global memory reads and writes. Registers and cache access does not count. For example, `saxpy!` reads 8 bytes, writes 4 bytes and perform a single floating point operation for each number([Fused multiply-add](https://en.wikipedia.org/wiki/Multiply%E2%80%93accumulate_operation) is one operation rather than a separate multiply then add). \n\n$\\mbox{arithmetic intensity of saxpy} = \\frac{1}{12}$"
      ],
      "metadata": {}
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.5.0"
    },
    "kernelspec": {
      "name": "julia-1.5",
      "display_name": "Julia 1.5.0",
      "language": "julia"
    }
  },
  "nbformat": 4
}
