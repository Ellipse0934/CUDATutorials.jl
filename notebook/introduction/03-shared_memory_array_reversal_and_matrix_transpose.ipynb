{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Shared memory\n\nIn this example we will explore shared memory. We will use array reversal and matrix transpose as examples.\n\n**Shared memory** is the memory in a SM(symmetric multiprocessor) which is accessable to all threads running on the SM. It is much faster than global memory much closer. The amount of shared memory available depends on the compute capability of the GPU. Increasing the amount of shared memory reduces occupancy.\n\n`syncthreads()` is a function which adds a [barrier](https://en.wikipedia.org/wiki/Barrier_(computer_science)) for all threads in a thread block. While all threads in a block execute concurrently, physically only a subset of these are running with true parallelism. A barrier ensures that all threads which belong to it stall until all have reached the barrier. This is commonly used as a synchronization mechanism to eliminate race conditions.\n\n### Array Reversal\n\nOur job is to reverse an array, i.e $[1, 2, 3] \\rightarrow [3, 2, 1]$."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using CUDA, BenchmarkTools\n\nfunction reverse(input, output = similar(input))\n    len = length(input)\n    for i = 1:cld(len,2)\n        output[i], output[len - i + 1] = input[len - i + 1], input[i]\n    end\n    output\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "reverse([1, 2, 3, 4, 5])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function gpu_reverse(input, output)\n    tid = threadIdx().x\n    len = length(input)\n    if tid <= cld(len, 2)\n        output[tid], output[len - tid + 1] = input[len - tid + 1], input[tid]\n    end\n    return\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "A = CuArray(collect(1:5))\nB = similar(A)\n@cuda blocks=1 threads=length(A) gpu_reverse(A, B)\nB"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two ways to declare shared memory: Statically and Dynamically. We declaring it statically when we the need is the same amount for all kernel launches and its known while writing the kernel. In the other case we declare it dynamically and specify while launching with the `@cuda` macro using the `shmem` argument."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "?@cuStaticSharedMem"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function gpu_stshmemreverse(input, output)\n    # Maximum size of array is 64\n    shmem = @cuStaticSharedMem(eltype(output), 64)\n    tid = threadIdx().x\n    len = length(input)\n    shmem[tid] = input[len - tid + 1]\n    output[tid] = shmem[tid]\n    return\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "A = CuArray(collect(1:32))\nB = similar(A)\n@cuda blocks=1 threads=length(A) gpu_stshmemreverse(A, B)\nprint(B)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the amount of shared memory required isn't known while writing the kernel overallocating is not a good idea because that may potentially reduce our occupancy. As an SM's resource usage increases it's occupancy goes down. Hence, it's best to use dynamically allocated memory when memory usage can only be known at launch time."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "?@cuDynamicSharedMem"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function gpu_dyshmemreverse(input, output)\n    shmem = @cuDynamicSharedMem(eltype(output), (length(output),))\n    tid = threadIdx().x\n    len = length(input)\n    shmem[tid] = input[len - tid + 1]\n    output[tid] = shmem[tid]\n    return\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "C = CuArray(collect(1:32))\nD = similar(C)\n@cuda blocks=1 threads=length(C) shmem=length(C) gpu_dyshmemreverse(C, D)\nprint(D)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matrix Transpose\nMatrix transpose is an operation which flips a matrix along its main diagonal.\n\n![transpose](../assets/transpose.png)\n\nThis section is inspired by Mark Harris' blog post on the same topic. [Link](https://developer.nvidia.com/blog/efficient-matrix-transpose-cuda-cc/)"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "A = reshape(1:9, (3, 3))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "A'"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "A'' # (A')'"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ignore the types that Julia returns. `LinearAlgebra.Adjoint` is a wrapper that uses [lazy evaluation](https://en.wikipedia.org/wiki/Lazy_evaluation) to compute the result as required."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# CPU implementation\nfunction cpu_transpose(input, output = similar(input, (size(input, 2), size(input, 1))))\n    # the dimensions of the resultant matrix are reversed\n    for index = CartesianIndices(input)\n            output[index[2], index[1]] = input[index]\n    end\n    output\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "A = reshape(1:20, (4, 5))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "cpu_transpose(A)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we begin working on the GPU consider the following code."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "A = CuArray(reshape(1.:9, (3, 3)))\n\nprintln(\"A => \", pointer(A))\nCUDA.@allowscalar begin\n    for i in eachindex(A)\n        println(i, \" \", A[i], \" \", pointer(A, i))\n    end\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how consecutive elements are in the same column rather than the same row. This is because Julia stores its multidimensional arrays in a column major order like Fortran. In contrast to C/C++ which are row-major languages. The reason for making Julia's arrays column major is because a lot of linear algebra libraries are column major to begin with (https://discourse.julialang.org/t/why-column-major/24374/3)."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# To index our 2-D array we will split the input into tiles of 32x32 elements. \n# Each thread block will launch with 32x8 = 256 threads \n# Each thread will work on 4 elements.\nconst TILE_DIM = 32\n\nfunction gpu_transpose_kernel(input, output)\n    tile_index = ((blockIdx().y, blockIdx().x) .- 1) .* TILE_DIM\n    \n    # each thread manages 4 rows (8x4 = 32)\n    for i in 1:4\n        thread_index = (threadIdx().y + (i - 1)*8, threadIdx().x)\n        index = CartesianIndex(tile_index .+ thread_index)\n        (index[1] > size(input, 1) || index[2] > size(input, 2)) && continue\n        @inbounds output[index] = input[index[2], index[1]]\n    end\n\n    return\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function gpu_transpose(input, output = similar(input, (size(input, 2), size(input, 1))))\n    threads = (32, 8)\n    blocks = cld.(size(input), (32, 32))\n    @cuda blocks=blocks threads=threads gpu_transpose_kernel(input, output)\n    output\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "A = CuArray(reshape(1f0:1089, 33, 33))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "gpu_transpose(A)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "A = CUDA.rand(10000, 10000)\nB = similar(A)\n@benchmark CUDA.@sync gpu_transpose(A, B)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@benchmark CUDA.@sync B .= A"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "#=\n# Not sure if this should be included, custom kernel does happen to be faster\n# than the broadcast copy.\n\nfunction gpu_copy_kernel(input, output)\n    x_index = (blockIdx().x - 1)*TILE_DIM + threadIdx().y\n    y_index = (blockIdx().y - 1)*TILE_DIM + threadIdx().x\n    \n    for i in 1:4 # each thread needs to manage 4 rows (8x4 = 32)\n        index = CartesianIndex(y_index , x_index + (i - 1)*8)\n        (index[1] > size(input, 1) || index[2] > size(input, 2)) && continue\n        @inbounds output[index] = input[index]\n    end\n    \n    return\nend\n\nfunction gpu_copy(input, output = similar(input, size(input)))\n    threads = (32, 8)\n    blocks = cld.(size(input), (32, 32))\n    @cuda blocks=blocks threads=threads gpu_copy_kernel(input, output)\n    output\nend\nA = CUDA.rand(12000, 12000)\nB = similar(A)\n@benchmark CUDA.@sync gpu_copy(A, B)\n=#"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coalescing Memory Access\n\nCompared to a simple elementwise copy we are roughly at 60% performance. Both kernels have a single load and store for each value. If all loads and stores were independent of each other then this should not have happened.\n\nConsider a thread accessing(load or store) a single value in global memory. Instead of transferring just the one value the GPU will instead transfer a larger chunk of memory as a single transaction. For example on NVIDIA's K20 GPU this size was 128 bytes.\nWhen threads in a warp access consecutive memory addresses the GPU can service multiple threads in the same transaction. This is known as memory coalesing. Access time is effectively reduced by minimizing the number of transactions. However when threads access non-sequentially or sparse data then transactions are serialised. (// TODO: Could be written better)\n\n\n\nWe want consecutive threads of a warp to access consecutive elements in memory. When the thread block is one-dimensional it is straightforward to determine `warpId = threadId().x % warpsize()`.\nAccording to NVIDIA's documentation on [thread hierarchy](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#thread-hierarchy).\n> The index of a thread and its thread ID relate to each other in a straightforward way: For a one-dimensional block, they are the same; for a two-dimensional block of size (Dx, Dy),the thread ID of a thread of index (x, y) is (x + y Dx); for a three-dimensional block of size (Dx, Dy, Dz), the thread ID of a thread of index (x, y, z) is (x + y Dx + z Dx Dy).\n\n\nIn our kernel there are four loads and stores per thread.\n- `tile_index = ((blockIdx().y, blockIdx().x) .- 1) .* TILE_DIM`\n- `thread_index = (threadIdx().y + (i - 1)*8, threadIdx().x)`\n- `index = CartesianIndex(tile_index .+ thread_index)`\n- `Load: input[index[2], index[1]]`\n- `Store: output[index[1], index[2]]`\n\nThe loads are coalesced because the column is indexed by `index[2]` which has `threadIdx().x` and the stores are non-coalesced because they are indexed by `index[1]` which has `threadIdx().y`.\n\nTo ensure coalescing during both loads and stores we will use shared memory. We will load from global memory a column and store it in shared memory as a row, effectively transposing it. Once all threads have written to shared memory we can write back to global memory column wise.\n\n![Coalesced transpose](../assets/coalesced_transpose.png)"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function gpu_transpose_kernel2(input, output)\n    # Declare shared memory\n    shared = @cuStaticSharedMem(eltype(input), (TILE_DIM, TILE_DIM))\n    \n    # Modify thread index so threadIdx().x dominates the column\n    block_index = ((blockIdx().y, blockIdx().x) .- 1) .* TILE_DIM\n    \n    for i in 1:4\n        thread_index = (threadIdx().x, threadIdx().y + (i - 1)*8)\n        index = CartesianIndex(block_index .+ thread_index)\n\n        (index[1] > size(input, 1) || index[2] > size(input, 2)) && continue\n        @inbounds shared[thread_index[2], thread_index[1]] = input[index]\n    end\n    \n    # Barrier to ensure all threads have completed writing to shared memory\n    sync_threads()\n    \n    # swap tile index\n    block_index = ((blockIdx().x, blockIdx().y) .- 1) .* TILE_DIM\n    \n    for i in 1:4 \n        thread_index = (threadIdx().x, threadIdx().y + (i - 1)*8)\n        index = CartesianIndex(block_index .+ thread_index)\n        \n        (index[1] > size(output, 1) || index[2] > size(output, 2)) && continue\n        @inbounds output[index] = shared[thread_index...]\n    end\n    return\nend\n\nfunction gpu_transpose_shmem(input, output = similar(input, (size(input, 2), size(input, 1))))\n    threads = (32, 8)\n    blocks = cld.(size(input), (32, 32))\n    @cuda blocks=blocks threads=threads gpu_transpose_kernel2(input, output)\n    output\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "A = CuArray(reshape(1f0:1089, (33, 33)))\nB = similar(A)\ngpu_transpose_shmem(A, B)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "A = CUDA.rand(10000, 10000)\nB = similar(A)\n@benchmark CUDA.@sync gpu_transpose_shmem(A, B)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@benchmark CUDA.@sync B .= A\n\n# TODO: Doesn't look like an inspiring case. We should investigate why the broadcast is slower.\n# ofc make it faster so that there is an inspiring claim of how there is more room \n# for performance. Making \"bank conflicts\" the next natural topic."
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Shared Memory Bank conflicts\n\nInside a , shared memory is divided into banks. Modern NVIDIA GPUs have 32 banks which have a 4-byte boundary. This means addresses 1-4 of shared memory are serviced by bank 1, addresses 5-8 are serviced by bank two and so on. When multiple threads access memory from the same bank then their requests are serialised.\n\nNsight compute gives statistics about shared memory usage. Running the profiler on `gpu_transpose_shmem` for an input of 33x33 of `Float32` we get:\n\n![shared-memory-conflicts](../assets/shmem-conflicts.png)\n\nIt reports zero conflicts during shared loads which makes sense during shared loads because we load column by column to write to output matrix.\n\nThe 1023 store conflicts can be explained as follows. When an entire column is read it is stored to a row. Consecutive elements in a row differ in address by `column_length*sizeof(datatype)`. In 33 tile columns we write directly to a complete row where 32 elements are written hence there are 31 write conflicts. 33*31 = 1023\n\n// DIAGRAM\n\nThe fix is quite simple, pad the column length in shared memory by 1. Now consecutive elements in a row will differ by 33 % 32 = 1 hence no more bank conflicts.\n\ni.e. `shared = @cuStaticSharedMem(eltype(input), (TILE_DIM + 1, TILE_DIM))`"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function gpu_transpose_kernel3(input, output)\n    # Declare shared memory\n    shared = @cuStaticSharedMem(eltype(input), (TILE_DIM + 1, TILE_DIM))\n    \n    # Modify thread index so threadIdx().x dominates the column\n    block_index = ((blockIdx().y, blockIdx().x) .- 1) .* TILE_DIM\n    \n    for i in 1:4\n        thread_index = (threadIdx().x, threadIdx().y + (i - 1)*8)\n        index = CartesianIndex(block_index .+ thread_index)\n\n        (index[1] > size(input, 1) || index[2] > size(input, 2)) && continue\n        @inbounds shared[thread_index[2], thread_index[1]] = input[index]\n    end\n    \n    # Barrier to ensure all threads have completed writing to shared memory\n    sync_threads()\n    \n    # swap tile index\n    block_index = ((blockIdx().x, blockIdx().y) .- 1) .* TILE_DIM\n    \n    for i in 1:4 \n        thread_index = (threadIdx().x, threadIdx().y + (i - 1)*8)\n        index = CartesianIndex(block_index .+ thread_index)\n        \n        (index[1] > size(output, 1) || index[2] > size(output, 2)) && continue\n        @inbounds output[index] = shared[thread_index...]\n    end\n    return\nend\n\nfunction gpu_transpose_noconf(input, output = similar(input, (size(input, 2), size(input, 1))))\n    threads = (32, 8)\n    blocks = cld.(size(input), (32, 32))\n    @cuda blocks=blocks threads=threads gpu_transpose_kernel3(input, output)\n    output\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "A = CUDA.rand(10000, 10000)\nB = similar(A)\n@benchmark CUDA.@sync gpu_transpose_noconf(A, B)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "A = CUDA.rand(10000, 10000)\nB = similar(A)\n@benchmark CUDA.@sync gpu_transpose_shmem(A, B)"
      ],
      "metadata": {},
      "execution_count": null
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.5.0"
    },
    "kernelspec": {
      "name": "julia-1.5",
      "display_name": "Julia 1.5.0",
      "language": "julia"
    }
  },
  "nbformat": 4
}
