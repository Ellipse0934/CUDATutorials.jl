{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Reduction\n",
    "\n",
    "The `reduce` function takes in a binary operator `⊕` and a ordered collection, applying the operator to that collection effectively reducing it to one final value.\n",
    "\n",
    "For example, the operator ⊕ can be `minimum` and the collection can be an array of Integers.\n",
    "⊕ can also be `addition` or `xor` and the collection may represent any type of object as long as the operator makes sense in the context."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using CUDA\n",
    "\n",
    "a = rand(10)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "reduce(min, a)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "reduce(*, a)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@doc reduce(*, a)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Writing `reduce` for a CPU is quite straightforward with a single `for-loop`. We will focus on writing a reduction for a linear array in this tutorial. We will iteratively develop a performant version using everything we have learnt in the previous tutorials.\n",
    "\n",
    "---\n",
    "\n",
    "# Reduction 1 : Divide and Conquer\n",
    "\n",
    "The first step is to write something that works on the GPU. If we were given a dual-core machine and expected to parallelize this we would split the input array into two halves and feed each half into a different CPU. Similarly the best hint is to use the [divide-and-conquer](https://en.wikipedia.org/wiki/Divide-and-conquer_algorithm) approach.\n",
    "By envisioning the reduction process as a binary tree we get:\n",
    "\n",
    "![reduction-1](../assets/reduction1.png)\n",
    "\n",
    "It would be a good exercise to try to write model the above process in pseudocode.\n",
    "\n",
    "One such approach is:\n",
    "\n",
    "\n",
    "The only issue with this approach on the GPU is that after each step we need to synchronize which won't be possible with arrays which span over a single thread block (1024 threads is the maximum threads in a block). Hence, we will have to use a recursive approach.\n",
    "\n",
    "Assume for now we have $1024$ threads per block and process one element per thread. If we have $2048$ threads then we will run our algorithm with two thread blocks, storing the results in an intermediate array. After our first kernel is done we will perform a reduction on the intermediate array. And if we have an array whose length is greater than $1024^2$ we will have another level of recursion. If there are $1024*1024 + 1$ elements then the $1^{st}$ level of reduction will return an intermediate array of size $1025$ which will take another level of recursion to process."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function reduction1(op, a::CuArray)\n",
    "    threadsPerBlock = 1024\n",
    "    len = length(a)\n",
    "\n",
    "    sums = similar(a, cld(len, threadsPerBlock))\n",
    "\n",
    "    blocks = cld(len, threadsPerBlock)\n",
    "    shmem = sizeof(eltype(a))*threadsPerBlock\n",
    "    @cuda shmem=shmem threads=threadsPerBlock blocks=blocks reduction1_kernel(op, a, sums)\n",
    "\n",
    "    # Recursively call reduction for larger arrays\n",
    "    if length(sums) > 1\n",
    "        return reduction1(op, sums)[1]\n",
    "    end\n",
    "\n",
    "    CUDA.@allowscalar return sums[1]\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function reduction1_kernel(op, a, sums)\n",
    "    shmem = @cuDynamicSharedMem(eltype(a), (blockDim().x, ))\n",
    "    tid = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n",
    "    index = threadIdx().x\n",
    "    len = blockDim().x\n",
    "\n",
    "    # Adjust length for the last block\n",
    "    if blockIdx().x == gridDim().x\n",
    "        len = mod1(length(a), blockDim().x)\n",
    "    end\n",
    "\n",
    "    if tid <= length(a)\n",
    "        shmem[index] = a[tid]\n",
    "    else\n",
    "        return\n",
    "    end\n",
    "    sync_threads()\n",
    "\n",
    "    steps = floor(Int, CUDA.log2(convert(Float32, len)))\n",
    "    for i = 0:steps\n",
    "        if mod(index - 1, 2^(i + 1)) == 0 && (index + 2^i) <= len\n",
    "            shmem[index] = op(shmem[index], shmem[index + 2^i])\n",
    "        end\n",
    "        sync_threads()\n",
    "    end\n",
    "\n",
    "    if index == 1\n",
    "        sums[blockIdx().x] = shmem[1]\n",
    "    end\n",
    "    return\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "a = CUDA.ones(1025);\n",
    "reduction1(+, a)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "a = CUDA.rand(100_000);\n",
    "println(reduction1(+, a),\"  \",reduce(+, a))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The two results above not being exactly equal is expected since IEEE floats are neither associative nor commutative. Since associativity is tough to achieve on a parallel algorithm we can expect some deviation.\n",
    "\n",
    "---\n",
    "\n",
    "# Reduction 2 : Strided Index\n",
    "\n",
    "One problem with the last reduction was divergent branching, for example thread three is active for exactly one computation (`a[3] op a[4]`) and is never used again. Because GPU's operate warpwise we want to use all the resources of a warp instead of a small subset. When threads in a warp do different things it has diverged and it's efficiency drops. In this case with each iteration half the number of threads go inactive.\n",
    "\n",
    "A simple fix is to change the way threads map to the elements by using a strided index\n",
    "\n",
    "![reduction-2](../assets/reduction2.png)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function reduction2_kernel(op, a, sums)\n",
    "    shmem = @cuDynamicSharedMem(eltype(a), (blockDim().x, ))\n",
    "    tid = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n",
    "    index = threadIdx().x\n",
    "    len = blockDim().x\n",
    "\n",
    "    # Adjust length for the last block\n",
    "    if blockIdx().x == gridDim().x\n",
    "        len = mod1(length(a), blockDim().x)\n",
    "    end\n",
    "\n",
    "    if tid <= length(a)\n",
    "        shmem[index] = a[tid]\n",
    "    else\n",
    "        return\n",
    "    end\n",
    "    sync_threads()\n",
    "\n",
    "    steps = floor(Int, CUDA.log2(convert(Float32, len)))\n",
    "    for i = 0:steps\n",
    "        stride = 2^i\n",
    "        sindex = 2*stride*(index - 1) + 1\n",
    "        if sindex + stride <= len\n",
    "            shmem[sindex] = op(shmem[sindex], shmem[sindex + stride])\n",
    "        end\n",
    "        sync_threads()\n",
    "    end\n",
    "\n",
    "    if index == 1\n",
    "        sums[blockIdx().x] = shmem[1]\n",
    "    end\n",
    "    return\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function reduction2(op, a::CuArray)\n",
    "    threadsPerBlock = 1024\n",
    "    len = length(a)\n",
    "\n",
    "    sums = similar(a, cld(len, threadsPerBlock))\n",
    "\n",
    "    blocks = cld(len, threadsPerBlock)\n",
    "    shmem = sizeof(eltype(a))*threadsPerBlock\n",
    "    @cuda shmem=shmem threads=threadsPerBlock blocks=blocks reduction2_kernel(op, a, sums)\n",
    "\n",
    "    # Recursively call reduction for larger arrays\n",
    "    if length(sums) > 1\n",
    "        return reduction2(op, sums)\n",
    "    end\n",
    "\n",
    "    CUDA.@allowscalar return sums[1]\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "a = CUDA.ones(100_000);\n",
    "reduction2(+, a)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@time CUDA.@sync reduction1(+, a);\n",
    "@time CUDA.@sync reduction2(+, a);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reduction 3 : Sequential access\n",
    "\n",
    "In both the above implementations our memory-access pattern is *strided* which is difficult to coalesce. We discussed *coalesced* memory access in the **Shared Memory** tutorial.\n",
    "\n",
    "**TL;DR** When consecutive threads access consecutive locations in memory, the GPU combines several transactions into a fewer transactions which is called coalesced memory access. When memory accesses are not consecutive which happens when the locations are non-sequantial, sparse or misaligned the GPU hardware is unable to reduce the number of transactions. Since transactions are serviced sequentially there is a significant performance penalty for non-coalesced access.\n",
    "\n",
    "To make use of sequantial access instead of `stride` iterating from 1 to `length ÷ 2` we can do it the other way around (`length ÷ 2`:1)\n",
    "\n",
    "**NOTE**: The algorithm below assumes that the `blockDim` is a power of two. Transforming it to become friendly with non-power of two can be done as an exercise."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function reduction3_kernel(op, a, sums)\n",
    "    shmem = @cuDynamicSharedMem(eltype(a), (blockDim().x, ))\n",
    "    tid = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n",
    "    index = threadIdx().x\n",
    "    len = blockDim().x\n",
    "\n",
    "    # Adjust length for the last block\n",
    "    if blockIdx().x == gridDim().x\n",
    "        len = mod1(length(a), blockDim().x)\n",
    "    end\n",
    "\n",
    "    if tid <= length(a)\n",
    "        @inbounds shmem[index] = a[tid]\n",
    "    else\n",
    "        return\n",
    "    end\n",
    "    sync_threads()\n",
    "\n",
    "    stride = len ÷ 2\n",
    "    while stride > 0\n",
    "        if index <= stride && index + stride <= len\n",
    "            shmem[index] = op(shmem[index], shmem[index + stride])\n",
    "        end\n",
    "        stride = stride ÷ 2\n",
    "        sync_threads()\n",
    "    end\n",
    "\n",
    "    if index == 1\n",
    "        @inbounds sums[blockIdx().x] = shmem[1]\n",
    "    end\n",
    "\n",
    "    return\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function reduction3(op, a::CuArray)\n",
    "    threadsPerBlock = 1024\n",
    "    len = length(a)\n",
    "\n",
    "    sums = similar(a, cld(len, threadsPerBlock))\n",
    "\n",
    "    blocks = cld(len, threadsPerBlock)\n",
    "    shmem = sizeof(eltype(a))*threadsPerBlock\n",
    "    @cuda shmem=shmem threads=threadsPerBlock blocks=blocks reduction3_kernel(op, a, sums)\n",
    "\n",
    "    # Recursively call reduction for larger arrays\n",
    "    if length(sums) > 1\n",
    "        return reduction3(op, sums)[1]\n",
    "    end\n",
    "\n",
    "    CUDA.@allowscalar return sums[1]\n",
    "    return sums\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "a = CUDA.ones(1024);\n",
    "reduction3(+, a)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "a = CUDA.ones(Int, 1024 * 1024);\n",
    "reduction3(+, a)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "CUDA.@time reduction2(+, a);\n",
    "CUDA.@time reduction3(+, a);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reduction 4 : Warp Shuffle\n",
    "\n",
    "A powerful feature in modern GPUs is the ability to communicate within warps with the help of special instructions. Currently we transfer data with the help of shared memory which obviously requires `sync_threads()` and access to shared memory. Warp shuffle functions allow transferring memory within a warp without the use of shared memory also being much faster and not requiring any explicit barrier. The only drawback is that only the following primitive types are supported: `Int32, UInt32, Int64, UInt64, Float32, Float64` and any arbitrary source to destination lane mapping is not permitted.\n",
    "\n",
    "There are four shuffle methods.\n",
    "- `shfl_sync`\n",
    "- `shfl_up_sync`\n",
    "- `shfl_down_sync`\n",
    "- `shfl_xor_sync`"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@doc shfl_sync"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "`shfl_sync` acts as a broadcast transferring a lane's value to all other lane."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function broadcast_gpu(lane)\n",
    "    id = threadIdx().x\n",
    "    val = id\n",
    "    mask = typemax(UInt32) # 0xffffffff\n",
    "    newval = shfl_sync(mask, val, lane)\n",
    "    @cuprint(\"id: \", id, \"\\t value: \", val, \"\\t new value: \", newval, \"\\n\")\n",
    "    return\n",
    "end\n",
    "\n",
    "@cuda threads=32 blocks=1 broadcast_gpu(19)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "`shfl_up_sync` and `shfl_down_sync` copy the value from lane = current_lane ± delta. If lane is out of bounds from the warp then"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@doc shfl_down_sync"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function shfldown_gpu(delta)\n",
    "    id = threadIdx().x\n",
    "    val = id\n",
    "    mask = typemax(UInt32) # 0xffffffff\n",
    "    newval = shfl_down_sync(mask, val, delta)\n",
    "    @cuprint(\"id: \", id, \"\\t old value: \", val, \"\\t new value: \", newval, \"\\n\")\n",
    "    return\n",
    "end\n",
    "\n",
    "@cuda threads=32 blocks=1 shfldown_gpu(2)\n",
    "synchronize()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can use `shfl_down_sync` to reduce a warp much faster than shared memory."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@inline function reducewarp(op, val, mask = typemax(UInt32))\n",
    "    val = op(val, shfl_down_sync(mask, val, 1))\n",
    "    val = op(val, shfl_down_sync(mask, val, 2))\n",
    "    val = op(val, shfl_down_sync(mask, val, 4))\n",
    "    val = op(val, shfl_down_sync(mask, val, 8))\n",
    "    val = op(val, shfl_down_sync(mask, val, 16))\n",
    "    return val\n",
    "end\n",
    "\n",
    "function reduction4_kernel(op, a, sums)\n",
    "    tid = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n",
    "    lane_id = tid % 32\n",
    "    warp_id = cld(tid, 32)\n",
    "\n",
    "    # exit\n",
    "    tid > length(a) && return\n",
    "\n",
    "    # set essential\n",
    "    tid <= length(a) && (val = a[tid])\n",
    "\n",
    "    val = reducewarp(op, val)\n",
    "    lane_id == 1 && (sums[warp_id] = val)\n",
    "    return\n",
    "end\n",
    "\n",
    "function reduction4(op, a::CuArray)\n",
    "    threadsPerBlock = 1024\n",
    "    len = length(a)\n",
    "\n",
    "    sums = similar(a, cld(len, 32))\n",
    "\n",
    "    blocks = cld(len, threadsPerBlock)\n",
    "    shmem = sizeof(eltype(a))*threadsPerBlock\n",
    "    @cuda threads=threadsPerBlock blocks=blocks reduction4_kernel(op, a, sums)\n",
    "\n",
    "    # Recursively call reduction for larger arrays\n",
    "    if length(sums) > 1\n",
    "        return reduction4(op, sums)\n",
    "    end\n",
    "\n",
    "    CUDA.@allowscalar return sums[1]\n",
    "    return sums\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "a = CUDA.ones(Int, 320_000)\n",
    "reduction4(+, a)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "There is one big problem with our implementation, the input length is expected to be a multiple of 32. This problem can be solved either by defining a neutral element for `op` (`zero(eltype(a))` for `+`, `one(eltype(a))` for `*`) however this won't work with `xor`. Another is to force the last warp's computation via shared memory like earlier examples.\n",
    "We correct this by having only 1 thread work for the last warp; this is a simple solution that is inefficient but when the number of warps is large the performance hit shouldn't be too high."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function reduction5_kernel(op, a, sums)\n",
    "    tid = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n",
    "    lane_id = tid % 32\n",
    "    warp_id = cld(tid, 32)\n",
    "\n",
    "    # exit non essential\n",
    "    tid > cld(length(a), 32)*32 && return\n",
    "\n",
    "    # set essential\n",
    "    tid <= length(a) && (val = a[tid])\n",
    "\n",
    "    # Manage last warp\n",
    "    if warp_id*32 > length(a)\n",
    "        if lane_id == 1\n",
    "            for i=(tid + 1):length(a)\n",
    "                val = op(val, a[i])\n",
    "            end\n",
    "        end\n",
    "    else\n",
    "        val = reducewarp(op, val)\n",
    "    end\n",
    "\n",
    "    lane_id == 1 && (sums[warp_id] = val)\n",
    "    return\n",
    "end\n",
    "\n",
    "function reduction5(op, a::CuArray)\n",
    "    threadsPerBlock = 1024\n",
    "    len = length(a)\n",
    "\n",
    "    sums = similar(a, cld(len, 32))\n",
    "\n",
    "    blocks = cld(len, threadsPerBlock)\n",
    "    shmem = sizeof(eltype(a))*threadsPerBlock\n",
    "    @cuda threads=threadsPerBlock blocks=blocks reduction5_kernel(op, a, sums)\n",
    "\n",
    "    # Recursively call reduction for larger arrays\n",
    "    if length(sums) > 1\n",
    "        return reduction5(op, sums)\n",
    "    end\n",
    "\n",
    "    CUDA.@allowscalar return sums[1]\n",
    "    return sums\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "a = CUDA.ones(Int, 5_000_000)\n",
    "reduction5(+, a)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@time CUDA.@sync reduction3(+, a);\n",
    "@time CUDA.@sync reduction5(+, a);"
   ],
   "metadata": {},
   "execution_count": null
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.0"
  },
  "kernelspec": {
   "name": "julia-1.5",
   "display_name": "Julia 1.5.0",
   "language": "julia"
  }
 },
 "nbformat": 4
}
