<!DOCTYPE html>
<HTML lang = "en">
<HEAD>
  <meta charset="UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  
  

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      TeX: { equationNumbers: { autoNumber: "AMS" } }
    });
  </script>

  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>

  
<style>
pre.hljl {
    border: 1px solid #ccc;
    margin: 5px;
    padding: 5px;
    overflow-x: auto;
    color: rgb(68,68,68); background-color: rgb(251,251,251); }
pre.hljl > span.hljl-t { }
pre.hljl > span.hljl-w { }
pre.hljl > span.hljl-e { }
pre.hljl > span.hljl-eB { }
pre.hljl > span.hljl-o { }
pre.hljl > span.hljl-k { color: rgb(148,91,176); font-weight: bold; }
pre.hljl > span.hljl-kc { color: rgb(59,151,46); font-style: italic; }
pre.hljl > span.hljl-kd { color: rgb(214,102,97); font-style: italic; }
pre.hljl > span.hljl-kn { color: rgb(148,91,176); font-weight: bold; }
pre.hljl > span.hljl-kp { color: rgb(148,91,176); font-weight: bold; }
pre.hljl > span.hljl-kr { color: rgb(148,91,176); font-weight: bold; }
pre.hljl > span.hljl-kt { color: rgb(148,91,176); font-weight: bold; }
pre.hljl > span.hljl-n { }
pre.hljl > span.hljl-na { }
pre.hljl > span.hljl-nb { }
pre.hljl > span.hljl-nbp { }
pre.hljl > span.hljl-nc { }
pre.hljl > span.hljl-ncB { }
pre.hljl > span.hljl-nd { color: rgb(214,102,97); }
pre.hljl > span.hljl-ne { }
pre.hljl > span.hljl-neB { }
pre.hljl > span.hljl-nf { color: rgb(66,102,213); }
pre.hljl > span.hljl-nfm { color: rgb(66,102,213); }
pre.hljl > span.hljl-np { }
pre.hljl > span.hljl-nl { }
pre.hljl > span.hljl-nn { }
pre.hljl > span.hljl-no { }
pre.hljl > span.hljl-nt { }
pre.hljl > span.hljl-nv { }
pre.hljl > span.hljl-nvc { }
pre.hljl > span.hljl-nvg { }
pre.hljl > span.hljl-nvi { }
pre.hljl > span.hljl-nvm { }
pre.hljl > span.hljl-l { }
pre.hljl > span.hljl-ld { color: rgb(148,91,176); font-style: italic; }
pre.hljl > span.hljl-s { color: rgb(201,61,57); }
pre.hljl > span.hljl-sa { color: rgb(201,61,57); }
pre.hljl > span.hljl-sb { color: rgb(201,61,57); }
pre.hljl > span.hljl-sc { color: rgb(201,61,57); }
pre.hljl > span.hljl-sd { color: rgb(201,61,57); }
pre.hljl > span.hljl-sdB { color: rgb(201,61,57); }
pre.hljl > span.hljl-sdC { color: rgb(201,61,57); }
pre.hljl > span.hljl-se { color: rgb(59,151,46); }
pre.hljl > span.hljl-sh { color: rgb(201,61,57); }
pre.hljl > span.hljl-si { }
pre.hljl > span.hljl-so { color: rgb(201,61,57); }
pre.hljl > span.hljl-sr { color: rgb(201,61,57); }
pre.hljl > span.hljl-ss { color: rgb(201,61,57); }
pre.hljl > span.hljl-ssB { color: rgb(201,61,57); }
pre.hljl > span.hljl-nB { color: rgb(59,151,46); }
pre.hljl > span.hljl-nbB { color: rgb(59,151,46); }
pre.hljl > span.hljl-nfB { color: rgb(59,151,46); }
pre.hljl > span.hljl-nh { color: rgb(59,151,46); }
pre.hljl > span.hljl-ni { color: rgb(59,151,46); }
pre.hljl > span.hljl-nil { color: rgb(59,151,46); }
pre.hljl > span.hljl-noB { color: rgb(59,151,46); }
pre.hljl > span.hljl-oB { color: rgb(102,102,102); font-weight: bold; }
pre.hljl > span.hljl-ow { color: rgb(102,102,102); font-weight: bold; }
pre.hljl > span.hljl-p { }
pre.hljl > span.hljl-c { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-ch { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-cm { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-cp { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-cpB { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-cs { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-csB { color: rgb(153,153,119); font-style: italic; }
pre.hljl > span.hljl-g { }
pre.hljl > span.hljl-gd { }
pre.hljl > span.hljl-ge { }
pre.hljl > span.hljl-geB { }
pre.hljl > span.hljl-gh { }
pre.hljl > span.hljl-gi { }
pre.hljl > span.hljl-go { }
pre.hljl > span.hljl-gp { }
pre.hljl > span.hljl-gs { }
pre.hljl > span.hljl-gsB { }
pre.hljl > span.hljl-gt { }
</style>



  <style type="text/css">
  @font-face {
  font-style: normal;
  font-weight: 300;
}
@font-face {
  font-style: normal;
  font-weight: 400;
}
@font-face {
  font-style: normal;
  font-weight: 600;
}
html {
  font-family: sans-serif; /* 1 */
  -ms-text-size-adjust: 100%; /* 2 */
  -webkit-text-size-adjust: 100%; /* 2 */
}
body {
  margin: 0;
}
article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
menu,
nav,
section,
summary {
  display: block;
}
audio,
canvas,
progress,
video {
  display: inline-block; /* 1 */
  vertical-align: baseline; /* 2 */
}
audio:not([controls]) {
  display: none;
  height: 0;
}
[hidden],
template {
  display: none;
}
a:active,
a:hover {
  outline: 0;
}
abbr[title] {
  border-bottom: 1px dotted;
}
b,
strong {
  font-weight: bold;
}
dfn {
  font-style: italic;
}
h1 {
  font-size: 2em;
  margin: 0.67em 0;
}
mark {
  background: #ff0;
  color: #000;
}
small {
  font-size: 80%;
}
sub,
sup {
  font-size: 75%;
  line-height: 0;
  position: relative;
  vertical-align: baseline;
}
sup {
  top: -0.5em;
}
sub {
  bottom: -0.25em;
}
img {
  border: 0;
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 70%;
}
svg:not(:root) {
  overflow: hidden;
}
figure {
  margin: 1em 40px;
}
hr {
  -moz-box-sizing: content-box;
  box-sizing: content-box;
  height: 0;
}
pre {
  overflow: auto;
}
code,
kbd,
pre,
samp {
  font-family: monospace, monospace;
  font-size: 1em;
}
button,
input,
optgroup,
select,
textarea {
  color: inherit; /* 1 */
  font: inherit; /* 2 */
  margin: 0; /* 3 */
}
button {
  overflow: visible;
}
button,
select {
  text-transform: none;
}
button,
html input[type="button"], /* 1 */
input[type="reset"],
input[type="submit"] {
  -webkit-appearance: button; /* 2 */
  cursor: pointer; /* 3 */
}
button[disabled],
html input[disabled] {
  cursor: default;
}
button::-moz-focus-inner,
input::-moz-focus-inner {
  border: 0;
  padding: 0;
}
input {
  line-height: normal;
}
input[type="checkbox"],
input[type="radio"] {
  box-sizing: border-box; /* 1 */
  padding: 0; /* 2 */
}
input[type="number"]::-webkit-inner-spin-button,
input[type="number"]::-webkit-outer-spin-button {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: textfield; /* 1 */
  -moz-box-sizing: content-box;
  -webkit-box-sizing: content-box; /* 2 */
  box-sizing: content-box;
}
input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
  -webkit-appearance: none;
}
fieldset {
  border: 1px solid #c0c0c0;
  margin: 0 2px;
  padding: 0.35em 0.625em 0.75em;
}
legend {
  border: 0; /* 1 */
  padding: 0; /* 2 */
}
textarea {
  overflow: auto;
}
optgroup {
  font-weight: bold;
}
table {
  font-family: monospace, monospace;
  font-size : 0.8em;
  border-collapse: collapse;
  border-spacing: 0;
}
td,
th {
  padding: 0;
}
thead th {
    border-bottom: 1px solid black;
    background-color: white;
}
tr:nth-child(odd){
  background-color: rgb(248,248,248);
}


/*
* Skeleton V2.0.4
* Copyright 2014, Dave Gamache
* www.getskeleton.com
* Free to use under the MIT license.
* http://www.opensource.org/licenses/mit-license.php
* 12/29/2014
*/
.container {
  position: relative;
  width: 100%;
  max-width: 960px;
  margin: 0 auto;
  padding: 0 20px;
  box-sizing: border-box; }
.column,
.columns {
  width: 100%;
  float: left;
  box-sizing: border-box; }
@media (min-width: 400px) {
  .container {
    width: 85%;
    padding: 0; }
}
@media (min-width: 550px) {
  .container {
    width: 80%; }
  .column,
  .columns {
    margin-left: 4%; }
  .column:first-child,
  .columns:first-child {
    margin-left: 0; }

  .one.column,
  .one.columns                    { width: 4.66666666667%; }
  .two.columns                    { width: 13.3333333333%; }
  .three.columns                  { width: 22%;            }
  .four.columns                   { width: 30.6666666667%; }
  .five.columns                   { width: 39.3333333333%; }
  .six.columns                    { width: 48%;            }
  .seven.columns                  { width: 56.6666666667%; }
  .eight.columns                  { width: 65.3333333333%; }
  .nine.columns                   { width: 74.0%;          }
  .ten.columns                    { width: 82.6666666667%; }
  .eleven.columns                 { width: 91.3333333333%; }
  .twelve.columns                 { width: 100%; margin-left: 0; }

  .one-third.column               { width: 30.6666666667%; }
  .two-thirds.column              { width: 65.3333333333%; }

  .one-half.column                { width: 48%; }

  /* Offsets */
  .offset-by-one.column,
  .offset-by-one.columns          { margin-left: 8.66666666667%; }
  .offset-by-two.column,
  .offset-by-two.columns          { margin-left: 17.3333333333%; }
  .offset-by-three.column,
  .offset-by-three.columns        { margin-left: 26%;            }
  .offset-by-four.column,
  .offset-by-four.columns         { margin-left: 34.6666666667%; }
  .offset-by-five.column,
  .offset-by-five.columns         { margin-left: 43.3333333333%; }
  .offset-by-six.column,
  .offset-by-six.columns          { margin-left: 52%;            }
  .offset-by-seven.column,
  .offset-by-seven.columns        { margin-left: 60.6666666667%; }
  .offset-by-eight.column,
  .offset-by-eight.columns        { margin-left: 69.3333333333%; }
  .offset-by-nine.column,
  .offset-by-nine.columns         { margin-left: 78.0%;          }
  .offset-by-ten.column,
  .offset-by-ten.columns          { margin-left: 86.6666666667%; }
  .offset-by-eleven.column,
  .offset-by-eleven.columns       { margin-left: 95.3333333333%; }

  .offset-by-one-third.column,
  .offset-by-one-third.columns    { margin-left: 34.6666666667%; }
  .offset-by-two-thirds.column,
  .offset-by-two-thirds.columns   { margin-left: 69.3333333333%; }

  .offset-by-one-half.column,
  .offset-by-one-half.columns     { margin-left: 52%; }

}
html {
  font-size: 62.5%; }
body {
  font-size: 1.5em; /* currently ems cause chrome bug misinterpreting rems on body element */
  line-height: 1.6;
  font-weight: 400;
  font-family: "Raleway", "HelveticaNeue", "Helvetica Neue", Helvetica, Arial, sans-serif;
  color: #222; }
h1, h2, h3, h4, h5, h6 {
  margin-top: 0;
  margin-bottom: 2rem;
  font-weight: 300; }
h1 { font-size: 3.6rem; line-height: 1.2;  letter-spacing: -.1rem;}
h2 { font-size: 3.4rem; line-height: 1.25; letter-spacing: -.1rem; }
h3 { font-size: 3.2rem; line-height: 1.3;  letter-spacing: -.1rem; }
h4 { font-size: 2.8rem; line-height: 1.35; letter-spacing: -.08rem; }
h5 { font-size: 2.4rem; line-height: 1.5;  letter-spacing: -.05rem; }
h6 { font-size: 1.5rem; line-height: 1.6;  letter-spacing: 0; }

p {
  margin-top: 0; }
a {
  color: #1EAEDB; }
a:hover {
  color: #0FA0CE; }
.button,
button,
input[type="submit"],
input[type="reset"],
input[type="button"] {
  display: inline-block;
  height: 38px;
  padding: 0 30px;
  color: #555;
  text-align: center;
  font-size: 11px;
  font-weight: 600;
  line-height: 38px;
  letter-spacing: .1rem;
  text-transform: uppercase;
  text-decoration: none;
  white-space: nowrap;
  background-color: transparent;
  border-radius: 4px;
  border: 1px solid #bbb;
  cursor: pointer;
  box-sizing: border-box; }
.button:hover,
button:hover,
input[type="submit"]:hover,
input[type="reset"]:hover,
input[type="button"]:hover,
.button:focus,
button:focus,
input[type="submit"]:focus,
input[type="reset"]:focus,
input[type="button"]:focus {
  color: #333;
  border-color: #888;
  outline: 0; }
.button.button-primary,
button.button-primary,
input[type="submit"].button-primary,
input[type="reset"].button-primary,
input[type="button"].button-primary {
  color: #FFF;
  background-color: #33C3F0;
  border-color: #33C3F0; }
.button.button-primary:hover,
button.button-primary:hover,
input[type="submit"].button-primary:hover,
input[type="reset"].button-primary:hover,
input[type="button"].button-primary:hover,
.button.button-primary:focus,
button.button-primary:focus,
input[type="submit"].button-primary:focus,
input[type="reset"].button-primary:focus,
input[type="button"].button-primary:focus {
  color: #FFF;
  background-color: #1EAEDB;
  border-color: #1EAEDB; }
input[type="email"],
input[type="number"],
input[type="search"],
input[type="text"],
input[type="tel"],
input[type="url"],
input[type="password"],
textarea,
select {
  height: 38px;
  padding: 6px 10px; /* The 6px vertically centers text on FF, ignored by Webkit */
  background-color: #fff;
  border: 1px solid #D1D1D1;
  border-radius: 4px;
  box-shadow: none;
  box-sizing: border-box; }
/* Removes awkward default styles on some inputs for iOS */
input[type="email"],
input[type="number"],
input[type="search"],
input[type="text"],
input[type="tel"],
input[type="url"],
input[type="password"],
textarea {
  -webkit-appearance: none;
     -moz-appearance: none;
          appearance: none; }
textarea {
  min-height: 65px;
  padding-top: 6px;
  padding-bottom: 6px; }
input[type="email"]:focus,
input[type="number"]:focus,
input[type="search"]:focus,
input[type="text"]:focus,
input[type="tel"]:focus,
input[type="url"]:focus,
input[type="password"]:focus,
textarea:focus,
select:focus {
  border: 1px solid #33C3F0;
  outline: 0; }
label,
legend {
  display: block;
  margin-bottom: .5rem;
  font-weight: 600; }
fieldset {
  padding: 0;
  border-width: 0; }
input[type="checkbox"],
input[type="radio"] {
  display: inline; }
label > .label-body {
  display: inline-block;
  margin-left: .5rem;
  font-weight: normal; }
ul {
  list-style: circle; }
ol {
  list-style: decimal; }
ul ul,
ul ol,
ol ol,
ol ul {
  margin: 1.5rem 0 1.5rem 3rem;
  font-size: 90%; }
li > p {margin : 0;}
th,
td {
  padding: 12px 15px;
  text-align: left;
  border-bottom: 1px solid #E1E1E1; }
th:first-child,
td:first-child {
  padding-left: 0; }
th:last-child,
td:last-child {
  padding-right: 0; }
button,
.button {
  margin-bottom: 1rem; }
input,
textarea,
select,
fieldset {
  margin-bottom: 1.5rem; }
pre,
blockquote,
dl,
figure,
table,
p,
ul,
ol,
form {
  margin-bottom: 1.0rem; }
.u-full-width {
  width: 100%;
  box-sizing: border-box; }
.u-max-full-width {
  max-width: 100%;
  box-sizing: border-box; }
.u-pull-right {
  float: right; }
.u-pull-left {
  float: left; }
hr {
  margin-top: 3rem;
  margin-bottom: 3.5rem;
  border-width: 0;
  border-top: 1px solid #E1E1E1; }
.container:after,
.row:after,
.u-cf {
  content: "";
  display: table;
  clear: both; }

pre {
  display: block;
  padding: 9.5px;
  margin: 0 0 10px;
  font-size: 13px;
  line-height: 1.42857143;
  word-break: break-all;
  word-wrap: break-word;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre.hljl {
  margin: 0 0 10px;
  display: block;
  background: #f5f5f5;
  border-radius: 4px;
  padding : 5px;
}

pre.output {
  background: #ffffff;
}

pre.code {
  background: #ffffff;
}

pre.julia-error {
  color : red
}

code,
kbd,
pre,
samp {
  font-family: Menlo, Monaco, Consolas, "Courier New", monospace;
  font-size: 13px;
}


@media (min-width: 400px) {}
@media (min-width: 550px) {}
@media (min-width: 750px) {}
@media (min-width: 1000px) {}
@media (min-width: 1200px) {}

h1.title {margin-top : 20px}
img {max-width : 100%}
div.title {text-align: center;}

  </style>
</HEAD>

<BODY>
  <div class ="container">
    <div class = "row">
      <div class = "col-md-12 twelve columns">
        <div class="title">
          
          
          
        </div>

        <h1>Shared memory</h1>
<p>In this example we will explore shared memory. We will use array reversal and matrix transpose as examples.</p>
<p><strong>Shared memory</strong> is the memory in a SM&#40;symmetric multiprocessor&#41; which is accessable to all threads running on the SM. It is much faster than global memory much closer. The amount of shared memory available depends on the compute capability of the GPU. Increasing the amount of shared memory reduces occupancy.</p>
<p><code>syncthreads&#40;&#41;</code> is a function which adds a <a href="https://en.wikipedia.org/wiki/Barrier_&#40;computer_science&#41;">barrier</a> for all threads in a thread block. While all threads in a block execute concurrently, physically only a subset of these are running with true parallelism. A barrier ensures that all threads which belong to it stall until all have reached the barrier. This is commonly used as a synchronization mechanism to eliminate race conditions.</p>
<h3>Array Reversal</h3>
<p>Our job is to reverse an array, i.e <span class="math">$[1, 2, 3] \rightarrow [3, 2, 1]$</span>.</p>


<pre class='hljl'>
<span class='hljl-k'>using</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>BenchmarkTools</span><span class='hljl-t'>

</span><span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>reverse</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>output</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>similar</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>))</span><span class='hljl-t'>
    </span><span class='hljl-n'>len</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-k'>for</span><span class='hljl-t'> </span><span class='hljl-n'>i</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-oB'>:</span><span class='hljl-nf'>cld</span><span class='hljl-p'>(</span><span class='hljl-n'>len</span><span class='hljl-p'>,</span><span class='hljl-ni'>2</span><span class='hljl-p'>)</span><span class='hljl-t'>
        </span><span class='hljl-n'>output</span><span class='hljl-p'>[</span><span class='hljl-n'>i</span><span class='hljl-p'>],</span><span class='hljl-t'> </span><span class='hljl-n'>output</span><span class='hljl-p'>[</span><span class='hljl-n'>len</span><span class='hljl-t'> </span><span class='hljl-oB'>-</span><span class='hljl-t'> </span><span class='hljl-n'>i</span><span class='hljl-t'> </span><span class='hljl-oB'>+</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-p'>]</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>input</span><span class='hljl-p'>[</span><span class='hljl-n'>len</span><span class='hljl-t'> </span><span class='hljl-oB'>-</span><span class='hljl-t'> </span><span class='hljl-n'>i</span><span class='hljl-t'> </span><span class='hljl-oB'>+</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-p'>],</span><span class='hljl-t'> </span><span class='hljl-n'>input</span><span class='hljl-p'>[</span><span class='hljl-n'>i</span><span class='hljl-p'>]</span><span class='hljl-t'>
    </span><span class='hljl-k'>end</span><span class='hljl-t'>
    </span><span class='hljl-n'>output</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
reverse &#40;generic function with 2 methods&#41;
</pre>



<pre class='hljl'>
<span class='hljl-nf'>reverse</span><span class='hljl-p'>([</span><span class='hljl-ni'>1</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>2</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>3</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>4</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>5</span><span class='hljl-p'>])</span>
</pre>


<pre class="output">
5-element Array&#123;Int64,1&#125;:
 5
 4
 3
 2
 1
</pre>



<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_reverse</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>output</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>tid</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>threadIdx</span><span class='hljl-p'>()</span><span class='hljl-oB'>.</span><span class='hljl-n'>x</span><span class='hljl-t'>
    </span><span class='hljl-n'>len</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-k'>if</span><span class='hljl-t'> </span><span class='hljl-n'>tid</span><span class='hljl-t'> </span><span class='hljl-oB'>&lt;=</span><span class='hljl-t'> </span><span class='hljl-nf'>cld</span><span class='hljl-p'>(</span><span class='hljl-n'>len</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>2</span><span class='hljl-p'>)</span><span class='hljl-t'>
        </span><span class='hljl-n'>output</span><span class='hljl-p'>[</span><span class='hljl-n'>tid</span><span class='hljl-p'>],</span><span class='hljl-t'> </span><span class='hljl-n'>output</span><span class='hljl-p'>[</span><span class='hljl-n'>len</span><span class='hljl-t'> </span><span class='hljl-oB'>-</span><span class='hljl-t'> </span><span class='hljl-n'>tid</span><span class='hljl-t'> </span><span class='hljl-oB'>+</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-p'>]</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>input</span><span class='hljl-p'>[</span><span class='hljl-n'>len</span><span class='hljl-t'> </span><span class='hljl-oB'>-</span><span class='hljl-t'> </span><span class='hljl-n'>tid</span><span class='hljl-t'> </span><span class='hljl-oB'>+</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-p'>],</span><span class='hljl-t'> </span><span class='hljl-n'>input</span><span class='hljl-p'>[</span><span class='hljl-n'>tid</span><span class='hljl-p'>]</span><span class='hljl-t'>
    </span><span class='hljl-k'>end</span><span class='hljl-t'>
    </span><span class='hljl-k'>return</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
gpu_reverse &#40;generic function with 1 method&#41;
</pre>



<pre class='hljl'>
<span class='hljl-n'>A</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>CuArray</span><span class='hljl-p'>(</span><span class='hljl-nf'>collect</span><span class='hljl-p'>(</span><span class='hljl-ni'>1</span><span class='hljl-oB'>:</span><span class='hljl-ni'>5</span><span class='hljl-p'>))</span><span class='hljl-t'>
</span><span class='hljl-n'>B</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>similar</span><span class='hljl-p'>(</span><span class='hljl-n'>A</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-nd'>@cuda</span><span class='hljl-t'> </span><span class='hljl-n'>blocks</span><span class='hljl-oB'>=</span><span class='hljl-ni'>1</span><span class='hljl-t'> </span><span class='hljl-n'>threads</span><span class='hljl-oB'>=</span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>A</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_reverse</span><span class='hljl-p'>(</span><span class='hljl-n'>A</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>B</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-n'>B</span>
</pre>


<pre class="output">
5-element CUDA.CuArray&#123;Int64,1&#125;:
 5
 4
 3
 2
 1
</pre>


<p>There are two ways to declare shared memory: Statically and Dynamically. We declaring it statically when we the need is the same amount for all kernel launches and its known while writing the kernel. In the other case we declare it dynamically and specify while launching with the <code>@cuda</code> macro using the <code>shmem</code> argument.</p>


<pre class='hljl'>
<span class='hljl-oB'>?</span><span class='hljl-nd'>@cuStaticSharedMem</span>
</pre>


<pre class="julia-error">
ERROR: syntax: invalid identifier name &quot;?&quot;
</pre>



<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_stshmemreverse</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>output</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-cs'># Maximum size of array is 64</span><span class='hljl-t'>
    </span><span class='hljl-n'>shmem</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nd'>@cuStaticSharedMem</span><span class='hljl-p'>(</span><span class='hljl-nf'>eltype</span><span class='hljl-p'>(</span><span class='hljl-n'>output</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-ni'>64</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>tid</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>threadIdx</span><span class='hljl-p'>()</span><span class='hljl-oB'>.</span><span class='hljl-n'>x</span><span class='hljl-t'>
    </span><span class='hljl-n'>len</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>shmem</span><span class='hljl-p'>[</span><span class='hljl-n'>tid</span><span class='hljl-p'>]</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>input</span><span class='hljl-p'>[</span><span class='hljl-n'>len</span><span class='hljl-t'> </span><span class='hljl-oB'>-</span><span class='hljl-t'> </span><span class='hljl-n'>tid</span><span class='hljl-t'> </span><span class='hljl-oB'>+</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-p'>]</span><span class='hljl-t'>
    </span><span class='hljl-n'>output</span><span class='hljl-p'>[</span><span class='hljl-n'>tid</span><span class='hljl-p'>]</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>shmem</span><span class='hljl-p'>[</span><span class='hljl-n'>tid</span><span class='hljl-p'>]</span><span class='hljl-t'>
    </span><span class='hljl-k'>return</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
gpu_stshmemreverse &#40;generic function with 1 method&#41;
</pre>



<pre class='hljl'>
<span class='hljl-n'>A</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>CuArray</span><span class='hljl-p'>(</span><span class='hljl-nf'>collect</span><span class='hljl-p'>(</span><span class='hljl-ni'>1</span><span class='hljl-oB'>:</span><span class='hljl-ni'>32</span><span class='hljl-p'>))</span><span class='hljl-t'>
</span><span class='hljl-n'>B</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>similar</span><span class='hljl-p'>(</span><span class='hljl-n'>A</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-nd'>@cuda</span><span class='hljl-t'> </span><span class='hljl-n'>blocks</span><span class='hljl-oB'>=</span><span class='hljl-ni'>1</span><span class='hljl-t'> </span><span class='hljl-n'>threads</span><span class='hljl-oB'>=</span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>A</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_stshmemreverse</span><span class='hljl-p'>(</span><span class='hljl-n'>A</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>B</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-nf'>print</span><span class='hljl-p'>(</span><span class='hljl-n'>B</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
&#91;32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14
, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1&#93;
</pre>


<p>When the amount of shared memory required isn&#39;t known while writing the kernel overallocating is not a good idea because that may potentially reduce our occupancy. As an SM&#39;s resource usage increases it&#39;s occupancy goes down. Hence, it&#39;s best to use dynamically allocated memory when memory usage can only be known at launch time.</p>


<pre class='hljl'>
<span class='hljl-oB'>?</span><span class='hljl-nd'>@cuDynamicSharedMem</span>
</pre>


<pre class="julia-error">
ERROR: syntax: invalid identifier name &quot;?&quot;
</pre>



<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_dyshmemreverse</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>output</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>shmem</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nd'>@cuDynamicSharedMem</span><span class='hljl-p'>(</span><span class='hljl-nf'>eltype</span><span class='hljl-p'>(</span><span class='hljl-n'>output</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>output</span><span class='hljl-p'>),))</span><span class='hljl-t'>
    </span><span class='hljl-n'>tid</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>threadIdx</span><span class='hljl-p'>()</span><span class='hljl-oB'>.</span><span class='hljl-n'>x</span><span class='hljl-t'>
    </span><span class='hljl-n'>len</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>shmem</span><span class='hljl-p'>[</span><span class='hljl-n'>tid</span><span class='hljl-p'>]</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>input</span><span class='hljl-p'>[</span><span class='hljl-n'>len</span><span class='hljl-t'> </span><span class='hljl-oB'>-</span><span class='hljl-t'> </span><span class='hljl-n'>tid</span><span class='hljl-t'> </span><span class='hljl-oB'>+</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-p'>]</span><span class='hljl-t'>
    </span><span class='hljl-n'>output</span><span class='hljl-p'>[</span><span class='hljl-n'>tid</span><span class='hljl-p'>]</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>shmem</span><span class='hljl-p'>[</span><span class='hljl-n'>tid</span><span class='hljl-p'>]</span><span class='hljl-t'>
    </span><span class='hljl-k'>return</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
gpu_dyshmemreverse &#40;generic function with 1 method&#41;
</pre>



<pre class='hljl'>
<span class='hljl-n'>C</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>CuArray</span><span class='hljl-p'>(</span><span class='hljl-nf'>collect</span><span class='hljl-p'>(</span><span class='hljl-ni'>1</span><span class='hljl-oB'>:</span><span class='hljl-ni'>32</span><span class='hljl-p'>))</span><span class='hljl-t'>
</span><span class='hljl-n'>D</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>similar</span><span class='hljl-p'>(</span><span class='hljl-n'>C</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-nd'>@cuda</span><span class='hljl-t'> </span><span class='hljl-n'>blocks</span><span class='hljl-oB'>=</span><span class='hljl-ni'>1</span><span class='hljl-t'> </span><span class='hljl-n'>threads</span><span class='hljl-oB'>=</span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>C</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-n'>shmem</span><span class='hljl-oB'>=</span><span class='hljl-nf'>length</span><span class='hljl-p'>(</span><span class='hljl-n'>C</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_dyshmemreverse</span><span class='hljl-p'>(</span><span class='hljl-n'>C</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>D</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-nf'>print</span><span class='hljl-p'>(</span><span class='hljl-n'>D</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
&#91;32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14
, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1&#93;
</pre>


<h2>Matrix Transpose</h2>
<p>Matrix transpose is an operation which flips a matrix along its main diagonal.</p>
<p><img src="../assets/transpose.png" alt="transpose" /></p>
<p>This section is inspired by Mark Harris&#39; blog post on the same topic. <a href="https://developer.nvidia.com/blog/efficient-matrix-transpose-cuda-cc/">Link</a></p>


<pre class='hljl'>
<span class='hljl-n'>A</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>reshape</span><span class='hljl-p'>(</span><span class='hljl-ni'>1</span><span class='hljl-oB'>:</span><span class='hljl-ni'>9</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-ni'>3</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>3</span><span class='hljl-p'>))</span>
</pre>


<pre class="output">
3×3 reshape&#40;::UnitRange&#123;Int64&#125;, 3, 3&#41; with eltype Int64:
 1  4  7
 2  5  8
 3  6  9
</pre>



<pre class='hljl'>
<span class='hljl-n'>A</span><span class='hljl-oB'>&#39;</span>
</pre>


<pre class="output">
3×3 LinearAlgebra.Adjoint&#123;Int64,Base.ReshapedArray&#123;Int64,2,UnitRange&#123;Int64&#125;
,Tuple&#123;&#125;&#125;&#125;:
 1  2  3
 4  5  6
 7  8  9
</pre>



<pre class='hljl'>
<span class='hljl-n'>A</span><span class='hljl-oB'>&#39;&#39;</span><span class='hljl-t'> </span><span class='hljl-cs'># (A&#39;)&#39;</span>
</pre>


<pre class="output">
3×3 reshape&#40;::UnitRange&#123;Int64&#125;, 3, 3&#41; with eltype Int64:
 1  4  7
 2  5  8
 3  6  9
</pre>


<p>Ignore the types that Julia returns. <code>LinearAlgebra.Adjoint</code> is a wrapper that uses <a href="https://en.wikipedia.org/wiki/Lazy_evaluation">lazy evaluation</a> to compute the result as required.</p>


<pre class='hljl'>
<span class='hljl-cs'># CPU implementation</span><span class='hljl-t'>
</span><span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>cpu_transpose</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>output</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>similar</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-nf'>size</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>2</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-nf'>size</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-p'>))))</span><span class='hljl-t'>
    </span><span class='hljl-cs'># the dimensions of the resultant matrix are reversed</span><span class='hljl-t'>
    </span><span class='hljl-k'>for</span><span class='hljl-t'> </span><span class='hljl-n'>index</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>CartesianIndices</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>)</span><span class='hljl-t'>
            </span><span class='hljl-n'>output</span><span class='hljl-p'>[</span><span class='hljl-n'>index</span><span class='hljl-p'>[</span><span class='hljl-ni'>2</span><span class='hljl-p'>],</span><span class='hljl-t'> </span><span class='hljl-n'>index</span><span class='hljl-p'>[</span><span class='hljl-ni'>1</span><span class='hljl-p'>]]</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>input</span><span class='hljl-p'>[</span><span class='hljl-n'>index</span><span class='hljl-p'>]</span><span class='hljl-t'>
    </span><span class='hljl-k'>end</span><span class='hljl-t'>
    </span><span class='hljl-n'>output</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
cpu_transpose &#40;generic function with 2 methods&#41;
</pre>



<pre class='hljl'>
<span class='hljl-n'>A</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>reshape</span><span class='hljl-p'>(</span><span class='hljl-ni'>1</span><span class='hljl-oB'>:</span><span class='hljl-ni'>20</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-ni'>4</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>5</span><span class='hljl-p'>))</span>
</pre>


<pre class="output">
4×5 reshape&#40;::UnitRange&#123;Int64&#125;, 4, 5&#41; with eltype Int64:
 1  5   9  13  17
 2  6  10  14  18
 3  7  11  15  19
 4  8  12  16  20
</pre>



<pre class='hljl'>
<span class='hljl-nf'>cpu_transpose</span><span class='hljl-p'>(</span><span class='hljl-n'>A</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
5×4 Array&#123;Int64,2&#125;:
  1   2   3   4
  5   6   7   8
  9  10  11  12
 13  14  15  16
 17  18  19  20
</pre>


<p>Before we begin working on the GPU consider the following code.</p>


<pre class='hljl'>
<span class='hljl-n'>A</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>CuArray</span><span class='hljl-p'>(</span><span class='hljl-nf'>reshape</span><span class='hljl-p'>(</span><span class='hljl-nfB'>1.</span><span class='hljl-oB'>:</span><span class='hljl-ni'>9</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-ni'>3</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>3</span><span class='hljl-p'>)))</span><span class='hljl-t'>

</span><span class='hljl-nf'>println</span><span class='hljl-p'>(</span><span class='hljl-s'>&quot;A =&gt; &quot;</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-nf'>pointer</span><span class='hljl-p'>(</span><span class='hljl-n'>A</span><span class='hljl-p'>))</span><span class='hljl-t'>
</span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nd'>@allowscalar</span><span class='hljl-t'> </span><span class='hljl-k'>begin</span><span class='hljl-t'>
    </span><span class='hljl-k'>for</span><span class='hljl-t'> </span><span class='hljl-n'>i</span><span class='hljl-t'> </span><span class='hljl-kp'>in</span><span class='hljl-t'> </span><span class='hljl-nf'>eachindex</span><span class='hljl-p'>(</span><span class='hljl-n'>A</span><span class='hljl-p'>)</span><span class='hljl-t'>
        </span><span class='hljl-nf'>println</span><span class='hljl-p'>(</span><span class='hljl-n'>i</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-s'>&quot; &quot;</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>A</span><span class='hljl-p'>[</span><span class='hljl-n'>i</span><span class='hljl-p'>],</span><span class='hljl-t'> </span><span class='hljl-s'>&quot; &quot;</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-nf'>pointer</span><span class='hljl-p'>(</span><span class='hljl-n'>A</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>i</span><span class='hljl-p'>))</span><span class='hljl-t'>
    </span><span class='hljl-k'>end</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
A &#61;&gt; CUDA.CuPtr&#123;Float64&#125;&#40;0x00007f5a8c831800&#41;
1 1.0 CUDA.CuPtr&#123;Float64&#125;&#40;0x00007f5a8c831800&#41;
2 2.0 CUDA.CuPtr&#123;Float64&#125;&#40;0x00007f5a8c831808&#41;
3 3.0 CUDA.CuPtr&#123;Float64&#125;&#40;0x00007f5a8c831810&#41;
4 4.0 CUDA.CuPtr&#123;Float64&#125;&#40;0x00007f5a8c831818&#41;
5 5.0 CUDA.CuPtr&#123;Float64&#125;&#40;0x00007f5a8c831820&#41;
6 6.0 CUDA.CuPtr&#123;Float64&#125;&#40;0x00007f5a8c831828&#41;
7 7.0 CUDA.CuPtr&#123;Float64&#125;&#40;0x00007f5a8c831830&#41;
8 8.0 CUDA.CuPtr&#123;Float64&#125;&#40;0x00007f5a8c831838&#41;
9 9.0 CUDA.CuPtr&#123;Float64&#125;&#40;0x00007f5a8c831840&#41;
</pre>


<p>Notice how consecutive elements are in the same column rather than the same row. This is because Julia stores its multidimensional arrays in a column major order like Fortran. In contrast to C/C&#43;&#43; which are row-major languages. The reason for making Julia&#39;s arrays column major is because a lot of linear algebra libraries are column major to begin with &#40;https://discourse.julialang.org/t/why-column-major/24374/3&#41;.</p>


<pre class='hljl'>
<span class='hljl-cs'># To index our 2-D array we will split the input into tiles of 32x32 elements. </span><span class='hljl-t'>
</span><span class='hljl-cs'># Each thread block will launch with 32x8 = 256 threads </span><span class='hljl-t'>
</span><span class='hljl-cs'># Each thread will work on 4 elements.</span><span class='hljl-t'>
</span><span class='hljl-kd'>const</span><span class='hljl-t'> </span><span class='hljl-n'>TILE_DIM</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-ni'>32</span><span class='hljl-t'>

</span><span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_transpose_kernel</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>output</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>tile_index</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-p'>((</span><span class='hljl-nf'>blockIdx</span><span class='hljl-p'>()</span><span class='hljl-oB'>.</span><span class='hljl-n'>y</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-nf'>blockIdx</span><span class='hljl-p'>()</span><span class='hljl-oB'>.</span><span class='hljl-n'>x</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>.-</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>.*</span><span class='hljl-t'> </span><span class='hljl-n'>TILE_DIM</span><span class='hljl-t'>
    
    </span><span class='hljl-cs'># each thread manages 4 rows (8x4 = 32)</span><span class='hljl-t'>
    </span><span class='hljl-k'>for</span><span class='hljl-t'> </span><span class='hljl-n'>i</span><span class='hljl-t'> </span><span class='hljl-kp'>in</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-oB'>:</span><span class='hljl-ni'>4</span><span class='hljl-t'>
        </span><span class='hljl-n'>thread_index</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-nf'>threadIdx</span><span class='hljl-p'>()</span><span class='hljl-oB'>.</span><span class='hljl-n'>y</span><span class='hljl-t'> </span><span class='hljl-oB'>+</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-n'>i</span><span class='hljl-t'> </span><span class='hljl-oB'>-</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-p'>)</span><span class='hljl-oB'>*</span><span class='hljl-ni'>8</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-nf'>threadIdx</span><span class='hljl-p'>()</span><span class='hljl-oB'>.</span><span class='hljl-n'>x</span><span class='hljl-p'>)</span><span class='hljl-t'>
        </span><span class='hljl-n'>index</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>CartesianIndex</span><span class='hljl-p'>(</span><span class='hljl-n'>tile_index</span><span class='hljl-t'> </span><span class='hljl-oB'>.+</span><span class='hljl-t'> </span><span class='hljl-n'>thread_index</span><span class='hljl-p'>)</span><span class='hljl-t'>
        </span><span class='hljl-p'>(</span><span class='hljl-n'>index</span><span class='hljl-p'>[</span><span class='hljl-ni'>1</span><span class='hljl-p'>]</span><span class='hljl-t'> </span><span class='hljl-oB'>&gt;</span><span class='hljl-t'> </span><span class='hljl-nf'>size</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>||</span><span class='hljl-t'> </span><span class='hljl-n'>index</span><span class='hljl-p'>[</span><span class='hljl-ni'>2</span><span class='hljl-p'>]</span><span class='hljl-t'> </span><span class='hljl-oB'>&gt;</span><span class='hljl-t'> </span><span class='hljl-nf'>size</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>2</span><span class='hljl-p'>))</span><span class='hljl-t'> </span><span class='hljl-oB'>&amp;&amp;</span><span class='hljl-t'> </span><span class='hljl-k'>continue</span><span class='hljl-t'>
        </span><span class='hljl-nd'>@inbounds</span><span class='hljl-t'> </span><span class='hljl-n'>output</span><span class='hljl-p'>[</span><span class='hljl-n'>index</span><span class='hljl-p'>]</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>input</span><span class='hljl-p'>[</span><span class='hljl-n'>index</span><span class='hljl-p'>[</span><span class='hljl-ni'>2</span><span class='hljl-p'>],</span><span class='hljl-t'> </span><span class='hljl-n'>index</span><span class='hljl-p'>[</span><span class='hljl-ni'>1</span><span class='hljl-p'>]]</span><span class='hljl-t'>
    </span><span class='hljl-k'>end</span><span class='hljl-t'>

    </span><span class='hljl-k'>return</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
gpu_transpose_kernel &#40;generic function with 1 method&#41;
</pre>



<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_transpose</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>output</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>similar</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-nf'>size</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>2</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-nf'>size</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-p'>))))</span><span class='hljl-t'>
    </span><span class='hljl-n'>threads</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-ni'>32</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>8</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>blocks</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>cld</span><span class='hljl-oB'>.</span><span class='hljl-p'>(</span><span class='hljl-nf'>size</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-ni'>32</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>32</span><span class='hljl-p'>))</span><span class='hljl-t'>
    </span><span class='hljl-nd'>@cuda</span><span class='hljl-t'> </span><span class='hljl-n'>blocks</span><span class='hljl-oB'>=</span><span class='hljl-n'>blocks</span><span class='hljl-t'> </span><span class='hljl-n'>threads</span><span class='hljl-oB'>=</span><span class='hljl-n'>threads</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_transpose_kernel</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>output</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>output</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
gpu_transpose &#40;generic function with 2 methods&#41;
</pre>



<pre class='hljl'>
<span class='hljl-n'>A</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>CuArray</span><span class='hljl-p'>(</span><span class='hljl-nf'>reshape</span><span class='hljl-p'>(</span><span class='hljl-nfB'>1f0</span><span class='hljl-oB'>:</span><span class='hljl-ni'>1089</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>33</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>33</span><span class='hljl-p'>))</span>
</pre>


<pre class="output">
33×33 CUDA.CuArray&#123;Float32,2&#125;:
  1.0  34.0  67.0  100.0  133.0  166.0  …  958.0   991.0  1024.0  1057.0
  2.0  35.0  68.0  101.0  134.0  167.0     959.0   992.0  1025.0  1058.0
  3.0  36.0  69.0  102.0  135.0  168.0     960.0   993.0  1026.0  1059.0
  4.0  37.0  70.0  103.0  136.0  169.0     961.0   994.0  1027.0  1060.0
  5.0  38.0  71.0  104.0  137.0  170.0     962.0   995.0  1028.0  1061.0
  6.0  39.0  72.0  105.0  138.0  171.0  …  963.0   996.0  1029.0  1062.0
  7.0  40.0  73.0  106.0  139.0  172.0     964.0   997.0  1030.0  1063.0
  8.0  41.0  74.0  107.0  140.0  173.0     965.0   998.0  1031.0  1064.0
  9.0  42.0  75.0  108.0  141.0  174.0     966.0   999.0  1032.0  1065.0
 10.0  43.0  76.0  109.0  142.0  175.0     967.0  1000.0  1033.0  1066.0
  ⋮                                ⋮    ⋱            ⋮            
 25.0  58.0  91.0  124.0  157.0  190.0     982.0  1015.0  1048.0  1081.0
 26.0  59.0  92.0  125.0  158.0  191.0  …  983.0  1016.0  1049.0  1082.0
 27.0  60.0  93.0  126.0  159.0  192.0     984.0  1017.0  1050.0  1083.0
 28.0  61.0  94.0  127.0  160.0  193.0     985.0  1018.0  1051.0  1084.0
 29.0  62.0  95.0  128.0  161.0  194.0     986.0  1019.0  1052.0  1085.0
 30.0  63.0  96.0  129.0  162.0  195.0     987.0  1020.0  1053.0  1086.0
 31.0  64.0  97.0  130.0  163.0  196.0  …  988.0  1021.0  1054.0  1087.0
 32.0  65.0  98.0  131.0  164.0  197.0     989.0  1022.0  1055.0  1088.0
 33.0  66.0  99.0  132.0  165.0  198.0     990.0  1023.0  1056.0  1089.0
</pre>



<pre class='hljl'>
<span class='hljl-nf'>gpu_transpose</span><span class='hljl-p'>(</span><span class='hljl-n'>A</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
33×33 CUDA.CuArray&#123;Float32,2&#125;:
    1.0     2.0     3.0     4.0     5.0  …    30.0    31.0    32.0    33.0
   34.0    35.0    36.0    37.0    38.0       63.0    64.0    65.0    66.0
   67.0    68.0    69.0    70.0    71.0       96.0    97.0    98.0    99.0
  100.0   101.0   102.0   103.0   104.0      129.0   130.0   131.0   132.0
  133.0   134.0   135.0   136.0   137.0      162.0   163.0   164.0   165.0
  166.0   167.0   168.0   169.0   170.0  …   195.0   196.0   197.0   198.0
  199.0   200.0   201.0   202.0   203.0      228.0   229.0   230.0   231.0
  232.0   233.0   234.0   235.0   236.0      261.0   262.0   263.0   264.0
  265.0   266.0   267.0   268.0   269.0      294.0   295.0   296.0   297.0
  298.0   299.0   300.0   301.0   302.0      327.0   328.0   329.0   330.0
    ⋮                                    ⋱             ⋮            
  793.0   794.0   795.0   796.0   797.0      822.0   823.0   824.0   825.0
  826.0   827.0   828.0   829.0   830.0  …   855.0   856.0   857.0   858.0
  859.0   860.0   861.0   862.0   863.0      888.0   889.0   890.0   891.0
  892.0   893.0   894.0   895.0   896.0      921.0   922.0   923.0   924.0
  925.0   926.0   927.0   928.0   929.0      954.0   955.0   956.0   957.0
  958.0   959.0   960.0   961.0   962.0      987.0   988.0   989.0   990.0
  991.0   992.0   993.0   994.0   995.0  …  1020.0  1021.0  1022.0  1023.0
 1024.0  1025.0  1026.0  1027.0  1028.0     1053.0  1054.0  1055.0  1056.0
 1057.0  1058.0  1059.0  1060.0  1061.0     1086.0  1087.0  1088.0  1089.0
</pre>



<pre class='hljl'>
<span class='hljl-n'>A</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nf'>rand</span><span class='hljl-p'>(</span><span class='hljl-ni'>10000</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>10000</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-n'>B</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>similar</span><span class='hljl-p'>(</span><span class='hljl-n'>A</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-nd'>@benchmark</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nd'>@sync</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_transpose</span><span class='hljl-p'>(</span><span class='hljl-n'>A</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>B</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
BenchmarkTools.Trial: 
  memory estimate:  320 bytes
  allocs estimate:  9
  --------------
  minimum time:     12.517 ms &#40;0.00&#37; GC&#41;
  median time:      12.947 ms &#40;0.00&#37; GC&#41;
  mean time:        12.875 ms &#40;0.00&#37; GC&#41;
  maximum time:     13.447 ms &#40;0.00&#37; GC&#41;
  --------------
  samples:          389
  evals/sample:     1
</pre>



<pre class='hljl'>
<span class='hljl-nd'>@benchmark</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nd'>@sync</span><span class='hljl-t'> </span><span class='hljl-n'>B</span><span class='hljl-t'> </span><span class='hljl-oB'>.=</span><span class='hljl-t'> </span><span class='hljl-n'>A</span>
</pre>


<pre class="output">
BenchmarkTools.Trial: 
  memory estimate:  464 bytes
  allocs estimate:  13
  --------------
  minimum time:     7.450 ms &#40;0.00&#37; GC&#41;
  median time:      7.636 ms &#40;0.00&#37; GC&#41;
  mean time:        7.739 ms &#40;0.00&#37; GC&#41;
  maximum time:     8.090 ms &#40;0.00&#37; GC&#41;
  --------------
  samples:          646
  evals/sample:     1
</pre>



<pre class='hljl'>
<span class='hljl-cm'>#=
# Not sure if this should be included, custom kernel does happen to be faster
# than the broadcast copy.

function gpu_copy_kernel(input, output)
    x_index = (blockIdx().x - 1)*TILE_DIM + threadIdx().y
    y_index = (blockIdx().y - 1)*TILE_DIM + threadIdx().x
    
    for i in 1:4 # each thread needs to manage 4 rows (8x4 = 32)
        index = CartesianIndex(y_index , x_index + (i - 1)*8)
        (index[1] &gt; size(input, 1) || index[2] &gt; size(input, 2)) &amp;&amp; continue
        @inbounds output[index] = input[index]
    end
    
    return
end

function gpu_copy(input, output = similar(input, size(input)))
    threads = (32, 8)
    blocks = cld.(size(input), (32, 32))
    @cuda blocks=blocks threads=threads gpu_copy_kernel(input, output)
    output
end
A = CUDA.rand(12000, 12000)
B = similar(A)
@benchmark CUDA.@sync gpu_copy(A, B)
=#</span>
</pre>



<h2>Coalescing Memory Access</h2>
<p>Compared to a simple elementwise copy we are roughly at 60&#37; performance. Both kernels have a single load and store for each value. If all loads and stores were independent of each other then this should not have happened.</p>
<p>Consider a thread accessing&#40;load or store&#41; a single value in global memory. Instead of transferring just the one value the GPU will instead transfer a larger chunk of memory as a single transaction. For example on NVIDIA&#39;s K20 GPU this size was 128 bytes. When threads in a warp access consecutive memory addresses the GPU can service multiple threads in the same transaction. This is known as memory coalesing. Access time is effectively reduced by minimizing the number of transactions. However when threads access non-sequentially or sparse data then transactions are serialised. &#40;// TODO: Could be written better&#41;</p>
<p>We want consecutive threads of a warp to access consecutive elements in memory. When the thread block is one-dimensional it is straightforward to determine <code>warpId &#61; threadId&#40;&#41;.x &#37; warpsize&#40;&#41;</code>. According to NVIDIA&#39;s documentation on <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#thread-hierarchy">thread hierarchy</a>.</p>
<blockquote>
<p>The index of a thread and its thread ID relate to each other in a straightforward way: For a one-dimensional block, they are the same; for a two-dimensional block of size &#40;Dx, Dy&#41;,the thread ID of a thread of index &#40;x, y&#41; is &#40;x &#43; y Dx&#41;; for a three-dimensional block of size &#40;Dx, Dy, Dz&#41;, the thread ID of a thread of index &#40;x, y, z&#41; is &#40;x &#43; y Dx &#43; z Dx Dy&#41;.</p>
</blockquote>
<p>In our kernel there are four loads and stores per thread.</p>
<ul>
<li><p><code>tile_index &#61; &#40;&#40;blockIdx&#40;&#41;.y, blockIdx&#40;&#41;.x&#41; .- 1&#41; .* TILE_DIM</code></p>
</li>
<li><p><code>thread_index &#61; &#40;threadIdx&#40;&#41;.y &#43; &#40;i - 1&#41;*8, threadIdx&#40;&#41;.x&#41;</code></p>
</li>
<li><p><code>index &#61; CartesianIndex&#40;tile_index .&#43; thread_index&#41;</code></p>
</li>
<li><p><code>Load: input&#91;index&#91;2&#93;, index&#91;1&#93;&#93;</code></p>
</li>
<li><p><code>Store: output&#91;index&#91;1&#93;, index&#91;2&#93;&#93;</code></p>
</li>
</ul>
<p>The loads are coalesced because the column is indexed by <code>index&#91;2&#93;</code> which has <code>threadIdx&#40;&#41;.x</code> and the stores are non-coalesced because they are indexed by <code>index&#91;1&#93;</code> which has <code>threadIdx&#40;&#41;.y</code>.</p>
<p>To ensure coalescing during both loads and stores we will use shared memory. We will load from global memory a column and store it in shared memory as a row, effectively transposing it. Once all threads have written to shared memory we can write back to global memory column wise.</p>
<p><img src="../assets/coalesced_transpose.png" alt="Coalesced transpose" /></p>


<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_transpose_kernel2</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>output</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-cs'># Declare shared memory</span><span class='hljl-t'>
    </span><span class='hljl-n'>shared</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nd'>@cuStaticSharedMem</span><span class='hljl-p'>(</span><span class='hljl-nf'>eltype</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-n'>TILE_DIM</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>TILE_DIM</span><span class='hljl-p'>))</span><span class='hljl-t'>
    
    </span><span class='hljl-cs'># Modify thread index so threadIdx().x dominates the column</span><span class='hljl-t'>
    </span><span class='hljl-n'>block_index</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-p'>((</span><span class='hljl-nf'>blockIdx</span><span class='hljl-p'>()</span><span class='hljl-oB'>.</span><span class='hljl-n'>y</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-nf'>blockIdx</span><span class='hljl-p'>()</span><span class='hljl-oB'>.</span><span class='hljl-n'>x</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>.-</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>.*</span><span class='hljl-t'> </span><span class='hljl-n'>TILE_DIM</span><span class='hljl-t'>
    
    </span><span class='hljl-k'>for</span><span class='hljl-t'> </span><span class='hljl-n'>i</span><span class='hljl-t'> </span><span class='hljl-kp'>in</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-oB'>:</span><span class='hljl-ni'>4</span><span class='hljl-t'>
        </span><span class='hljl-n'>thread_index</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-nf'>threadIdx</span><span class='hljl-p'>()</span><span class='hljl-oB'>.</span><span class='hljl-n'>x</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-nf'>threadIdx</span><span class='hljl-p'>()</span><span class='hljl-oB'>.</span><span class='hljl-n'>y</span><span class='hljl-t'> </span><span class='hljl-oB'>+</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-n'>i</span><span class='hljl-t'> </span><span class='hljl-oB'>-</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-p'>)</span><span class='hljl-oB'>*</span><span class='hljl-ni'>8</span><span class='hljl-p'>)</span><span class='hljl-t'>
        </span><span class='hljl-n'>index</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>CartesianIndex</span><span class='hljl-p'>(</span><span class='hljl-n'>block_index</span><span class='hljl-t'> </span><span class='hljl-oB'>.+</span><span class='hljl-t'> </span><span class='hljl-n'>thread_index</span><span class='hljl-p'>)</span><span class='hljl-t'>

        </span><span class='hljl-p'>(</span><span class='hljl-n'>index</span><span class='hljl-p'>[</span><span class='hljl-ni'>1</span><span class='hljl-p'>]</span><span class='hljl-t'> </span><span class='hljl-oB'>&gt;</span><span class='hljl-t'> </span><span class='hljl-nf'>size</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>||</span><span class='hljl-t'> </span><span class='hljl-n'>index</span><span class='hljl-p'>[</span><span class='hljl-ni'>2</span><span class='hljl-p'>]</span><span class='hljl-t'> </span><span class='hljl-oB'>&gt;</span><span class='hljl-t'> </span><span class='hljl-nf'>size</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>2</span><span class='hljl-p'>))</span><span class='hljl-t'> </span><span class='hljl-oB'>&amp;&amp;</span><span class='hljl-t'> </span><span class='hljl-k'>continue</span><span class='hljl-t'>
        </span><span class='hljl-nd'>@inbounds</span><span class='hljl-t'> </span><span class='hljl-n'>shared</span><span class='hljl-p'>[</span><span class='hljl-n'>thread_index</span><span class='hljl-p'>[</span><span class='hljl-ni'>2</span><span class='hljl-p'>],</span><span class='hljl-t'> </span><span class='hljl-n'>thread_index</span><span class='hljl-p'>[</span><span class='hljl-ni'>1</span><span class='hljl-p'>]]</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>input</span><span class='hljl-p'>[</span><span class='hljl-n'>index</span><span class='hljl-p'>]</span><span class='hljl-t'>
    </span><span class='hljl-k'>end</span><span class='hljl-t'>
    
    </span><span class='hljl-cs'># Barrier to ensure all threads have completed writing to shared memory</span><span class='hljl-t'>
    </span><span class='hljl-nf'>sync_threads</span><span class='hljl-p'>()</span><span class='hljl-t'>
    
    </span><span class='hljl-cs'># swap tile index</span><span class='hljl-t'>
    </span><span class='hljl-n'>block_index</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-p'>((</span><span class='hljl-nf'>blockIdx</span><span class='hljl-p'>()</span><span class='hljl-oB'>.</span><span class='hljl-n'>x</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-nf'>blockIdx</span><span class='hljl-p'>()</span><span class='hljl-oB'>.</span><span class='hljl-n'>y</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>.-</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>.*</span><span class='hljl-t'> </span><span class='hljl-n'>TILE_DIM</span><span class='hljl-t'>
    
    </span><span class='hljl-k'>for</span><span class='hljl-t'> </span><span class='hljl-n'>i</span><span class='hljl-t'> </span><span class='hljl-kp'>in</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-oB'>:</span><span class='hljl-ni'>4</span><span class='hljl-t'> 
        </span><span class='hljl-n'>thread_index</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-nf'>threadIdx</span><span class='hljl-p'>()</span><span class='hljl-oB'>.</span><span class='hljl-n'>x</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-nf'>threadIdx</span><span class='hljl-p'>()</span><span class='hljl-oB'>.</span><span class='hljl-n'>y</span><span class='hljl-t'> </span><span class='hljl-oB'>+</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-n'>i</span><span class='hljl-t'> </span><span class='hljl-oB'>-</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-p'>)</span><span class='hljl-oB'>*</span><span class='hljl-ni'>8</span><span class='hljl-p'>)</span><span class='hljl-t'>
        </span><span class='hljl-n'>index</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>CartesianIndex</span><span class='hljl-p'>(</span><span class='hljl-n'>block_index</span><span class='hljl-t'> </span><span class='hljl-oB'>.+</span><span class='hljl-t'> </span><span class='hljl-n'>thread_index</span><span class='hljl-p'>)</span><span class='hljl-t'>
        
        </span><span class='hljl-p'>(</span><span class='hljl-n'>index</span><span class='hljl-p'>[</span><span class='hljl-ni'>1</span><span class='hljl-p'>]</span><span class='hljl-t'> </span><span class='hljl-oB'>&gt;</span><span class='hljl-t'> </span><span class='hljl-nf'>size</span><span class='hljl-p'>(</span><span class='hljl-n'>output</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>||</span><span class='hljl-t'> </span><span class='hljl-n'>index</span><span class='hljl-p'>[</span><span class='hljl-ni'>2</span><span class='hljl-p'>]</span><span class='hljl-t'> </span><span class='hljl-oB'>&gt;</span><span class='hljl-t'> </span><span class='hljl-nf'>size</span><span class='hljl-p'>(</span><span class='hljl-n'>output</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>2</span><span class='hljl-p'>))</span><span class='hljl-t'> </span><span class='hljl-oB'>&amp;&amp;</span><span class='hljl-t'> </span><span class='hljl-k'>continue</span><span class='hljl-t'>
        </span><span class='hljl-nd'>@inbounds</span><span class='hljl-t'> </span><span class='hljl-n'>output</span><span class='hljl-p'>[</span><span class='hljl-n'>index</span><span class='hljl-p'>]</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>shared</span><span class='hljl-p'>[</span><span class='hljl-n'>thread_index</span><span class='hljl-oB'>...</span><span class='hljl-p'>]</span><span class='hljl-t'>
    </span><span class='hljl-k'>end</span><span class='hljl-t'>
    </span><span class='hljl-k'>return</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span><span class='hljl-t'>

</span><span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_transpose_shmem</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>output</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>similar</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-nf'>size</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>2</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-nf'>size</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-p'>))))</span><span class='hljl-t'>
    </span><span class='hljl-n'>threads</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-ni'>32</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>8</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>blocks</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>cld</span><span class='hljl-oB'>.</span><span class='hljl-p'>(</span><span class='hljl-nf'>size</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-ni'>32</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>32</span><span class='hljl-p'>))</span><span class='hljl-t'>
    </span><span class='hljl-nd'>@cuda</span><span class='hljl-t'> </span><span class='hljl-n'>blocks</span><span class='hljl-oB'>=</span><span class='hljl-n'>blocks</span><span class='hljl-t'> </span><span class='hljl-n'>threads</span><span class='hljl-oB'>=</span><span class='hljl-n'>threads</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_transpose_kernel2</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>output</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>output</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
gpu_transpose_shmem &#40;generic function with 2 methods&#41;
</pre>



<pre class='hljl'>
<span class='hljl-n'>A</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>CuArray</span><span class='hljl-p'>(</span><span class='hljl-nf'>reshape</span><span class='hljl-p'>(</span><span class='hljl-nfB'>1f0</span><span class='hljl-oB'>:</span><span class='hljl-ni'>1089</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-ni'>33</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>33</span><span class='hljl-p'>)))</span><span class='hljl-t'>
</span><span class='hljl-n'>B</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>similar</span><span class='hljl-p'>(</span><span class='hljl-n'>A</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-nf'>gpu_transpose_shmem</span><span class='hljl-p'>(</span><span class='hljl-n'>A</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>B</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
33×33 CUDA.CuArray&#123;Float32,2&#125;:
    1.0     2.0     3.0     4.0     5.0  …    30.0    31.0    32.0    33.0
   34.0    35.0    36.0    37.0    38.0       63.0    64.0    65.0    66.0
   67.0    68.0    69.0    70.0    71.0       96.0    97.0    98.0    99.0
  100.0   101.0   102.0   103.0   104.0      129.0   130.0   131.0   132.0
  133.0   134.0   135.0   136.0   137.0      162.0   163.0   164.0   165.0
  166.0   167.0   168.0   169.0   170.0  …   195.0   196.0   197.0   198.0
  199.0   200.0   201.0   202.0   203.0      228.0   229.0   230.0   231.0
  232.0   233.0   234.0   235.0   236.0      261.0   262.0   263.0   264.0
  265.0   266.0   267.0   268.0   269.0      294.0   295.0   296.0   297.0
  298.0   299.0   300.0   301.0   302.0      327.0   328.0   329.0   330.0
    ⋮                                    ⋱             ⋮            
  793.0   794.0   795.0   796.0   797.0      822.0   823.0   824.0   825.0
  826.0   827.0   828.0   829.0   830.0  …   855.0   856.0   857.0   858.0
  859.0   860.0   861.0   862.0   863.0      888.0   889.0   890.0   891.0
  892.0   893.0   894.0   895.0   896.0      921.0   922.0   923.0   924.0
  925.0   926.0   927.0   928.0   929.0      954.0   955.0   956.0   957.0
  958.0   959.0   960.0   961.0   962.0      987.0   988.0   989.0   990.0
  991.0   992.0   993.0   994.0   995.0  …  1020.0  1021.0  1022.0  1023.0
 1024.0  1025.0  1026.0  1027.0  1028.0     1053.0  1054.0  1055.0  1056.0
 1057.0  1058.0  1059.0  1060.0  1061.0     1086.0  1087.0  1088.0  1089.0
</pre>



<pre class='hljl'>
<span class='hljl-n'>A</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nf'>rand</span><span class='hljl-p'>(</span><span class='hljl-ni'>10000</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>10000</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-n'>B</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>similar</span><span class='hljl-p'>(</span><span class='hljl-n'>A</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-nd'>@benchmark</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nd'>@sync</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_transpose_shmem</span><span class='hljl-p'>(</span><span class='hljl-n'>A</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>B</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
BenchmarkTools.Trial: 
  memory estimate:  320 bytes
  allocs estimate:  9
  --------------
  minimum time:     7.415 ms &#40;0.00&#37; GC&#41;
  median time:      7.625 ms &#40;0.00&#37; GC&#41;
  mean time:        7.742 ms &#40;0.00&#37; GC&#41;
  maximum time:     8.339 ms &#40;0.00&#37; GC&#41;
  --------------
  samples:          646
  evals/sample:     1
</pre>



<pre class='hljl'>
<span class='hljl-nd'>@benchmark</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nd'>@sync</span><span class='hljl-t'> </span><span class='hljl-n'>B</span><span class='hljl-t'> </span><span class='hljl-oB'>.=</span><span class='hljl-t'> </span><span class='hljl-n'>A</span><span class='hljl-t'>

</span><span class='hljl-cs'># TODO: Doesn&#39;t look like an inspiring case. We should investigate why the broadcast is slower.</span><span class='hljl-t'>
</span><span class='hljl-cs'># ofc make it faster so that there is an inspiring claim of how there is more room </span><span class='hljl-t'>
</span><span class='hljl-cs'># for performance. Making &quot;bank conflicts&quot; the next natural topic.</span>
</pre>


<pre class="output">
BenchmarkTools.Trial: 
  memory estimate:  464 bytes
  allocs estimate:  13
  --------------
  minimum time:     7.451 ms &#40;0.00&#37; GC&#41;
  median time:      7.674 ms &#40;0.00&#37; GC&#41;
  mean time:        7.784 ms &#40;0.00&#37; GC&#41;
  maximum time:     18.362 ms &#40;0.00&#37; GC&#41;
  --------------
  samples:          642
  evals/sample:     1
</pre>


<h3>Shared Memory Bank conflicts</h3>
<p>Inside a , shared memory is divided into banks. Modern NVIDIA GPUs have 32 banks which have a 4-byte boundary. This means addresses 1-4 of shared memory are serviced by bank 1, addresses 5-8 are serviced by bank two and so on. When multiple threads access memory from the same bank then their requests are serialised.</p>
<p>Nsight compute gives statistics about shared memory usage. Running the profiler on <code>gpu_transpose_shmem</code> for an input of 33x33 of <code>Float32</code> we get:</p>
<p><img src="../assets/shmem-conflicts.png" alt="shared-memory-conflicts" /></p>
<p>It reports zero conflicts during shared loads which makes sense during shared loads because we load column by column to write to output matrix.</p>
<p>The 1023 store conflicts can be explained as follows. When an entire column is read it is stored to a row. Consecutive elements in a row differ in address by <code>column_length*sizeof&#40;datatype&#41;</code>. In 33 tile columns we write directly to a complete row where 32 elements are written hence there are 31 write conflicts. 33*31 &#61; 1023</p>
<p>// DIAGRAM</p>
<p>The fix is quite simple, pad the column length in shared memory by 1. Now consecutive elements in a row will differ by 33 &#37; 32 &#61; 1 hence no more bank conflicts.</p>
<p>i.e. <code>shared &#61; @cuStaticSharedMem&#40;eltype&#40;input&#41;, &#40;TILE_DIM &#43; 1, TILE_DIM&#41;&#41;</code></p>


<pre class='hljl'>
<span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_transpose_kernel3</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>output</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-cs'># Declare shared memory</span><span class='hljl-t'>
    </span><span class='hljl-n'>shared</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nd'>@cuStaticSharedMem</span><span class='hljl-p'>(</span><span class='hljl-nf'>eltype</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-n'>TILE_DIM</span><span class='hljl-t'> </span><span class='hljl-oB'>+</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>TILE_DIM</span><span class='hljl-p'>))</span><span class='hljl-t'>
    
    </span><span class='hljl-cs'># Modify thread index so threadIdx().x dominates the column</span><span class='hljl-t'>
    </span><span class='hljl-n'>block_index</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-p'>((</span><span class='hljl-nf'>blockIdx</span><span class='hljl-p'>()</span><span class='hljl-oB'>.</span><span class='hljl-n'>y</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-nf'>blockIdx</span><span class='hljl-p'>()</span><span class='hljl-oB'>.</span><span class='hljl-n'>x</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>.-</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>.*</span><span class='hljl-t'> </span><span class='hljl-n'>TILE_DIM</span><span class='hljl-t'>
    
    </span><span class='hljl-k'>for</span><span class='hljl-t'> </span><span class='hljl-n'>i</span><span class='hljl-t'> </span><span class='hljl-kp'>in</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-oB'>:</span><span class='hljl-ni'>4</span><span class='hljl-t'>
        </span><span class='hljl-n'>thread_index</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-nf'>threadIdx</span><span class='hljl-p'>()</span><span class='hljl-oB'>.</span><span class='hljl-n'>x</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-nf'>threadIdx</span><span class='hljl-p'>()</span><span class='hljl-oB'>.</span><span class='hljl-n'>y</span><span class='hljl-t'> </span><span class='hljl-oB'>+</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-n'>i</span><span class='hljl-t'> </span><span class='hljl-oB'>-</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-p'>)</span><span class='hljl-oB'>*</span><span class='hljl-ni'>8</span><span class='hljl-p'>)</span><span class='hljl-t'>
        </span><span class='hljl-n'>index</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>CartesianIndex</span><span class='hljl-p'>(</span><span class='hljl-n'>block_index</span><span class='hljl-t'> </span><span class='hljl-oB'>.+</span><span class='hljl-t'> </span><span class='hljl-n'>thread_index</span><span class='hljl-p'>)</span><span class='hljl-t'>

        </span><span class='hljl-p'>(</span><span class='hljl-n'>index</span><span class='hljl-p'>[</span><span class='hljl-ni'>1</span><span class='hljl-p'>]</span><span class='hljl-t'> </span><span class='hljl-oB'>&gt;</span><span class='hljl-t'> </span><span class='hljl-nf'>size</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>||</span><span class='hljl-t'> </span><span class='hljl-n'>index</span><span class='hljl-p'>[</span><span class='hljl-ni'>2</span><span class='hljl-p'>]</span><span class='hljl-t'> </span><span class='hljl-oB'>&gt;</span><span class='hljl-t'> </span><span class='hljl-nf'>size</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>2</span><span class='hljl-p'>))</span><span class='hljl-t'> </span><span class='hljl-oB'>&amp;&amp;</span><span class='hljl-t'> </span><span class='hljl-k'>continue</span><span class='hljl-t'>
        </span><span class='hljl-nd'>@inbounds</span><span class='hljl-t'> </span><span class='hljl-n'>shared</span><span class='hljl-p'>[</span><span class='hljl-n'>thread_index</span><span class='hljl-p'>[</span><span class='hljl-ni'>2</span><span class='hljl-p'>],</span><span class='hljl-t'> </span><span class='hljl-n'>thread_index</span><span class='hljl-p'>[</span><span class='hljl-ni'>1</span><span class='hljl-p'>]]</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>input</span><span class='hljl-p'>[</span><span class='hljl-n'>index</span><span class='hljl-p'>]</span><span class='hljl-t'>
    </span><span class='hljl-k'>end</span><span class='hljl-t'>
    
    </span><span class='hljl-cs'># Barrier to ensure all threads have completed writing to shared memory</span><span class='hljl-t'>
    </span><span class='hljl-nf'>sync_threads</span><span class='hljl-p'>()</span><span class='hljl-t'>
    
    </span><span class='hljl-cs'># swap tile index</span><span class='hljl-t'>
    </span><span class='hljl-n'>block_index</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-p'>((</span><span class='hljl-nf'>blockIdx</span><span class='hljl-p'>()</span><span class='hljl-oB'>.</span><span class='hljl-n'>x</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-nf'>blockIdx</span><span class='hljl-p'>()</span><span class='hljl-oB'>.</span><span class='hljl-n'>y</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>.-</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>.*</span><span class='hljl-t'> </span><span class='hljl-n'>TILE_DIM</span><span class='hljl-t'>
    
    </span><span class='hljl-k'>for</span><span class='hljl-t'> </span><span class='hljl-n'>i</span><span class='hljl-t'> </span><span class='hljl-kp'>in</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-oB'>:</span><span class='hljl-ni'>4</span><span class='hljl-t'> 
        </span><span class='hljl-n'>thread_index</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-nf'>threadIdx</span><span class='hljl-p'>()</span><span class='hljl-oB'>.</span><span class='hljl-n'>x</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-nf'>threadIdx</span><span class='hljl-p'>()</span><span class='hljl-oB'>.</span><span class='hljl-n'>y</span><span class='hljl-t'> </span><span class='hljl-oB'>+</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-n'>i</span><span class='hljl-t'> </span><span class='hljl-oB'>-</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-p'>)</span><span class='hljl-oB'>*</span><span class='hljl-ni'>8</span><span class='hljl-p'>)</span><span class='hljl-t'>
        </span><span class='hljl-n'>index</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>CartesianIndex</span><span class='hljl-p'>(</span><span class='hljl-n'>block_index</span><span class='hljl-t'> </span><span class='hljl-oB'>.+</span><span class='hljl-t'> </span><span class='hljl-n'>thread_index</span><span class='hljl-p'>)</span><span class='hljl-t'>
        
        </span><span class='hljl-p'>(</span><span class='hljl-n'>index</span><span class='hljl-p'>[</span><span class='hljl-ni'>1</span><span class='hljl-p'>]</span><span class='hljl-t'> </span><span class='hljl-oB'>&gt;</span><span class='hljl-t'> </span><span class='hljl-nf'>size</span><span class='hljl-p'>(</span><span class='hljl-n'>output</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-p'>)</span><span class='hljl-t'> </span><span class='hljl-oB'>||</span><span class='hljl-t'> </span><span class='hljl-n'>index</span><span class='hljl-p'>[</span><span class='hljl-ni'>2</span><span class='hljl-p'>]</span><span class='hljl-t'> </span><span class='hljl-oB'>&gt;</span><span class='hljl-t'> </span><span class='hljl-nf'>size</span><span class='hljl-p'>(</span><span class='hljl-n'>output</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>2</span><span class='hljl-p'>))</span><span class='hljl-t'> </span><span class='hljl-oB'>&amp;&amp;</span><span class='hljl-t'> </span><span class='hljl-k'>continue</span><span class='hljl-t'>
        </span><span class='hljl-nd'>@inbounds</span><span class='hljl-t'> </span><span class='hljl-n'>output</span><span class='hljl-p'>[</span><span class='hljl-n'>index</span><span class='hljl-p'>]</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>shared</span><span class='hljl-p'>[</span><span class='hljl-n'>thread_index</span><span class='hljl-oB'>...</span><span class='hljl-p'>]</span><span class='hljl-t'>
    </span><span class='hljl-k'>end</span><span class='hljl-t'>
    </span><span class='hljl-k'>return</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span><span class='hljl-t'>

</span><span class='hljl-k'>function</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_transpose_noconf</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>output</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>similar</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-nf'>size</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>2</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-nf'>size</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>1</span><span class='hljl-p'>))))</span><span class='hljl-t'>
    </span><span class='hljl-n'>threads</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-ni'>32</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>8</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>blocks</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>cld</span><span class='hljl-oB'>.</span><span class='hljl-p'>(</span><span class='hljl-nf'>size</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>),</span><span class='hljl-t'> </span><span class='hljl-p'>(</span><span class='hljl-ni'>32</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>32</span><span class='hljl-p'>))</span><span class='hljl-t'>
    </span><span class='hljl-nd'>@cuda</span><span class='hljl-t'> </span><span class='hljl-n'>blocks</span><span class='hljl-oB'>=</span><span class='hljl-n'>blocks</span><span class='hljl-t'> </span><span class='hljl-n'>threads</span><span class='hljl-oB'>=</span><span class='hljl-n'>threads</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_transpose_kernel3</span><span class='hljl-p'>(</span><span class='hljl-n'>input</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>output</span><span class='hljl-p'>)</span><span class='hljl-t'>
    </span><span class='hljl-n'>output</span><span class='hljl-t'>
</span><span class='hljl-k'>end</span>
</pre>


<pre class="output">
gpu_transpose_noconf &#40;generic function with 2 methods&#41;
</pre>



<pre class='hljl'>
<span class='hljl-n'>A</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nf'>rand</span><span class='hljl-p'>(</span><span class='hljl-ni'>10000</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>10000</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-n'>B</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>similar</span><span class='hljl-p'>(</span><span class='hljl-n'>A</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-nd'>@benchmark</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nd'>@sync</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_transpose_noconf</span><span class='hljl-p'>(</span><span class='hljl-n'>A</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>B</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
BenchmarkTools.Trial: 
  memory estimate:  320 bytes
  allocs estimate:  9
  --------------
  minimum time:     6.020 ms &#40;0.00&#37; GC&#41;
  median time:      6.240 ms &#40;0.00&#37; GC&#41;
  mean time:        6.324 ms &#40;0.00&#37; GC&#41;
  maximum time:     6.695 ms &#40;0.00&#37; GC&#41;
  --------------
  samples:          790
  evals/sample:     1
</pre>



<pre class='hljl'>
<span class='hljl-n'>A</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nf'>rand</span><span class='hljl-p'>(</span><span class='hljl-ni'>10000</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-ni'>10000</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-n'>B</span><span class='hljl-t'> </span><span class='hljl-oB'>=</span><span class='hljl-t'> </span><span class='hljl-nf'>similar</span><span class='hljl-p'>(</span><span class='hljl-n'>A</span><span class='hljl-p'>)</span><span class='hljl-t'>
</span><span class='hljl-nd'>@benchmark</span><span class='hljl-t'> </span><span class='hljl-n'>CUDA</span><span class='hljl-oB'>.</span><span class='hljl-nd'>@sync</span><span class='hljl-t'> </span><span class='hljl-nf'>gpu_transpose_shmem</span><span class='hljl-p'>(</span><span class='hljl-n'>A</span><span class='hljl-p'>,</span><span class='hljl-t'> </span><span class='hljl-n'>B</span><span class='hljl-p'>)</span>
</pre>


<pre class="output">
BenchmarkTools.Trial: 
  memory estimate:  320 bytes
  allocs estimate:  9
  --------------
  minimum time:     7.453 ms &#40;0.00&#37; GC&#41;
  median time:      7.674 ms &#40;0.00&#37; GC&#41;
  mean time:        7.787 ms &#40;0.00&#37; GC&#41;
  maximum time:     8.423 ms &#40;0.00&#37; GC&#41;
  --------------
  samples:          642
  evals/sample:     1
</pre>



        <HR/>
        <div class="footer">
          <p>
            Published from <a href="03-shared_memory_array_reversal_and_matrix_transpose.jmd">03-shared_memory_array_reversal_and_matrix_transpose.jmd</a>
            using <a href="http://github.com/JunoLab/Weave.jl">Weave.jl</a> v0.10.6 on 2020-10-26.
          </p>
        </div>
      </div>
    </div>
  </div>
</BODY>

</HTML>
