var documenterSearchIndex = {"docs":
[{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"EditURL = \"<unknown>/../src/tutorials/introduction/03-Shared_Memory.jl\"","category":"page"},{"location":"introduction/03-Shared_Memory.html#Shared-memory","page":"Shared Memory","title":"Shared memory","text":"","category":"section"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"Shared memory is the memory in a SM(symmetric multiprocessor) which is accessable to all threads running on the SM. It is much faster than global memory being much closer in proximity. The amount of shared memory available depends on the compute capability of the GPU. Increasing the amount of shared memory may reduce occupancy.","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"sync_threads() is a function which adds a barrier for all threads in a thread block. A barrier ensures that all threads which belong to it stall once they reach it until all other threads reach the barrier. This is commonly used as a synchronization mechanism to eliminate race conditions.","category":"page"},{"location":"introduction/03-Shared_Memory.html#Array-Reversal","page":"Shared Memory","title":"Array Reversal","text":"","category":"section"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"Our job is to reverse an array, i.e 1 2 3 rightarrow 3 2 1.","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"using CUDA, BenchmarkTools\n\nfunction reverse(input, output = similar(input))\n    len = length(input)\n    for i = 1:cld(len,2)\n        output[i], output[len - i + 1] = input[len - i + 1], input[i]\n    end\n    output\nend","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"reverse (generic function with 2 methods)","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"reverse([1, 2, 3, 4, 5])","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"5-element Array{Int64,1}:\n 5\n 4\n 3\n 2\n 1","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"function gpu_reverse(input, output)\n    tid = threadIdx().x\n    len = length(input)\n    if tid <= cld(len, 2)\n        output[tid], output[len - tid + 1] = input[len - tid + 1], input[tid]\n    end\n    return\nend","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"gpu_reverse (generic function with 1 method)","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"A = CuArray(collect(1:5))\nB = similar(A)\n@cuda blocks=1 threads=length(A) gpu_reverse(A, B)\nB","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"5-element CUDA.CuArray{Int64,1}:\n 5\n 4\n 3\n 2\n 1","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"There are two ways to declare shared memory: Statically and Dynamically. We declaring it statically when the amount we need is the same amount for all kernel launches and known when writing the kernel. In the other case we declare it dynamically and specify it while launching with the @cuda macro using the shmem argument.","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"@doc @cuStaticSharedMem","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"@cuStaticSharedMem(T::Type, dims) -> CuDeviceArray{T,AS.Shared}","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"Get an array of type T and dimensions dims (either an integer length or tuple shape) pointing to a statically-allocated piece of shared memory. The type should be statically inferable and the dimensions should be constant, or an error will be thrown and the generator function will be called dynamically.","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"function gpu_stshmemreverse(input, output)\n    # Maximum size of array is 64\n    shmem = @cuStaticSharedMem(eltype(output), 64)\n    tid = threadIdx().x\n    len = length(input)\n    shmem[tid] = input[len - tid + 1]\n    output[tid] = shmem[tid]\n    return\nend","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"gpu_stshmemreverse (generic function with 1 method)","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"A = CuArray(collect(1:32))\nB = similar(A)\n@cuda blocks=1 threads=length(A) gpu_stshmemreverse(A, B)\nprint(B)","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"[32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1]","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"When the amount of shared memory required isn't known while writing the kernel overallocating is not a good idea because that may potentially reduce our occupancy. As an SM's resource usage increases it's occupancy goes down. Hence, it's best to use dynamically allocated memory when memory usage can only be known at launch time.","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"@doc @cuDynamicSharedMem","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"@cuDynamicSharedMem(T::Type, dims, offset::Integer=0) -> CuDeviceArray{T,AS.Shared}","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"Get an array of type T and dimensions dims (either an integer length or tuple shape) pointing to a dynamically-allocated piece of shared memory. The type should be statically inferable or an error will be thrown and the generator function will be called dynamically.","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"Note that the amount of dynamic shared memory needs to specified when launching the kernel.","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"Optionally, an offset parameter indicating how many bytes to add to the base shared memory pointer can be specified. This is useful when dealing with a heterogeneous buffer of dynamic shared memory; in the case of a homogeneous multi-part buffer it is preferred to use view.","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"function gpu_dyshmemreverse(input, output)\n    shmem = @cuDynamicSharedMem(eltype(output), (length(output),))\n    tid = threadIdx().x\n    len = length(input)\n    shmem[tid] = input[len - tid + 1]\n    output[tid] = shmem[tid]\n    return\nend","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"gpu_dyshmemreverse (generic function with 1 method)","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"C = CuArray(collect(1:32))\nD = similar(C)\n@cuda blocks=1 threads=length(C) shmem=length(C) gpu_dyshmemreverse(C, D)\nprint(D)","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"[32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1]","category":"page"},{"location":"introduction/03-Shared_Memory.html#Matrix-Transpose","page":"Shared Memory","title":"Matrix Transpose","text":"","category":"section"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"Matrix transpose is an operation which flips a matrix along its main diagonal.","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"(Image: transpose)","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"A = reshape(1:9, (3, 3))","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"3×3 reshape(::UnitRange{Int64}, 3, 3) with eltype Int64:\n 1  4  7\n 2  5  8\n 3  6  9","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"A'","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"3×3 LinearAlgebra.Adjoint{Int64,Base.ReshapedArray{Int64,2,UnitRange{Int64},Tuple{}}}:\n 1  2  3\n 4  5  6\n 7  8  9","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"A'' ## (A')'","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"3×3 reshape(::UnitRange{Int64}, 3, 3) with eltype Int64:\n 1  4  7\n 2  5  8\n 3  6  9","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"Ignore the types that Julia returns. LinearAlgebra.Adjoint is a wrapper that uses lazy evaluation to compute the result as required.","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"# CPU implementation\nfunction cpu_transpose(input, output = similar(input, (size(input, 2), size(input, 1))))\n    # the dimensions of the resultant matrix are reversed\n    for index = CartesianIndices(input)\n            output[index[2], index[1]] = input[index]\n    end\n    output\nend","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"cpu_transpose (generic function with 2 methods)","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"A = reshape(1:20, (4, 5))","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"4×5 reshape(::UnitRange{Int64}, 4, 5) with eltype Int64:\n 1  5   9  13  17\n 2  6  10  14  18\n 3  7  11  15  19\n 4  8  12  16  20","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"cpu_transpose(A)","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"5×4 Array{Int64,2}:\n  1   2   3   4\n  5   6   7   8\n  9  10  11  12\n 13  14  15  16\n 17  18  19  20","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"Before we begin working on the GPU consider the following code.","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"A = CuArray(reshape(1.:9, (3, 3)))\n\nprintln(\"A => \", pointer(A))\nCUDA.@allowscalar begin\n    for i in eachindex(A)\n        println(i, \" \", A[i], \" \", pointer(A, i))\n    end\nend","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"A => CUDA.CuPtr{Float64}(0x00007f7066831a00)\n1 1.0 CUDA.CuPtr{Float64}(0x00007f7066831a00)\n2 2.0 CUDA.CuPtr{Float64}(0x00007f7066831a08)\n3 3.0 CUDA.CuPtr{Float64}(0x00007f7066831a10)\n4 4.0 CUDA.CuPtr{Float64}(0x00007f7066831a18)\n5 5.0 CUDA.CuPtr{Float64}(0x00007f7066831a20)\n6 6.0 CUDA.CuPtr{Float64}(0x00007f7066831a28)\n7 7.0 CUDA.CuPtr{Float64}(0x00007f7066831a30)\n8 8.0 CUDA.CuPtr{Float64}(0x00007f7066831a38)\n9 9.0 CUDA.CuPtr{Float64}(0x00007f7066831a40)\n","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"Notice how consecutive elements are in the same column rather than the same row. This is because Julia stores its multidimensional arrays in a column major order like Fortran. In contrast to C/C++ which are row-major languages. The reason for making Julia's arrays column major is because a lot of linear algebra libraries are column major to begin with (https://discourse.julialang.org/t/why-column-major/24374/3).","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"# To index our 2-D array we will split the input into tiles of 32x32 elements.\n# Each thread block will launch with 32x8 = 256 threads\n# Each thread will work on 4 elements.\nconst TILE_DIM = 32\n\nfunction gpu_transpose_kernel(input, output)\n    tile_index = ((blockIdx().y, blockIdx().x) .- 1) .* TILE_DIM\n\n    # each thread manages 4 rows (8x4 = 32)\n    for i in 1:4\n        thread_index = (threadIdx().y + (i - 1)*8, threadIdx().x)\n        index = CartesianIndex(tile_index .+ thread_index)\n        (index[1] > size(input, 1) || index[2] > size(input, 2)) && continue\n        @inbounds output[index] = input[index[2], index[1]]\n    end\n\n    return\nend","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"gpu_transpose_kernel (generic function with 1 method)","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"function gpu_transpose(input, output = similar(input, (size(input, 2), size(input, 1))))\n    threads = (32, 8)\n    blocks = cld.(size(input), (32, 32))\n    @cuda blocks=blocks threads=threads gpu_transpose_kernel(input, output)\n    output\nend","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"gpu_transpose (generic function with 2 methods)","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"A = CuArray(reshape(1f0:1089, 33, 33))","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"33×33 CUDA.CuArray{Float32,2}:\n  1.0  34.0  67.0  100.0  133.0  166.0  199.0  232.0  265.0  298.0  331.0  364.0  397.0  430.0  463.0  496.0  529.0  562.0  595.0  628.0  661.0  694.0  727.0  760.0  793.0  826.0  859.0  892.0  925.0  958.0   991.0  1024.0  1057.0\n  2.0  35.0  68.0  101.0  134.0  167.0  200.0  233.0  266.0  299.0  332.0  365.0  398.0  431.0  464.0  497.0  530.0  563.0  596.0  629.0  662.0  695.0  728.0  761.0  794.0  827.0  860.0  893.0  926.0  959.0   992.0  1025.0  1058.0\n  3.0  36.0  69.0  102.0  135.0  168.0  201.0  234.0  267.0  300.0  333.0  366.0  399.0  432.0  465.0  498.0  531.0  564.0  597.0  630.0  663.0  696.0  729.0  762.0  795.0  828.0  861.0  894.0  927.0  960.0   993.0  1026.0  1059.0\n  4.0  37.0  70.0  103.0  136.0  169.0  202.0  235.0  268.0  301.0  334.0  367.0  400.0  433.0  466.0  499.0  532.0  565.0  598.0  631.0  664.0  697.0  730.0  763.0  796.0  829.0  862.0  895.0  928.0  961.0   994.0  1027.0  1060.0\n  5.0  38.0  71.0  104.0  137.0  170.0  203.0  236.0  269.0  302.0  335.0  368.0  401.0  434.0  467.0  500.0  533.0  566.0  599.0  632.0  665.0  698.0  731.0  764.0  797.0  830.0  863.0  896.0  929.0  962.0   995.0  1028.0  1061.0\n  6.0  39.0  72.0  105.0  138.0  171.0  204.0  237.0  270.0  303.0  336.0  369.0  402.0  435.0  468.0  501.0  534.0  567.0  600.0  633.0  666.0  699.0  732.0  765.0  798.0  831.0  864.0  897.0  930.0  963.0   996.0  1029.0  1062.0\n  7.0  40.0  73.0  106.0  139.0  172.0  205.0  238.0  271.0  304.0  337.0  370.0  403.0  436.0  469.0  502.0  535.0  568.0  601.0  634.0  667.0  700.0  733.0  766.0  799.0  832.0  865.0  898.0  931.0  964.0   997.0  1030.0  1063.0\n  8.0  41.0  74.0  107.0  140.0  173.0  206.0  239.0  272.0  305.0  338.0  371.0  404.0  437.0  470.0  503.0  536.0  569.0  602.0  635.0  668.0  701.0  734.0  767.0  800.0  833.0  866.0  899.0  932.0  965.0   998.0  1031.0  1064.0\n  9.0  42.0  75.0  108.0  141.0  174.0  207.0  240.0  273.0  306.0  339.0  372.0  405.0  438.0  471.0  504.0  537.0  570.0  603.0  636.0  669.0  702.0  735.0  768.0  801.0  834.0  867.0  900.0  933.0  966.0   999.0  1032.0  1065.0\n 10.0  43.0  76.0  109.0  142.0  175.0  208.0  241.0  274.0  307.0  340.0  373.0  406.0  439.0  472.0  505.0  538.0  571.0  604.0  637.0  670.0  703.0  736.0  769.0  802.0  835.0  868.0  901.0  934.0  967.0  1000.0  1033.0  1066.0\n 11.0  44.0  77.0  110.0  143.0  176.0  209.0  242.0  275.0  308.0  341.0  374.0  407.0  440.0  473.0  506.0  539.0  572.0  605.0  638.0  671.0  704.0  737.0  770.0  803.0  836.0  869.0  902.0  935.0  968.0  1001.0  1034.0  1067.0\n 12.0  45.0  78.0  111.0  144.0  177.0  210.0  243.0  276.0  309.0  342.0  375.0  408.0  441.0  474.0  507.0  540.0  573.0  606.0  639.0  672.0  705.0  738.0  771.0  804.0  837.0  870.0  903.0  936.0  969.0  1002.0  1035.0  1068.0\n 13.0  46.0  79.0  112.0  145.0  178.0  211.0  244.0  277.0  310.0  343.0  376.0  409.0  442.0  475.0  508.0  541.0  574.0  607.0  640.0  673.0  706.0  739.0  772.0  805.0  838.0  871.0  904.0  937.0  970.0  1003.0  1036.0  1069.0\n 14.0  47.0  80.0  113.0  146.0  179.0  212.0  245.0  278.0  311.0  344.0  377.0  410.0  443.0  476.0  509.0  542.0  575.0  608.0  641.0  674.0  707.0  740.0  773.0  806.0  839.0  872.0  905.0  938.0  971.0  1004.0  1037.0  1070.0\n 15.0  48.0  81.0  114.0  147.0  180.0  213.0  246.0  279.0  312.0  345.0  378.0  411.0  444.0  477.0  510.0  543.0  576.0  609.0  642.0  675.0  708.0  741.0  774.0  807.0  840.0  873.0  906.0  939.0  972.0  1005.0  1038.0  1071.0\n 16.0  49.0  82.0  115.0  148.0  181.0  214.0  247.0  280.0  313.0  346.0  379.0  412.0  445.0  478.0  511.0  544.0  577.0  610.0  643.0  676.0  709.0  742.0  775.0  808.0  841.0  874.0  907.0  940.0  973.0  1006.0  1039.0  1072.0\n 17.0  50.0  83.0  116.0  149.0  182.0  215.0  248.0  281.0  314.0  347.0  380.0  413.0  446.0  479.0  512.0  545.0  578.0  611.0  644.0  677.0  710.0  743.0  776.0  809.0  842.0  875.0  908.0  941.0  974.0  1007.0  1040.0  1073.0\n 18.0  51.0  84.0  117.0  150.0  183.0  216.0  249.0  282.0  315.0  348.0  381.0  414.0  447.0  480.0  513.0  546.0  579.0  612.0  645.0  678.0  711.0  744.0  777.0  810.0  843.0  876.0  909.0  942.0  975.0  1008.0  1041.0  1074.0\n 19.0  52.0  85.0  118.0  151.0  184.0  217.0  250.0  283.0  316.0  349.0  382.0  415.0  448.0  481.0  514.0  547.0  580.0  613.0  646.0  679.0  712.0  745.0  778.0  811.0  844.0  877.0  910.0  943.0  976.0  1009.0  1042.0  1075.0\n 20.0  53.0  86.0  119.0  152.0  185.0  218.0  251.0  284.0  317.0  350.0  383.0  416.0  449.0  482.0  515.0  548.0  581.0  614.0  647.0  680.0  713.0  746.0  779.0  812.0  845.0  878.0  911.0  944.0  977.0  1010.0  1043.0  1076.0\n 21.0  54.0  87.0  120.0  153.0  186.0  219.0  252.0  285.0  318.0  351.0  384.0  417.0  450.0  483.0  516.0  549.0  582.0  615.0  648.0  681.0  714.0  747.0  780.0  813.0  846.0  879.0  912.0  945.0  978.0  1011.0  1044.0  1077.0\n 22.0  55.0  88.0  121.0  154.0  187.0  220.0  253.0  286.0  319.0  352.0  385.0  418.0  451.0  484.0  517.0  550.0  583.0  616.0  649.0  682.0  715.0  748.0  781.0  814.0  847.0  880.0  913.0  946.0  979.0  1012.0  1045.0  1078.0\n 23.0  56.0  89.0  122.0  155.0  188.0  221.0  254.0  287.0  320.0  353.0  386.0  419.0  452.0  485.0  518.0  551.0  584.0  617.0  650.0  683.0  716.0  749.0  782.0  815.0  848.0  881.0  914.0  947.0  980.0  1013.0  1046.0  1079.0\n 24.0  57.0  90.0  123.0  156.0  189.0  222.0  255.0  288.0  321.0  354.0  387.0  420.0  453.0  486.0  519.0  552.0  585.0  618.0  651.0  684.0  717.0  750.0  783.0  816.0  849.0  882.0  915.0  948.0  981.0  1014.0  1047.0  1080.0\n 25.0  58.0  91.0  124.0  157.0  190.0  223.0  256.0  289.0  322.0  355.0  388.0  421.0  454.0  487.0  520.0  553.0  586.0  619.0  652.0  685.0  718.0  751.0  784.0  817.0  850.0  883.0  916.0  949.0  982.0  1015.0  1048.0  1081.0\n 26.0  59.0  92.0  125.0  158.0  191.0  224.0  257.0  290.0  323.0  356.0  389.0  422.0  455.0  488.0  521.0  554.0  587.0  620.0  653.0  686.0  719.0  752.0  785.0  818.0  851.0  884.0  917.0  950.0  983.0  1016.0  1049.0  1082.0\n 27.0  60.0  93.0  126.0  159.0  192.0  225.0  258.0  291.0  324.0  357.0  390.0  423.0  456.0  489.0  522.0  555.0  588.0  621.0  654.0  687.0  720.0  753.0  786.0  819.0  852.0  885.0  918.0  951.0  984.0  1017.0  1050.0  1083.0\n 28.0  61.0  94.0  127.0  160.0  193.0  226.0  259.0  292.0  325.0  358.0  391.0  424.0  457.0  490.0  523.0  556.0  589.0  622.0  655.0  688.0  721.0  754.0  787.0  820.0  853.0  886.0  919.0  952.0  985.0  1018.0  1051.0  1084.0\n 29.0  62.0  95.0  128.0  161.0  194.0  227.0  260.0  293.0  326.0  359.0  392.0  425.0  458.0  491.0  524.0  557.0  590.0  623.0  656.0  689.0  722.0  755.0  788.0  821.0  854.0  887.0  920.0  953.0  986.0  1019.0  1052.0  1085.0\n 30.0  63.0  96.0  129.0  162.0  195.0  228.0  261.0  294.0  327.0  360.0  393.0  426.0  459.0  492.0  525.0  558.0  591.0  624.0  657.0  690.0  723.0  756.0  789.0  822.0  855.0  888.0  921.0  954.0  987.0  1020.0  1053.0  1086.0\n 31.0  64.0  97.0  130.0  163.0  196.0  229.0  262.0  295.0  328.0  361.0  394.0  427.0  460.0  493.0  526.0  559.0  592.0  625.0  658.0  691.0  724.0  757.0  790.0  823.0  856.0  889.0  922.0  955.0  988.0  1021.0  1054.0  1087.0\n 32.0  65.0  98.0  131.0  164.0  197.0  230.0  263.0  296.0  329.0  362.0  395.0  428.0  461.0  494.0  527.0  560.0  593.0  626.0  659.0  692.0  725.0  758.0  791.0  824.0  857.0  890.0  923.0  956.0  989.0  1022.0  1055.0  1088.0\n 33.0  66.0  99.0  132.0  165.0  198.0  231.0  264.0  297.0  330.0  363.0  396.0  429.0  462.0  495.0  528.0  561.0  594.0  627.0  660.0  693.0  726.0  759.0  792.0  825.0  858.0  891.0  924.0  957.0  990.0  1023.0  1056.0  1089.0","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"gpu_transpose(A)","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"33×33 CUDA.CuArray{Float32,2}:\n    1.0     2.0     3.0     4.0     5.0     6.0     7.0     8.0     9.0    10.0    11.0    12.0    13.0    14.0    15.0    16.0    17.0    18.0    19.0    20.0    21.0    22.0    23.0    24.0    25.0    26.0    27.0    28.0    29.0    30.0    31.0    32.0    33.0\n   34.0    35.0    36.0    37.0    38.0    39.0    40.0    41.0    42.0    43.0    44.0    45.0    46.0    47.0    48.0    49.0    50.0    51.0    52.0    53.0    54.0    55.0    56.0    57.0    58.0    59.0    60.0    61.0    62.0    63.0    64.0    65.0    66.0\n   67.0    68.0    69.0    70.0    71.0    72.0    73.0    74.0    75.0    76.0    77.0    78.0    79.0    80.0    81.0    82.0    83.0    84.0    85.0    86.0    87.0    88.0    89.0    90.0    91.0    92.0    93.0    94.0    95.0    96.0    97.0    98.0    99.0\n  100.0   101.0   102.0   103.0   104.0   105.0   106.0   107.0   108.0   109.0   110.0   111.0   112.0   113.0   114.0   115.0   116.0   117.0   118.0   119.0   120.0   121.0   122.0   123.0   124.0   125.0   126.0   127.0   128.0   129.0   130.0   131.0   132.0\n  133.0   134.0   135.0   136.0   137.0   138.0   139.0   140.0   141.0   142.0   143.0   144.0   145.0   146.0   147.0   148.0   149.0   150.0   151.0   152.0   153.0   154.0   155.0   156.0   157.0   158.0   159.0   160.0   161.0   162.0   163.0   164.0   165.0\n  166.0   167.0   168.0   169.0   170.0   171.0   172.0   173.0   174.0   175.0   176.0   177.0   178.0   179.0   180.0   181.0   182.0   183.0   184.0   185.0   186.0   187.0   188.0   189.0   190.0   191.0   192.0   193.0   194.0   195.0   196.0   197.0   198.0\n  199.0   200.0   201.0   202.0   203.0   204.0   205.0   206.0   207.0   208.0   209.0   210.0   211.0   212.0   213.0   214.0   215.0   216.0   217.0   218.0   219.0   220.0   221.0   222.0   223.0   224.0   225.0   226.0   227.0   228.0   229.0   230.0   231.0\n  232.0   233.0   234.0   235.0   236.0   237.0   238.0   239.0   240.0   241.0   242.0   243.0   244.0   245.0   246.0   247.0   248.0   249.0   250.0   251.0   252.0   253.0   254.0   255.0   256.0   257.0   258.0   259.0   260.0   261.0   262.0   263.0   264.0\n  265.0   266.0   267.0   268.0   269.0   270.0   271.0   272.0   273.0   274.0   275.0   276.0   277.0   278.0   279.0   280.0   281.0   282.0   283.0   284.0   285.0   286.0   287.0   288.0   289.0   290.0   291.0   292.0   293.0   294.0   295.0   296.0   297.0\n  298.0   299.0   300.0   301.0   302.0   303.0   304.0   305.0   306.0   307.0   308.0   309.0   310.0   311.0   312.0   313.0   314.0   315.0   316.0   317.0   318.0   319.0   320.0   321.0   322.0   323.0   324.0   325.0   326.0   327.0   328.0   329.0   330.0\n  331.0   332.0   333.0   334.0   335.0   336.0   337.0   338.0   339.0   340.0   341.0   342.0   343.0   344.0   345.0   346.0   347.0   348.0   349.0   350.0   351.0   352.0   353.0   354.0   355.0   356.0   357.0   358.0   359.0   360.0   361.0   362.0   363.0\n  364.0   365.0   366.0   367.0   368.0   369.0   370.0   371.0   372.0   373.0   374.0   375.0   376.0   377.0   378.0   379.0   380.0   381.0   382.0   383.0   384.0   385.0   386.0   387.0   388.0   389.0   390.0   391.0   392.0   393.0   394.0   395.0   396.0\n  397.0   398.0   399.0   400.0   401.0   402.0   403.0   404.0   405.0   406.0   407.0   408.0   409.0   410.0   411.0   412.0   413.0   414.0   415.0   416.0   417.0   418.0   419.0   420.0   421.0   422.0   423.0   424.0   425.0   426.0   427.0   428.0   429.0\n  430.0   431.0   432.0   433.0   434.0   435.0   436.0   437.0   438.0   439.0   440.0   441.0   442.0   443.0   444.0   445.0   446.0   447.0   448.0   449.0   450.0   451.0   452.0   453.0   454.0   455.0   456.0   457.0   458.0   459.0   460.0   461.0   462.0\n  463.0   464.0   465.0   466.0   467.0   468.0   469.0   470.0   471.0   472.0   473.0   474.0   475.0   476.0   477.0   478.0   479.0   480.0   481.0   482.0   483.0   484.0   485.0   486.0   487.0   488.0   489.0   490.0   491.0   492.0   493.0   494.0   495.0\n  496.0   497.0   498.0   499.0   500.0   501.0   502.0   503.0   504.0   505.0   506.0   507.0   508.0   509.0   510.0   511.0   512.0   513.0   514.0   515.0   516.0   517.0   518.0   519.0   520.0   521.0   522.0   523.0   524.0   525.0   526.0   527.0   528.0\n  529.0   530.0   531.0   532.0   533.0   534.0   535.0   536.0   537.0   538.0   539.0   540.0   541.0   542.0   543.0   544.0   545.0   546.0   547.0   548.0   549.0   550.0   551.0   552.0   553.0   554.0   555.0   556.0   557.0   558.0   559.0   560.0   561.0\n  562.0   563.0   564.0   565.0   566.0   567.0   568.0   569.0   570.0   571.0   572.0   573.0   574.0   575.0   576.0   577.0   578.0   579.0   580.0   581.0   582.0   583.0   584.0   585.0   586.0   587.0   588.0   589.0   590.0   591.0   592.0   593.0   594.0\n  595.0   596.0   597.0   598.0   599.0   600.0   601.0   602.0   603.0   604.0   605.0   606.0   607.0   608.0   609.0   610.0   611.0   612.0   613.0   614.0   615.0   616.0   617.0   618.0   619.0   620.0   621.0   622.0   623.0   624.0   625.0   626.0   627.0\n  628.0   629.0   630.0   631.0   632.0   633.0   634.0   635.0   636.0   637.0   638.0   639.0   640.0   641.0   642.0   643.0   644.0   645.0   646.0   647.0   648.0   649.0   650.0   651.0   652.0   653.0   654.0   655.0   656.0   657.0   658.0   659.0   660.0\n  661.0   662.0   663.0   664.0   665.0   666.0   667.0   668.0   669.0   670.0   671.0   672.0   673.0   674.0   675.0   676.0   677.0   678.0   679.0   680.0   681.0   682.0   683.0   684.0   685.0   686.0   687.0   688.0   689.0   690.0   691.0   692.0   693.0\n  694.0   695.0   696.0   697.0   698.0   699.0   700.0   701.0   702.0   703.0   704.0   705.0   706.0   707.0   708.0   709.0   710.0   711.0   712.0   713.0   714.0   715.0   716.0   717.0   718.0   719.0   720.0   721.0   722.0   723.0   724.0   725.0   726.0\n  727.0   728.0   729.0   730.0   731.0   732.0   733.0   734.0   735.0   736.0   737.0   738.0   739.0   740.0   741.0   742.0   743.0   744.0   745.0   746.0   747.0   748.0   749.0   750.0   751.0   752.0   753.0   754.0   755.0   756.0   757.0   758.0   759.0\n  760.0   761.0   762.0   763.0   764.0   765.0   766.0   767.0   768.0   769.0   770.0   771.0   772.0   773.0   774.0   775.0   776.0   777.0   778.0   779.0   780.0   781.0   782.0   783.0   784.0   785.0   786.0   787.0   788.0   789.0   790.0   791.0   792.0\n  793.0   794.0   795.0   796.0   797.0   798.0   799.0   800.0   801.0   802.0   803.0   804.0   805.0   806.0   807.0   808.0   809.0   810.0   811.0   812.0   813.0   814.0   815.0   816.0   817.0   818.0   819.0   820.0   821.0   822.0   823.0   824.0   825.0\n  826.0   827.0   828.0   829.0   830.0   831.0   832.0   833.0   834.0   835.0   836.0   837.0   838.0   839.0   840.0   841.0   842.0   843.0   844.0   845.0   846.0   847.0   848.0   849.0   850.0   851.0   852.0   853.0   854.0   855.0   856.0   857.0   858.0\n  859.0   860.0   861.0   862.0   863.0   864.0   865.0   866.0   867.0   868.0   869.0   870.0   871.0   872.0   873.0   874.0   875.0   876.0   877.0   878.0   879.0   880.0   881.0   882.0   883.0   884.0   885.0   886.0   887.0   888.0   889.0   890.0   891.0\n  892.0   893.0   894.0   895.0   896.0   897.0   898.0   899.0   900.0   901.0   902.0   903.0   904.0   905.0   906.0   907.0   908.0   909.0   910.0   911.0   912.0   913.0   914.0   915.0   916.0   917.0   918.0   919.0   920.0   921.0   922.0   923.0   924.0\n  925.0   926.0   927.0   928.0   929.0   930.0   931.0   932.0   933.0   934.0   935.0   936.0   937.0   938.0   939.0   940.0   941.0   942.0   943.0   944.0   945.0   946.0   947.0   948.0   949.0   950.0   951.0   952.0   953.0   954.0   955.0   956.0   957.0\n  958.0   959.0   960.0   961.0   962.0   963.0   964.0   965.0   966.0   967.0   968.0   969.0   970.0   971.0   972.0   973.0   974.0   975.0   976.0   977.0   978.0   979.0   980.0   981.0   982.0   983.0   984.0   985.0   986.0   987.0   988.0   989.0   990.0\n  991.0   992.0   993.0   994.0   995.0   996.0   997.0   998.0   999.0  1000.0  1001.0  1002.0  1003.0  1004.0  1005.0  1006.0  1007.0  1008.0  1009.0  1010.0  1011.0  1012.0  1013.0  1014.0  1015.0  1016.0  1017.0  1018.0  1019.0  1020.0  1021.0  1022.0  1023.0\n 1024.0  1025.0  1026.0  1027.0  1028.0  1029.0  1030.0  1031.0  1032.0  1033.0  1034.0  1035.0  1036.0  1037.0  1038.0  1039.0  1040.0  1041.0  1042.0  1043.0  1044.0  1045.0  1046.0  1047.0  1048.0  1049.0  1050.0  1051.0  1052.0  1053.0  1054.0  1055.0  1056.0\n 1057.0  1058.0  1059.0  1060.0  1061.0  1062.0  1063.0  1064.0  1065.0  1066.0  1067.0  1068.0  1069.0  1070.0  1071.0  1072.0  1073.0  1074.0  1075.0  1076.0  1077.0  1078.0  1079.0  1080.0  1081.0  1082.0  1083.0  1084.0  1085.0  1086.0  1087.0  1088.0  1089.0","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"A = CUDA.rand(10000, 10000)\nB = similar(A)\n@benchmark CUDA.@sync gpu_transpose($A, $B)","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"BenchmarkTools.Trial: \n  memory estimate:  320 bytes\n  allocs estimate:  9\n  --------------\n  minimum time:     12.294 ms (0.00% GC)\n  median time:      12.439 ms (0.00% GC)\n  mean time:        12.488 ms (0.00% GC)\n  maximum time:     21.293 ms (0.00% GC)\n  --------------\n  samples:          401\n  evals/sample:     1","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"@benchmark CUDA.@sync $B .= $A","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"BenchmarkTools.Trial: \n  memory estimate:  464 bytes\n  allocs estimate:  13\n  --------------\n  minimum time:     7.312 ms (0.00% GC)\n  median time:      7.459 ms (0.00% GC)\n  mean time:        7.492 ms (0.00% GC)\n  maximum time:     14.024 ms (0.00% GC)\n  --------------\n  samples:          668\n  evals/sample:     1","category":"page"},{"location":"introduction/03-Shared_Memory.html#Coalescing-Memory-Access","page":"Shared Memory","title":"Coalescing Memory Access","text":"","category":"section"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"Compared to a simple elementwise copy we are roughly at 60% performance. Both kernels have a single load and store for each value. If all loads and stores were independent of each other then this should not have happened.","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"Consider a thread accessing(load or store) a single value in global memory. Instead of transferring just the one value the GPU will instead transfer a larger chunk of memory as a single transaction. For example on NVIDIA's K20 GPU this size was 128 bytes. When threads in a warp access consecutive memory addresses the GPU can service multiple threads in the same transaction. This is known as coalesced memory access. Access time is effectively reduced by minimizing the number of transactions. However when threads access non-sequentially or sparse data then transactions are serialised.","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"We want consecutive threads of a warp to access consecutive elements in memory. When the thread block is one-dimensional it is straightforward to determine a thread'swarpId i.e. warpId = threadId().x % warpsize(). According to NVIDIA's documentation on thread hierarchy.","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"The index of a thread and its thread ID relate to each other in a straightforward way: For a one-dimensional block, they are the same; for a two-dimensional block of size (Dx, Dy),the thread ID of a thread of index (x, y) is (x + y Dx); for a three-dimensional block of size (Dx, Dy, Dz), the thread ID of a thread of index (x, y, z) is (x + y Dx + z Dx Dy).","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"In our kernel there are four loads and stores per thread.","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"tile_index = ((blockIdx().y, blockIdx().x) .- 1) .* TILE_DIM\nthread_index = (threadIdx().y + (i - 1)*8, threadIdx().x)\nindex = CartesianIndex(tile_index .+ thread_index)\nLoad: input[index[2], index[1]]\nStore: output[index[1], index[2]]","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"The loads are coalesced because the column is indexed by index[2] which has threadIdx().x and the stores are non-coalesced because they are indexed by index[1] which has threadIdx().y.","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"To ensure coalescing during both loads and stores we will use shared memory. We will load from global memory a column and store it in shared memory as a row, effectively transposing it. Once all threads have written to shared memory we can write back to global memory column wise.","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"(Image: Coalesced transpose)","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"function gpu_transpose_kernel2(input, output)\n    # Declare shared memory\n    shared = @cuStaticSharedMem(eltype(input), (TILE_DIM, TILE_DIM))\n\n    # Modify thread index so threadIdx().x dominates the column\n    block_index = ((blockIdx().y, blockIdx().x) .- 1) .* TILE_DIM\n\n    for i in 1:4\n        thread_index = (threadIdx().x, threadIdx().y + (i - 1)*8)\n        index = CartesianIndex(block_index .+ thread_index)\n\n        (index[1] > size(input, 1) || index[2] > size(input, 2)) && continue\n        @inbounds shared[thread_index[2], thread_index[1]] = input[index]\n    end\n\n    # Barrier to ensure all threads have completed writing to shared memory\n    sync_threads()\n\n    # swap tile index\n    block_index = ((blockIdx().x, blockIdx().y) .- 1) .* TILE_DIM\n\n    for i in 1:4\n        thread_index = (threadIdx().x, threadIdx().y + (i - 1)*8)\n        index = CartesianIndex(block_index .+ thread_index)\n\n        (index[1] > size(output, 1) || index[2] > size(output, 2)) && continue\n        @inbounds output[index] = shared[thread_index...]\n    end\n    return\nend\n\nfunction gpu_transpose_shmem(input, output = similar(input, (size(input, 2), size(input, 1))))\n    threads = (32, 8)\n    blocks = cld.(size(input), (32, 32))\n    @cuda blocks=blocks threads=threads gpu_transpose_kernel2(input, output)\n    output\nend","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"gpu_transpose_shmem (generic function with 2 methods)","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"X = CuArray(reshape(1f0:1089, (33, 33)))\nY = similar(X)\ngpu_transpose_shmem(X, Y)","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"33×33 CUDA.CuArray{Float32,2}:\n    1.0     2.0     3.0     4.0     5.0     6.0     7.0     8.0     9.0    10.0    11.0    12.0    13.0    14.0    15.0    16.0    17.0    18.0    19.0    20.0    21.0    22.0    23.0    24.0    25.0    26.0    27.0    28.0    29.0    30.0    31.0    32.0    33.0\n   34.0    35.0    36.0    37.0    38.0    39.0    40.0    41.0    42.0    43.0    44.0    45.0    46.0    47.0    48.0    49.0    50.0    51.0    52.0    53.0    54.0    55.0    56.0    57.0    58.0    59.0    60.0    61.0    62.0    63.0    64.0    65.0    66.0\n   67.0    68.0    69.0    70.0    71.0    72.0    73.0    74.0    75.0    76.0    77.0    78.0    79.0    80.0    81.0    82.0    83.0    84.0    85.0    86.0    87.0    88.0    89.0    90.0    91.0    92.0    93.0    94.0    95.0    96.0    97.0    98.0    99.0\n  100.0   101.0   102.0   103.0   104.0   105.0   106.0   107.0   108.0   109.0   110.0   111.0   112.0   113.0   114.0   115.0   116.0   117.0   118.0   119.0   120.0   121.0   122.0   123.0   124.0   125.0   126.0   127.0   128.0   129.0   130.0   131.0   132.0\n  133.0   134.0   135.0   136.0   137.0   138.0   139.0   140.0   141.0   142.0   143.0   144.0   145.0   146.0   147.0   148.0   149.0   150.0   151.0   152.0   153.0   154.0   155.0   156.0   157.0   158.0   159.0   160.0   161.0   162.0   163.0   164.0   165.0\n  166.0   167.0   168.0   169.0   170.0   171.0   172.0   173.0   174.0   175.0   176.0   177.0   178.0   179.0   180.0   181.0   182.0   183.0   184.0   185.0   186.0   187.0   188.0   189.0   190.0   191.0   192.0   193.0   194.0   195.0   196.0   197.0   198.0\n  199.0   200.0   201.0   202.0   203.0   204.0   205.0   206.0   207.0   208.0   209.0   210.0   211.0   212.0   213.0   214.0   215.0   216.0   217.0   218.0   219.0   220.0   221.0   222.0   223.0   224.0   225.0   226.0   227.0   228.0   229.0   230.0   231.0\n  232.0   233.0   234.0   235.0   236.0   237.0   238.0   239.0   240.0   241.0   242.0   243.0   244.0   245.0   246.0   247.0   248.0   249.0   250.0   251.0   252.0   253.0   254.0   255.0   256.0   257.0   258.0   259.0   260.0   261.0   262.0   263.0   264.0\n  265.0   266.0   267.0   268.0   269.0   270.0   271.0   272.0   273.0   274.0   275.0   276.0   277.0   278.0   279.0   280.0   281.0   282.0   283.0   284.0   285.0   286.0   287.0   288.0   289.0   290.0   291.0   292.0   293.0   294.0   295.0   296.0   297.0\n  298.0   299.0   300.0   301.0   302.0   303.0   304.0   305.0   306.0   307.0   308.0   309.0   310.0   311.0   312.0   313.0   314.0   315.0   316.0   317.0   318.0   319.0   320.0   321.0   322.0   323.0   324.0   325.0   326.0   327.0   328.0   329.0   330.0\n  331.0   332.0   333.0   334.0   335.0   336.0   337.0   338.0   339.0   340.0   341.0   342.0   343.0   344.0   345.0   346.0   347.0   348.0   349.0   350.0   351.0   352.0   353.0   354.0   355.0   356.0   357.0   358.0   359.0   360.0   361.0   362.0   363.0\n  364.0   365.0   366.0   367.0   368.0   369.0   370.0   371.0   372.0   373.0   374.0   375.0   376.0   377.0   378.0   379.0   380.0   381.0   382.0   383.0   384.0   385.0   386.0   387.0   388.0   389.0   390.0   391.0   392.0   393.0   394.0   395.0   396.0\n  397.0   398.0   399.0   400.0   401.0   402.0   403.0   404.0   405.0   406.0   407.0   408.0   409.0   410.0   411.0   412.0   413.0   414.0   415.0   416.0   417.0   418.0   419.0   420.0   421.0   422.0   423.0   424.0   425.0   426.0   427.0   428.0   429.0\n  430.0   431.0   432.0   433.0   434.0   435.0   436.0   437.0   438.0   439.0   440.0   441.0   442.0   443.0   444.0   445.0   446.0   447.0   448.0   449.0   450.0   451.0   452.0   453.0   454.0   455.0   456.0   457.0   458.0   459.0   460.0   461.0   462.0\n  463.0   464.0   465.0   466.0   467.0   468.0   469.0   470.0   471.0   472.0   473.0   474.0   475.0   476.0   477.0   478.0   479.0   480.0   481.0   482.0   483.0   484.0   485.0   486.0   487.0   488.0   489.0   490.0   491.0   492.0   493.0   494.0   495.0\n  496.0   497.0   498.0   499.0   500.0   501.0   502.0   503.0   504.0   505.0   506.0   507.0   508.0   509.0   510.0   511.0   512.0   513.0   514.0   515.0   516.0   517.0   518.0   519.0   520.0   521.0   522.0   523.0   524.0   525.0   526.0   527.0   528.0\n  529.0   530.0   531.0   532.0   533.0   534.0   535.0   536.0   537.0   538.0   539.0   540.0   541.0   542.0   543.0   544.0   545.0   546.0   547.0   548.0   549.0   550.0   551.0   552.0   553.0   554.0   555.0   556.0   557.0   558.0   559.0   560.0   561.0\n  562.0   563.0   564.0   565.0   566.0   567.0   568.0   569.0   570.0   571.0   572.0   573.0   574.0   575.0   576.0   577.0   578.0   579.0   580.0   581.0   582.0   583.0   584.0   585.0   586.0   587.0   588.0   589.0   590.0   591.0   592.0   593.0   594.0\n  595.0   596.0   597.0   598.0   599.0   600.0   601.0   602.0   603.0   604.0   605.0   606.0   607.0   608.0   609.0   610.0   611.0   612.0   613.0   614.0   615.0   616.0   617.0   618.0   619.0   620.0   621.0   622.0   623.0   624.0   625.0   626.0   627.0\n  628.0   629.0   630.0   631.0   632.0   633.0   634.0   635.0   636.0   637.0   638.0   639.0   640.0   641.0   642.0   643.0   644.0   645.0   646.0   647.0   648.0   649.0   650.0   651.0   652.0   653.0   654.0   655.0   656.0   657.0   658.0   659.0   660.0\n  661.0   662.0   663.0   664.0   665.0   666.0   667.0   668.0   669.0   670.0   671.0   672.0   673.0   674.0   675.0   676.0   677.0   678.0   679.0   680.0   681.0   682.0   683.0   684.0   685.0   686.0   687.0   688.0   689.0   690.0   691.0   692.0   693.0\n  694.0   695.0   696.0   697.0   698.0   699.0   700.0   701.0   702.0   703.0   704.0   705.0   706.0   707.0   708.0   709.0   710.0   711.0   712.0   713.0   714.0   715.0   716.0   717.0   718.0   719.0   720.0   721.0   722.0   723.0   724.0   725.0   726.0\n  727.0   728.0   729.0   730.0   731.0   732.0   733.0   734.0   735.0   736.0   737.0   738.0   739.0   740.0   741.0   742.0   743.0   744.0   745.0   746.0   747.0   748.0   749.0   750.0   751.0   752.0   753.0   754.0   755.0   756.0   757.0   758.0   759.0\n  760.0   761.0   762.0   763.0   764.0   765.0   766.0   767.0   768.0   769.0   770.0   771.0   772.0   773.0   774.0   775.0   776.0   777.0   778.0   779.0   780.0   781.0   782.0   783.0   784.0   785.0   786.0   787.0   788.0   789.0   790.0   791.0   792.0\n  793.0   794.0   795.0   796.0   797.0   798.0   799.0   800.0   801.0   802.0   803.0   804.0   805.0   806.0   807.0   808.0   809.0   810.0   811.0   812.0   813.0   814.0   815.0   816.0   817.0   818.0   819.0   820.0   821.0   822.0   823.0   824.0   825.0\n  826.0   827.0   828.0   829.0   830.0   831.0   832.0   833.0   834.0   835.0   836.0   837.0   838.0   839.0   840.0   841.0   842.0   843.0   844.0   845.0   846.0   847.0   848.0   849.0   850.0   851.0   852.0   853.0   854.0   855.0   856.0   857.0   858.0\n  859.0   860.0   861.0   862.0   863.0   864.0   865.0   866.0   867.0   868.0   869.0   870.0   871.0   872.0   873.0   874.0   875.0   876.0   877.0   878.0   879.0   880.0   881.0   882.0   883.0   884.0   885.0   886.0   887.0   888.0   889.0   890.0   891.0\n  892.0   893.0   894.0   895.0   896.0   897.0   898.0   899.0   900.0   901.0   902.0   903.0   904.0   905.0   906.0   907.0   908.0   909.0   910.0   911.0   912.0   913.0   914.0   915.0   916.0   917.0   918.0   919.0   920.0   921.0   922.0   923.0   924.0\n  925.0   926.0   927.0   928.0   929.0   930.0   931.0   932.0   933.0   934.0   935.0   936.0   937.0   938.0   939.0   940.0   941.0   942.0   943.0   944.0   945.0   946.0   947.0   948.0   949.0   950.0   951.0   952.0   953.0   954.0   955.0   956.0   957.0\n  958.0   959.0   960.0   961.0   962.0   963.0   964.0   965.0   966.0   967.0   968.0   969.0   970.0   971.0   972.0   973.0   974.0   975.0   976.0   977.0   978.0   979.0   980.0   981.0   982.0   983.0   984.0   985.0   986.0   987.0   988.0   989.0   990.0\n  991.0   992.0   993.0   994.0   995.0   996.0   997.0   998.0   999.0  1000.0  1001.0  1002.0  1003.0  1004.0  1005.0  1006.0  1007.0  1008.0  1009.0  1010.0  1011.0  1012.0  1013.0  1014.0  1015.0  1016.0  1017.0  1018.0  1019.0  1020.0  1021.0  1022.0  1023.0\n 1024.0  1025.0  1026.0  1027.0  1028.0  1029.0  1030.0  1031.0  1032.0  1033.0  1034.0  1035.0  1036.0  1037.0  1038.0  1039.0  1040.0  1041.0  1042.0  1043.0  1044.0  1045.0  1046.0  1047.0  1048.0  1049.0  1050.0  1051.0  1052.0  1053.0  1054.0  1055.0  1056.0\n 1057.0  1058.0  1059.0  1060.0  1061.0  1062.0  1063.0  1064.0  1065.0  1066.0  1067.0  1068.0  1069.0  1070.0  1071.0  1072.0  1073.0  1074.0  1075.0  1076.0  1077.0  1078.0  1079.0  1080.0  1081.0  1082.0  1083.0  1084.0  1085.0  1086.0  1087.0  1088.0  1089.0","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"@benchmark CUDA.@sync gpu_transpose_shmem($A, $B)","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"BenchmarkTools.Trial: \n  memory estimate:  320 bytes\n  allocs estimate:  9\n  --------------\n  minimum time:     7.362 ms (0.00% GC)\n  median time:      7.496 ms (0.00% GC)\n  mean time:        7.520 ms (0.00% GC)\n  maximum time:     10.611 ms (0.00% GC)\n  --------------\n  samples:          665\n  evals/sample:     1","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"@benchmark CUDA.@sync B .= A","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"BenchmarkTools.Trial: \n  memory estimate:  464 bytes\n  allocs estimate:  13\n  --------------\n  minimum time:     7.398 ms (0.00% GC)\n  median time:      7.516 ms (0.00% GC)\n  mean time:        7.538 ms (0.00% GC)\n  maximum time:     9.225 ms (0.00% GC)\n  --------------\n  samples:          663\n  evals/sample:     1","category":"page"},{"location":"introduction/03-Shared_Memory.html#Shared-Memory-Bank-conflicts","page":"Shared Memory","title":"Shared Memory Bank conflicts","text":"","category":"section"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"Inside a SM, shared memory is divided into banks. Modern NVIDIA GPUs have 32 banks which have a 4-byte boundary. This means addresses 1-4 of shared memory are serviced by bank 1, addresses 5-8 are serviced by bank two and so on. When multiple threads access memory from the same bank then their requests are serialised.","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"Nsight compute gives statistics about shared memory usage. Running the profiler on gpu_transpose_shmem for an input of 33x33 of Float32 we get:","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"(Image: shared-memory-conflicts)","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"It reports zero conflicts during shared loads because of we load columnwise. The 1023 store conflicts can be explained as follows. When an entire column is read it is stored to a row. Consecutive elements in a row differ in address by column_length*sizeof(datatype). In 33 tile columns we write directly to a complete row where 32 elements are written hence there are 31 write conflicts (33*31 = 1023). CUDA.jl docs have a brief Nsight compute usage guide here.","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"The fix is quite simple, pad the column length in shared memory by 1. Now consecutive elements in a row will differ by 33 % 32 = 1 hence no more bank conflicts.","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"i.e. shared = @cuStaticSharedMem(eltype(input), (TILE_DIM + 1, TILE_DIM))","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"function gpu_transpose_kernel3(input, output)\n    # Declare shared memory\n    shared = @cuStaticSharedMem(eltype(input), (TILE_DIM + 1, TILE_DIM))\n\n    # Modify thread index so threadIdx().x dominates the column\n    block_index = ((blockIdx().y, blockIdx().x) .- 1) .* TILE_DIM\n\n    for i in 1:4\n        thread_index = (threadIdx().x, threadIdx().y + (i - 1)*8)\n        index = CartesianIndex(block_index .+ thread_index)\n\n        (index[1] > size(input, 1) || index[2] > size(input, 2)) && continue\n        @inbounds shared[thread_index[2], thread_index[1]] = input[index]\n    end\n\n    # Barrier to ensure all threads have completed writing to shared memory\n    sync_threads()\n\n    # swap tile index\n    block_index = ((blockIdx().x, blockIdx().y) .- 1) .* TILE_DIM\n\n    for i in 1:4\n        thread_index = (threadIdx().x, threadIdx().y + (i - 1)*8)\n        index = CartesianIndex(block_index .+ thread_index)\n\n        (index[1] > size(output, 1) || index[2] > size(output, 2)) && continue\n        @inbounds output[index] = shared[thread_index...]\n    end\n    return\nend\n\nfunction gpu_transpose_noconf(input, output = similar(input, (size(input, 2), size(input, 1))))\n    threads = (32, 8)\n    blocks = cld.(size(input), (32, 32))\n    @cuda blocks=blocks threads=threads gpu_transpose_kernel3(input, output)\n    output\nend","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"gpu_transpose_noconf (generic function with 2 methods)","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"@benchmark CUDA.@sync gpu_transpose_noconf($A, $B)","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"BenchmarkTools.Trial: \n  memory estimate:  320 bytes\n  allocs estimate:  9\n  --------------\n  minimum time:     6.034 ms (0.00% GC)\n  median time:      6.178 ms (0.00% GC)\n  mean time:        6.193 ms (0.00% GC)\n  maximum time:     9.271 ms (0.00% GC)\n  --------------\n  samples:          808\n  evals/sample:     1","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"@benchmark CUDA.@sync gpu_transpose_shmem($A, $B)","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"BenchmarkTools.Trial: \n  memory estimate:  320 bytes\n  allocs estimate:  9\n  --------------\n  minimum time:     7.387 ms (0.00% GC)\n  median time:      7.535 ms (0.00% GC)\n  mean time:        7.556 ms (0.00% GC)\n  maximum time:     8.623 ms (0.00% GC)\n  --------------\n  samples:          662\n  evals/sample:     1","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"An obvious improvement, we can also confirm with Nsight compute if there are no bank conflicts.","category":"page"},{"location":"introduction/03-Shared_Memory.html","page":"Shared Memory","title":"Shared Memory","text":"(Image: shmem-noconf)","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"EditURL = \"<unknown>/../src/tutorials/introduction/01-Introduction.jl\"","category":"page"},{"location":"introduction/01-Introduction.html#Introduction-to-GPU-Programming","page":"Introduction","title":"Introduction to GPU Programming","text":"","category":"section"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"The following tutorials assume that you have have setup CUDA.jl. Detailed installation instructions can be found here.","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"You may check if your CUDA.jl installation is functional using","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"# using Pkg\n# Pkg.add(\"CUDA\")\nusing CUDA\nCUDA.functional()","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"true","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"If CUDA.functional() returns false then the package is in a non-functional state and you should follow the documentation to get it working.","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"Also explained in the usage section of the CUDA.jl docs is an overview of CUDA.jl's functionality which can work at three distinct levels.","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"Array Abstractions: With the help of the CuArray type we can use Base's array abstractions like broadcasting and mapreduce.\nNative Kernels: Write kernels which compiles to native GPU code directly from Julia.\nCUDA API wrappers: Call CUDA libraries directly from Julia for bleeding edge performance.","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"The purpose of these tutorials is to teach you effective GPU programming. The tutorials here complement other GPU programming resources such as the NVIDIA blogs, other online resources and formal textbooks. Using other resources in your study will complement these tutorials and is highly encouraged. Some suggested resources are listed at the end of this tutorial.","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"A GPU (graphical processing unit) is a device specially designed for graphics work. Graphical tasks are a good candidate for parallelization and  GPU's exploit it by having a large number of less powerful processors instead of a single very powerful processor. In 2007 NVIDIA released CUDA (Compute Unified Device Architecture), a parallel programming platform (hardware and software stack) which alongside graphics also focusses scientific computation. Modern GPU's are commonly called GPGPU (general purpose GPU) which shows their importance in scientific computation alongside graphics.","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"Programs which execute on the GPU are vastly different due to its different architecture. There are new paradigms and algorithms to learn. Understanding how a GPU works is crucial to maximizing the performance of your application.","category":"page"},{"location":"introduction/01-Introduction.html#Parallelizing-AXPY","page":"Introduction","title":"Parallelizing AXPY","text":"","category":"section"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"Basic Linear Algebra Subroutines(BLAS) are subroutines for Linear Algebra operations. Linear algebra's importance in scientific computing makes BLAS essential to GPU computing. One of the most primitive BLAS operations is to scale a vector and add it to another vector. Given two vectors (x and y) and a scalar (alpha) we add alphacdot x to y. In BLAS libraries this manifests as the functions SAXPY, DAXPY and CAXPY. The difference between the three is that the data type of the vectors is Float32, Float64 and Complex{Float32} respectively. However in this example we call our subroutine axpy and let Julia take care of the types.","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"function axpy!(A, X, Y)\n    for i in eachindex(Y)\n        @inbounds Y[i] = A * X[i] + Y[i]\n    end\nend\n\nN = 2^27\nv1 = rand(Float32, N)\nv2 = rand(Float32, N)\nv2_copy = copy(v2) # maintain a copy of the original\nα = rand()\n\naxpy!(α, v1, v2)","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"Alternatively, we can also use Julia's broadcasting syntax which allows us to write it in simpler and equally performant version.","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"v3 = copy(v2_copy)\nv3 .+= α * v1\n\n@show v2 == v3","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"true","category":"page"},{"location":"introduction/01-Introduction.html#CPU-multithreaded-version","page":"Introduction","title":"CPU multithreaded version","text":"","category":"section"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"Consider parallelization on a CPU with p processors. We can divide our arrays into p subarrays of equal size and assign a processor to each subarray. This can theoretically make our parallel version p times faster. We say \"theoretically\" because there is an overhead of starting threads and synchronizing them. Our hope in parallel computing is that the cost will get amortized with the speedup of parallelization, but that may not be the case. Which is why measuring performance is extremely important. Nevertheless, the parallel version asymptotically scales linearly w.r.t p which is really good, so much so that these types of problems are classified as \"embarassingly parallel\". In other cases when processors need to communicate and synchronize frquently the benefit does not scale linearly with the number of processors.","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"We can use Julia's inbuilt multithreading functionality to use multiple CPU threads which is documented(here). You need to ensure that Julia starts with the appropriate number of threads using the environment variable or startup option(-t NUMTHREADS), instructions for which are given in the docs.","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"A common theme in parallel computing is the concept of thread rank or id. Each thread has a unique id/rank which helps us identify them and map them to tasks easily.","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"using Base.Threads\n\nprintln(\"Number of CPU threads = \", nthreads())\n\n# pseudocode for parallel saxpy\nfunction parallel_axpy!(A, X, Y)\n    len = cld(length(X), nthreads())\n\n    # Launch threads = nthreads()\n    Threads.@threads for i in 1:nthreads()\n        # set id to thread rank/id\n        tid = threadid()\n        low = 1 + (tid - 1)*len\n        high = min(length(X), len * tid) ## The last segment might have lesser elements than len\n\n        # Broadcast syntax, views used to avoid copying\n        view(Y, low:high) .+= A.*view(X, low:high)\n    end\n    return\nend\n\nv4 = copy(v2_copy)\nparallel_axpy!(α, v1, v4)\n\n@show v2 == v4","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"true","category":"page"},{"location":"introduction/01-Introduction.html#GPU-version","page":"Introduction","title":"GPU version","text":"","category":"section"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"Given below is the code for GPU","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"function gpu_axpy!(A, X, Y)\n    # set tid to thread rank\n    tid = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n    tid > length(Y) && return\n    @inbounds Y[tid] = A*X[tid] + Y[tid]\n    return\nend\n\n# Transfer array to GPU memory\ngpu_v1 = CuArray(v1)\ngpu_v2 = CuArray(v2_copy)\n\nnumthreads = 256\nnumblocks = cld(N, numthreads)\n\n@show numthreads\n@show numblocks\n\n# Launch the gpu_axpy! on the GPU\n@cuda threads=numthreads blocks=numblocks gpu_axpy!(α, gpu_v1, gpu_v2)\n\n# Copy back to RAM\nv4 = Array(gpu_v2)\n\n# Verify that the answers are the same\n@show v2 == v4","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"true","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"Compared to the CPU code there are a number of differences:","category":"page"},{"location":"introduction/01-Introduction.html#)-Thread-Indexing","page":"Introduction","title":"1) Thread Indexing","text":"","category":"section"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"The multithreaded CPU code used threadid() to get the current thread's rank whereas on the GPU a complicated expression tid = (blockIdx().x - 1) * blockDim().x + threadIdx().x computed rank. Furthermore, we are using two distinct terms, blocks and threads coupled with idx(index) and Dim (dimension).","category":"page"},{"location":"introduction/01-Introduction.html#)-SIMT-architecture","page":"Introduction","title":"2) SIMT architecture","text":"","category":"section"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"The multithreaded CPU code divided the array up into a handful of pieces equal to the number of processors. A modern consumer CPU has a handful of cores(4 - 8), hence each thread still works on a relatively large array whereas the GPU processes one element per thread. While both demonstrate parallelism their scales differ vastly.","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"Flynn's taxonomy is a popular way to classify parallel computer architectures.","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"|                      | single data | multiple data | |:––––––––––-|:––––––|:–––––––| | single instruction   | SISD        | SIMD          | | multiple instruction | MISD        | MIMD          |","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"SISD(single instruction single data) is the classical uniprocessor model. A single instruction stream executes, acting on a single data element at a time.\nSIMD(single instruction multiple data) incorporates a level of parallelism by having a single instruction stream acting on multiple data elements at a time. An example of this is vectorized CPU instructions which use large registers containing multiple data elements. Instructions that work with these large vector registers effectively work on multiple data elements in parallel with a single instruction by utilizing special hardware.","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"(Image: SIMD AVX-2 4x64i addition)","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"MISD (multiple instruction single data) is currently only a theoretical model and no commercial machine has been built which uses it.\nMIMD (multiple instruction multiple data) is able to manage multiple instruction streams and acts on multiple data elements at the same time. The CPU multithreading model belongs to it. Each processor can work independently using a different instruction stream acting on different data as required.","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"To describe CUDA's parallel model NVIDIA coined the term SIMT (single instruction multiple threads) as an extention to SIMD classification. Just like a SIMD vector packs a fixed number of data elements in a wide register, a GPU packs a fixed number of threads in a single warp. Currently NVIDIA packs 32 threads in a single warp and AMD cards pack 64 threads. For more details refer to NVIDIA's docs here.","category":"page"},{"location":"introduction/01-Introduction.html#)-Memory","page":"Introduction","title":"3) Memory","text":"","category":"section"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"CPU's memory (RAM) and GPU memory are distinct and is called host (CPU) and device (GPU) memory respectively. In Julia we need to explicitly transfer memory to and from GPU memory. The reason for this is that copying memory is an expensive operation with high latency. GPU's use PCIe lanes to transfer memory to and from RAM. Poor usage of memory transfers is detrimental to performace and can easily negate all benefits of using a GPU.","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"Device code referencing CPU memory will result in errors. Host code referencing device memory is produces a warning.","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"arr = CUDA.rand(10);\narr[1]","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"0.9164015f0","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"To disallow scalar operations altogether use the CUDA.allowscalar() function.","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"CUDA.allowscalar(false)\ntry\n    arr[1]\ncatch e\n    println(\"Error Caught: \", e)\nend","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"Error Caught: ErrorException(\"scalar getindex is disallowed\")\n","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"To temporarily allow it in an experssion use the @allowscalar macro. However it is suggested that once your application executes correctly on the GPU, you should disallow scalar indexing and use GPU-friendly array operations instead. Accessing GPU memory in a scalar fashion is extremely detrimental to performance.","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"CUDA.@allowscalar arr[1]","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"0.9164015f0","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"A GPU also has different types of memory such as global memory, texture memory, constant memory which will be discussed later. In general what we call global memory is the GPUs DRAM which can be accessed by all threads and is what will be used most often. Memory transfers between the host and device involve the GPUs global memory.","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"You can check your GPU's memory using CUDA.available_memory() and CUDA.total_memory() which returns the number of bytes.","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"@show CUDA.available_memory()\n@show CUDA.total_memory();\nnothing #hide","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"CUDA.available_memory() = 4099342336\nCUDA.total_memory() = 6370426880\n","category":"page"},{"location":"introduction/01-Introduction.html#)-Kernel","page":"Introduction","title":"4) Kernel","text":"","category":"section"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"When we used the @cuda macro, it compiled the gpu_saxpy! function for execution on the GPU. A GPU has it's own instruction set just like a CPU. The compiled function is called the kernel and is sent to the GPU for execution. Once sent we can either wait for the GPU to complete execution or work on something different while it is executing. This can be done using the blocking option.","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"Although CUDA's native instruction set is proprietary there are other ways to inspect code at various stages of compilation. The reflection page of the documentation should be consulted.","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"As an example consider PTX which resembles low level RISC-ISA like code. PTX is commonly used to inspect code and NVIDIA's PTX docs explains it well.","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"@device_code_ptx @cuda threads=numthreads blocks=numblocks gpu_axpy!(α, gpu_v1, gpu_v2)","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"// PTX CompilerJob of kernel gpu_axpy!(Float64, CUDA.CuDeviceArray{Float32,1,1}, CUDA.CuDeviceArray{Float32,1,1}) for sm_61\n\n//\n// Generated by LLVM NVPTX Back-End\n//\n\n.version 6.0\n.target sm_61\n.address_size 64\n\n\t// .globl\t_Z23julia_gpu_axpyNOT__35497Float6413CuDeviceArrayI7Float32Li1ELi1EES0_IS1_Li1ELi1EE // -- Begin function _Z23julia_gpu_axpyNOT__35497Float6413CuDeviceArrayI7Float32Li1ELi1EES0_IS1_Li1ELi1EE\n.weak .global .align 8 .u64 exception_flag;\n                                        // @_Z23julia_gpu_axpyNOT__35497Float6413CuDeviceArrayI7Float32Li1ELi1EES0_IS1_Li1ELi1EE\n.visible .entry _Z23julia_gpu_axpyNOT__35497Float6413CuDeviceArrayI7Float32Li1ELi1EES0_IS1_Li1ELi1EE(\n\t.param .f64 _Z23julia_gpu_axpyNOT__35497Float6413CuDeviceArrayI7Float32Li1ELi1EES0_IS1_Li1ELi1EE_param_0,\n\t.param .align 8 .b8 _Z23julia_gpu_axpyNOT__35497Float6413CuDeviceArrayI7Float32Li1ELi1EES0_IS1_Li1ELi1EE_param_1[16],\n\t.param .align 8 .b8 _Z23julia_gpu_axpyNOT__35497Float6413CuDeviceArrayI7Float32Li1ELi1EES0_IS1_Li1ELi1EE_param_2[16]\n)\n{\n\t.reg .pred \t%p<2>;\n\t.reg .f32 \t%f<4>;\n\t.reg .b32 \t%r<5>;\n\t.reg .f64 \t%fd<5>;\n\t.reg .b64 \t%rd<15>;\n\n// %bb.0:                               // %top\n\tmov.b64 \t%rd5, _Z23julia_gpu_axpyNOT__35497Float6413CuDeviceArrayI7Float32Li1ELi1EES0_IS1_Li1ELi1EE_param_2;\n\tld.param.u64 \t%rd6, [%rd5];\n\tmov.u32 \t%r2, %ctaid.x;\n\tmov.u32 \t%r3, %ntid.x;\n\tmul.wide.u32 \t%rd3, %r3, %r2;\n\tmov.u32 \t%r1, %tid.x;\n\tadd.s32 \t%r4, %r1, 1;\n\tcvt.u64.u32 \t%rd7, %r4;\n\tadd.s64 \t%rd8, %rd3, %rd7;\n\tsetp.ge.s64 \t%p1, %rd6, %rd8;\n\t@%p1 bra \tLBB0_2;\n\tbra.uni \tLBB0_1;\nLBB0_2:                                 // %L46\n\tld.param.f64 \t%fd1, [_Z23julia_gpu_axpyNOT__35497Float6413CuDeviceArrayI7Float32Li1ELi1EES0_IS1_Li1ELi1EE_param_0];\n\tmov.b64 \t%rd4, _Z23julia_gpu_axpyNOT__35497Float6413CuDeviceArrayI7Float32Li1ELi1EES0_IS1_Li1ELi1EE_param_1;\n\tld.param.u64 \t%rd1, [%rd5+8];\n\tld.param.u64 \t%rd2, [%rd4+8];\n\tcvt.u64.u32 \t%rd9, %r1;\n\tadd.s64 \t%rd10, %rd3, %rd9;\n\tshl.b64 \t%rd11, %rd10, 2;\n\tadd.s64 \t%rd12, %rd11, -4;\n\tadd.s64 \t%rd13, %rd2, %rd12;\n\tld.global.f32 \t%f1, [%rd13+4];\n\tcvt.f64.f32 \t%fd2, %f1;\n\tadd.s64 \t%rd14, %rd1, %rd12;\n\tld.global.f32 \t%f2, [%rd14+4];\n\tcvt.f64.f32 \t%fd3, %f2;\n\tfma.rn.f64 \t%fd4, %fd2, %fd1, %fd3;\n\tcvt.rn.f32.f64 \t%f3, %fd4;\n\tst.global.f32 \t[%rd14+4], %f3;\nLBB0_1:                                 // %L31\n\tret;\n                                        // -- End function\n}\n\n","category":"page"},{"location":"introduction/01-Introduction.html#Measuring-Time","page":"Introduction","title":"Measuring Time","text":"","category":"section"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"Since the primary inspiration for parallel programming is performance it is important to measure it effectively. Because the CPU and GPU can execute asynchronously there some nuance in their profiling. When we launch a CUDA kernel using @cuda after the kernel is launched, control is immediately returned back to the CPU. The CPU can continue executing other code until it's forced to synchronize with the GPU. Certain events like memory transfers and kernel launches can force synchronization. While measuring time and benchmarking we need to force synchronization otherwise we are measuring the time to launch kernels rather than the time it took to execute on the GPU.","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"Two simple ways to force synchronization are to use the CUDA.@sync ex where the CPU is blocked until ex finishes execution. The other is to use CUDA.@time which synchronizes before and after ex. Using CUDA.@sync is advisable when using a benchmarking package like BenchmarkTools.jl. Example @benchmark CUDA.@sync ex","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"Another way is to use CUDA Events which can be used in scenarios where a number of events and their statistics are to be collected.","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"Finally, having a look at NVIDIA's benchmarking tools like Nsight Systems and Nsight Compute can be very helpful in understanding an applications timeline and individual kernel performance. Both of these will be discussed in future tutorials.","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"@time axpy!(α, v1, v2)\n@time parallel_axpy!(α, v1, v2)\n@time @cuda threads=numthreads blocks=numblocks gpu_axpy!(α, gpu_v1, gpu_v2)\nsleep(0.1) ## Wait for the previous function to finish\n@time CUDA.@sync @cuda threads=numthreads blocks=numblocks gpu_axpy!(α, gpu_v1, gpu_v2)\nCUDA.@time @cuda threads=numthreads blocks=numblocks gpu_axpy!(α, gpu_v1, gpu_v2)","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"  0.075701 seconds\n  0.069492 seconds (34 allocations: 4.562 KiB)\n  0.000042 seconds (43 allocations: 1.625 KiB)\n  0.011623 seconds (63 allocations: 2.281 KiB)\n  0.011896 seconds (54 CPU allocations: 1.812 KiB)\n","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"Notice how the time with @time @cuda is much lesser than the @time CUDA.@sync and CUDA.@time counterparts.","category":"page"},{"location":"introduction/01-Introduction.html#GPU-Architecture","page":"Introduction","title":"GPU Architecture","text":"","category":"section"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"A GPU is made up of an array of Streaming Multi-Processors(SM) connected to Global Memory. Each streaming multiprocessor consists of warp schedulers, a register file and functional units like single/double precision ALU, Load-Store units,.etc to execute multiple warps concurrently. Effectively hundreds of threads can be executed concurrently on a single SM. Performance of a GPU scales with the number of SM's it has.","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"(Image: GPU Architecture)","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"(Image: SM Architecture)","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"When a kernel is launched on a GPU we also specify a grid configuration using  the blocks and threads arguments. A grid is composed of \"thread blocks\": a logical collection of threads. The blocks argument defines the block configuration for the grid and the threads argument defines the thread configuration for the thread block.","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"The GPU schedules each thread block to any available SM with sufficient resources. Blocks can be processed in any order by the GPU. Multiple thread blocks may execute on a single SM if sufficient resources are available. As thread blocks complete execution other thread blocks take their place.","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"Each thread block contains a cooperative thread array(CTA) which is specified by the threads argument. Threads which belong to the same CTA can easily communicate and coordinate with each other because they belong to the same SM. They also have access to a shared memory which is much faster than global memory. The maximum size of a CTA is currently 1024 on NVIDIA hardware.","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"A small summary of some of the new terms we came across.","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"thread warp: A set of threads with a fixed size(32). Instructions in a warp are executed together.\nthread block: A logical collection of threads which can communicate and coordinate easily.\ngrid: A logical collection of thread blocks.","category":"page"},{"location":"introduction/01-Introduction.html#Resources-for-learning","page":"Introduction","title":"Resources for learning","text":"","category":"section"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"Most learning material uses the C/C++ flavor of CUDA. However, there aren't any significant differences and most material can easily be translated from C/C++ to Julia. The main objective is to understand the programming model and how a GPU works rather than the syntax.","category":"page"},{"location":"introduction/01-Introduction.html","page":"Introduction","title":"Introduction","text":"CUDA C Programming Guide: A reference to the CUDA platform with details on hardware and CUDA C/C++ features.\nNVIDIA Developer Blog: Contains many educational blogposts such as reduction, Matrix Multiply,.etc\nProgramming Massively Parallel Processors: A Hands-on Approach (David Kirk and Wen-mei Hwu): A formal textbook on parallel programming and GPU programming using CUDA.\nComputer Architecture: A quantitative approach (6th Edition, Chapter 4): Exploring the hardware side of parallel technologies like vector architectures, vectorised SIMD instructions and GPU's.","category":"page"},{"location":"index.html#CUDA-Tutorials","page":"Home","title":"CUDA Tutorials","text":"","category":"section"},{"location":"index.html#Index","page":"Home","title":"Index","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"Pages = map(file -> joinpath(\"introduction\", file), readdir(\"introduction\"))","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"EditURL = \"<unknown>/../src/tutorials/introduction/02-Mandelbrot_Set.jl\"","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html#Mandelbrot-Set","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"","category":"section"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"In the last tutorial we got a brief overview of the architecture of a GPU. In this tutorial We will elborate a bit more on Grid configuration and compute the mandelbrot set on the GPU.","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html#Grid-configuration","page":"Mandelbrot Set","title":"Grid configuration","text":"","category":"section"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"When we launch a CUDA kernel we use the syntax @cuda blocks=x threads=y f(a, b, c...) where f is the function and a, b, c... are its arguments.","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"The blocks and threads options to @cuda are used to specify the grid configuration of the kernel which is being launched.","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"blocks specifies the dimension and the size of the grid of blocks. This can be one, two or three dimensional. The reason for having multiple dimensions is to make it easier to express algorithms which index over two or three dimensional spaces.","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"threads specifies the cooperative thread array (CTA). Threads in the same CTA have access to better coordination and communication utilities. CTA's can also two or three dimensional for convenient indexing.","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"There are restrictions related to the grid given below.","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"Maximum x-dimension of a grid of thread blocks : 2^31 - 1\nMaximum y-, or z-dimension of a grid of thread blocks : 65535 (2^16 - 1)\nMaximum x- or y-dimension of a CTA: 1024\nMaximum z-dimension of a CTA: 64\nMaximum number of threads per block: 1024","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"Now let's go back to our SAXPY example and verify the flexibility in choosing grid configurations.","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"using CUDA, BenchmarkTools\n\nfunction gpu_axpy!(A, X, Y)\n    # set tid to thread rank\n    tid = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n    tid > length(X) && return\n    Y[tid] = A*X[tid] + Y[tid]\n    return\nend\n\nN = 2^20\ngpu_v1 = CUDA.rand(N)\ngpu_v2 = CUDA.rand(N)\ngpu_v3 = copy(gpu_v2)\n\nα = 0.48\ngpu_v2 .+= α * gpu_v1\n\nfunction verify_grid(args, result, numthreads, numblocks = cld(N, numthreads))\n    u = copy(args[3])\n\n    @cuda threads=numthreads blocks=numblocks gpu_axpy!(args...)\n    println(\"Explicit kernel launch with threads=$numthreads and blocks=$numblocks is correct: \"\n    ,result == args[3])\n\n    args[3] = u\n    return\nend\n\nargs = [α, gpu_v1, gpu_v3]\n\nverify_grid(args, gpu_v2, 1024)\nverify_grid(args, gpu_v2, 1)\nverify_grid(args, gpu_v2, 33)","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"Explicit kernel launch with threads=1024 and blocks=1024 is correct: true\nExplicit kernel launch with threads=1 and blocks=1048576 is correct: true\nExplicit kernel launch with threads=33 and blocks=31776 is correct: true\n","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html#Occupancy","page":"Mandelbrot Set","title":"Occupancy","text":"","category":"section"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"The above exercise shows the flexibility in deciding grid configuration. However this raises an important question of how the configuration affects performance. The best way to determine is to actually measure what works best in a given scenario however that may prove to be cumbersome for most workflows. Expecially while developing an application it may not be worth our time to find the optimal configuration.","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"While each SM(streaming multiprocessor) might be executing 100's of threads from the perspective of the programmer, the GPU Hardware deals with warps. Each warp is a set of fixed number of threads(32 on NVIDIA hardware). Scheduling and issuing instructions is done at a per warp basis rather than a per thread basis. There is atleast one warp scheduler inside each SM whose job it is to keep the SM as busy as possible. Switching between warps(context switch) is extremely fast and essential to hide latencies such as global memory access. Whenever a warp stalls another warp is immediately switched to.","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"Coming back to out original question of how to determine the optimal grid configuration. One possible solution is to use the occupancy heurestic. mboxoccupancy = fracmboxactive warpsmboxmaximum number of active warps. Since each SM has a finite amount of resources. As the number of resources per thread increases, fewer of them can be concurrently executed. Occupancy can limited by register usage, shared memory and block size.","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"The launch_configuration function analyses the kernel's resource usage and suggests a configuration that maximises occupancy.","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"@show kernel_args = cudaconvert((α, gpu_v1, gpu_v3)) # Convert to GPU friendly types\n@show kernel_tt = Tuple{Core.Typeof.(kernel_args)...}\nkernel = cufunction(gpu_axpy!, kernel_tt)\nkernel_config = launch_configuration(kernel.fun, shmem = 0)","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"(blocks = 20, threads = 1024)","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"For our example the configurator returns 20 blocks and 1024 threads. The occupancy API does not understand what our kernel is doing, it can only see the input types and the function definition. It's our job to figure out if the suggested configuration will work or not. It's best to keep the suggested block size in account while deciding the launch config.","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"For this example it's perhaps best to set the block size to 1024 and determine the grid size based on that.","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html#Mandelbrot-Set-2","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"","category":"section"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"A popular example of mathematical visualization is the Mandelbrot set. Mathematically it is defined as the set of complex numbers c such f_c(z) = z^2 + c is bounded.","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"In other words:","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"‎‎Z_0 = 0‎\nZ_n + 1 = Z_n^2 + c\nc\nis in the mandelbrot set if the value of Z_n is bounded.","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"It can be mathematically shown |Z_n| \\leq 2.0 $ for bounded points.","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"using Images\n\nimg = rand(Bool, 10, 10)\nGray.(img)","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"(Image: )","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"dims = (3000, 3000)\n\nmset = Array{Bool, 2}(undef, dims)\n\nfunction mandelbrot_cpu(mset::AbstractArray, dims, iterations)\n    origin = CartesianIndex(div.(dims, (2, 2), RoundUp))\n    for ind in CartesianIndices(mset)\n        # Compute coordinates for true canvas\n        coordinates = Tuple(ind - origin) ./ 1000.\n        c = ComplexF32(coordinates[1]im + coordinates[2])\n        mset[ind] = mandelbrot(c, iterations)\n    end\nend\n\nfunction mandelbrot(c, iterations)\n    z = ComplexF32(0, 0)\n    for i in 1:iterations\n        z = z^2 + c\n        abs(z) > 2.0 && return false\n    end\n    return true\nend","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"mandelbrot (generic function with 1 method)","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"mandelbrot_cpu(mset, dims, 32)\nGray.(mset)","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"(Image: )","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"This black and white image is no fun. To add color let's map a color to the number iterations it took z to become greater than two.","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"mset_color = Array{UInt8}(undef, dims)\nfunction mandelbrot(c, iterations)\n    z = ComplexF32(0, 0)\n    for i in 1:iterations\n        z = z^2 + c\n        abs2(z) > 4.0 && return i % UInt8\n    end\n    return zero(UInt8)\nend\nmandelbrot_cpu(mset_color, dims, 32)","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"cmap = colormap(\"RdBu\", 32 + 1)","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"33-element Array{RGB{Float64},1} with eltype ColorTypes.RGB{Float64}:\n RGB{Float64}(0.37862450906363854,0.005629812389804824,0.00639579230016143)\n RGB{Float64}(0.511401750140007,0.04652655393466978,0.039614635593812626)\n RGB{Float64}(0.6304694697815748,0.111511303470373,0.09349062211801834)\n RGB{Float64}(0.7341846474081442,0.18142074006590392,0.15064199717871202)\n RGB{Float64}(0.8213788222501716,0.25509282164235364,0.21136287499536927)\n RGB{Float64}(0.8910797965365008,0.33147475283041355,0.276272164392164)\n RGB{Float64}(0.9438288754018904,0.40845200262409787,0.34476905911577005)\n RGB{Float64}(0.9830039965189913,0.48278611060443816,0.4141941306055418)\n RGB{Float64}(1.0,0.5531385993434152,0.4830738846173907)\n RGB{Float64}(1.0,0.6190973834013392,0.5505942126365359)\n RGB{Float64}(1.0,0.6806122944900334,0.6162245746256042)\n RGB{Float64}(1.0,0.7377908937319887,0.6795941045814847)\n RGB{Float64}(1.0,0.7908142485796253,0.7404417566251789)\n RGB{Float64}(1.0,0.8398979258332038,0.7985905981163555)\n RGB{Float64}(1.0,0.8852720670401505,0.8539310704500294)\n RGB{Float64}(1.0,0.9271703845649601,0.9064082739881878)\n RGB{Float64}(0.9749443162453106,0.9729960643600561,0.9760354756310183)\n RGB{Float64}(0.8979042299386503,0.9573644313198612,0.9903024401640127)\n RGB{Float64}(0.8429105655008079,0.9319117355402927,0.9839833167236404)\n RGB{Float64}(0.7850007393219273,0.9035155489426953,0.9768000840962133)\n RGB{Float64}(0.72435893785698,0.8718607481283359,0.9683418382146513)\n RGB{Float64}(0.6612870709584006,0.8366167646970688,0.9580626838319154)\n RGB{Float64}(0.5962365033043193,0.7974464925230237,0.9452500598729462)\n RGB{Float64}(0.5298439087363732,0.7540213709805738,0.9289844201989069)\n RGB{Float64}(0.4629680761727147,0.7060460587259852,0.9080843400563837)\n RGB{Float64}(0.39672180442974636,0.6532979969269443,0.8810251891819273)\n RGB{Float64}(0.3324964124038308,0.5956897491839529,0.8458071789614362)\n RGB{Float64}(0.27201828187372706,0.533364073599101,0.799722253554423)\n RGB{Float64}(0.21739327339452402,0.4668109932720261,0.7391793385823607)\n RGB{Float64}(0.16815701063787106,0.39670390363118646,0.6627783326784157)\n RGB{Float64}(0.12187323443014847,0.32366608303519157,0.5712721112999869)\n RGB{Float64}(0.07682818255970775,0.24837168208180366,0.4656186846834139)\n RGB{Float64}(0.03279600471716905,0.1716696001598839,0.3473117629721944)","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"map(x -> cmap[x + 1], mset_color)","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"(Image: )","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"Our task is to move this computation to the GPU. The tricky part with moving to the GPU is that the idexing gets tricky. We can use 1-D indexing then figure out inside the kernel what our 2-D index is or use 2-D index from the get go.","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"mset_gpu = CuArray{UInt8}(undef, dims)\nfunction mandelbrot_gpu(mset::AbstractArray, dims, iterations)\n    ind = CartesianIndex((blockIdx().y - 1)*blockDim().y + threadIdx().y,\n                        (blockIdx().x - 1)*blockDim().x + threadIdx().x)\n    # Check if index is valid, if not then exit\n    !(ind in CartesianIndices(dims)) && return\n    origin = CartesianIndex(div.(dims, (2, 2), RoundUp))\n\n    # Scale the 3000x3000 image to -1.5 to 1.5\n    coordinates = Tuple(ind - origin) ./ 1000.\n    c = ComplexF32(coordinates[1]im + coordinates[2]) # x + yi\n    mset[ind] = mandelbrot(c, iterations)\n    return\nend","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"mandelbrot_gpu (generic function with 1 method)","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"blkdim = (16, 16)\n@cuda blocks=cld.(dims, blkdim) threads=blkdim mandelbrot_gpu(mset_gpu, dims, 32)","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"# copy back to host and display the same image\nmap(x -> cmap[x + 1], Array(mset_gpu))","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"(Image: )","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html#Thread-Divergence","page":"Mandelbrot Set","title":"Thread Divergence","text":"","category":"section"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"We mentioned in the last tutorial that threads in a warp execute the same instruction. This was not the entire picture as you can guess from the mandelbrot set example. Inside the mandelbrot inner loop we have consecutive threads exiting at different points during iteration.","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"    for i in 1:iterations\n        z = z^2 + c\n        abs2(z) > 4.0 && return i % UInt8\n    end","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"When threads of the same warp are following different execution paths we call it thread divergence. Even when 1 thread out of 32 follows takes a different branch there is thread divergence.","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"For example","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"if threadIdx().x % 2 == 0\n    # Do something\nelse\n    # Do something else\nend","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"In the above example threads with an even index will follow the first path and the odd indexed ones will follow the second path. Inside the GPU when a branching condition is evaluated a 32-bit mask is generated for that warp (1-bit for each lane). All lanes whose corresponding mask bit is true will be active and the remaining lanes will be idle. When execution reaches a convergence point, the mask is inverted so that all lanes which were idle become active and vice versa. This kind of an IF-ELSE branch has 50% efficiency. Even if a single thread had diverged (say condition was threadIdx().x % 32 == 0) we would still be at 50% efficiency. However, nesting IF-ELSE will further reduce efficiency.","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"In our mandelbrot example while we definitely had a lot of thread divergence it was still beneficial because threads in a warp represented physically close pixels which diverge less.","category":"page"},{"location":"introduction/02-Mandelbrot_Set.html","page":"Mandelbrot Set","title":"Mandelbrot Set","text":"Also, note that thread divergence refers to intra-warp divergence rather than inter-warp divergence which does not matter to performance. Because doing work efficiently at the warp level is extremely important for performance we will consider it while writing algorithms. In addition there are a number of functions that work at the warp-level such as sync_warp()and shfl_up_sync(). We will explore these in the reduction and prefix scan tutorials.","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"EditURL = \"<unknown>/../src/tutorials/introduction/04-Reduction.jl\"","category":"page"},{"location":"introduction/04-Reduction.html#Reduction","page":"Reduction","title":"Reduction","text":"","category":"section"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"The reduce function takes in a binary operator ⊕ and a ordered collection, applying the operator to that collection effectively reducing it to one final value.","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"For example, the operator ⊕ can be minimum and the collection can be an array of Integers. ⊕ can also be addition or xor and the collection may represent any type of object as long as the operator makes sense in the context.","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"using CUDA\n\na = rand(10)","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"10-element Array{Float64,1}:\n 0.7230095099881184\n 0.16638704100244883\n 0.9595290619436863\n 0.9624943069172827\n 0.7628593834842048\n 0.293362503794262\n 0.8860580664131226\n 0.6855748945100439\n 0.7001695020731629\n 0.7521785598391935","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"reduce(min, a)","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"0.16638704100244883","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"reduce(*, a)","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"0.007954449625853871","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"@doc reduce(*, a)","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"reduce(op, itr; [init])","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"Reduce the given collection itr with the given binary operator op. If provided, the initial value init must be a neutral element for op that will be returned for empty collections. It is unspecified whether init is used for non-empty collections.","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"For empty collections, providing init will be necessary, except for some special cases (e.g. when op is one of +, *, max, min, &, |) when Julia can determine the neutral element of op.","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"Reductions for certain commonly-used operators may have special implementations, and should be used instead: maximum(itr), minimum(itr), sum(itr), prod(itr),  any(itr), all(itr).","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"The associativity of the reduction is implementation dependent. This means that you can't use non-associative operations like - because it is undefined whether reduce(-,[1,2,3]) should be evaluated as (1-2)-3 or 1-(2-3). Use foldl or foldr instead for guaranteed left or right associativity.","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"Some operations accumulate error. Parallelism will be easier if the reduction can be executed in groups. Future versions of Julia might change the algorithm. Note that the elements are not reordered if you use an ordered collection.","category":"page"},{"location":"introduction/04-Reduction.html#Examples","page":"Reduction","title":"Examples","text":"","category":"section"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"julia> reduce(*, [2; 3; 4])\n24\n\njulia> reduce(*, [2; 3; 4]; init=-1)\n-24","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"Writing reduce for a CPU is quite straightforward with a single for-loop. We will focus on writing a reduction for a linear array in this tutorial. We will iteratively develop a performant version using everything we have learnt in the previous tutorials.","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"","category":"page"},{"location":"introduction/04-Reduction.html#Reduction-1-:-Divide-and-Conquer","page":"Reduction","title":"Reduction 1 : Divide and Conquer","text":"","category":"section"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"The first step is to write something that works on the GPU. If we were given a dual-core machine and expected to parallelize this we would split the input array into two halves and feed each half into a different CPU. Similarly the best hint is to use the divide-and-conquer approach. By envisioning the reduction process as a binary tree we get:","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"(Image: reduction-1)","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"It would be a good exercise to try to write model the above process in pseudocode.","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"One such approach is:","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"The only issue with this approach on the GPU is that after each step we need to synchronize which won't be possible with arrays which span over a single thread block (1024 threads is the maximum threads in a block). Hence, we will have to use a recursive approach.","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"Assume for now we have 1024 threads per block and process one element per thread. If we have 2048 threads then we will run our algorithm with two thread blocks, storing the results in an intermediate array. After our first kernel is done we will perform a reduction on the intermediate array. And if we have an array whose length is greater than 1024^2 we will have another level of recursion. If there are 1024*1024 + 1 elements then the 1^st level of reduction will return an intermediate array of size 1025 which will take another level of recursion to process.","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"function reduction1(op, a::CuArray)\n    threadsPerBlock = 1024\n    len = length(a)\n\n    sums = similar(a, cld(len, threadsPerBlock))\n\n    blocks = cld(len, threadsPerBlock)\n    shmem = sizeof(eltype(a))*threadsPerBlock\n    @cuda shmem=shmem threads=threadsPerBlock blocks=blocks reduction1_kernel(op, a, sums)\n\n    # Recursively call reduction for larger arrays\n    if length(sums) > 1\n        return reduction1(op, sums)[1]\n    end\n\n    CUDA.@allowscalar return sums[1]\nend","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"reduction1 (generic function with 1 method)","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"function reduction1_kernel(op, a, sums)\n    shmem = @cuDynamicSharedMem(eltype(a), (blockDim().x, ))\n    tid = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n    index = threadIdx().x\n    len = blockDim().x\n\n    # Adjust length for the last block\n    if blockIdx().x == gridDim().x\n        len = mod1(length(a), blockDim().x)\n    end\n\n    if tid <= length(a)\n        shmem[index] = a[tid]\n    else\n        return\n    end\n    sync_threads()\n\n    steps = floor(Int, CUDA.log2(convert(Float32, len)))\n    for i = 0:steps\n        if mod(index - 1, 2^(i + 1)) == 0 && (index + 2^i) <= len\n            shmem[index] = op(shmem[index], shmem[index + 2^i])\n        end\n        sync_threads()\n    end\n\n    if index == 1\n        sums[blockIdx().x] = shmem[1]\n    end\n    return\nend","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"reduction1_kernel (generic function with 1 method)","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"a = CUDA.ones(1025);\nreduction1(+, a)","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"1025.0f0","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"a = CUDA.rand(100_000);\nprintln(reduction1(+, a),\"  \",reduce(+, a))","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"49962.688  49962.68\n","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"The two results above not being exactly equal is expected since IEEE floats are neither associative nor commutative. Since associativity is tough to achieve on a parallel algorithm we can expect some deviation.","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"","category":"page"},{"location":"introduction/04-Reduction.html#Reduction-2-:-Strided-Index","page":"Reduction","title":"Reduction 2 : Strided Index","text":"","category":"section"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"One problem with the last reduction was divergent branching, for example thread three is active for exactly one computation (a[3] op a[4]) and is never used again. Because GPU's operate warpwise we want to use all the resources of a warp instead of a small subset. When threads in a warp do different things it has diverged and it's efficiency drops. In this case with each iteration half the number of threads go inactive.","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"A simple fix is to change the way threads map to the elements by using a strided index","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"(Image: reduction-2)","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"function reduction2_kernel(op, a, sums)\n    shmem = @cuDynamicSharedMem(eltype(a), (blockDim().x, ))\n    tid = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n    index = threadIdx().x\n    len = blockDim().x\n\n    # Adjust length for the last block\n    if blockIdx().x == gridDim().x\n        len = mod1(length(a), blockDim().x)\n    end\n\n    if tid <= length(a)\n        shmem[index] = a[tid]\n    else\n        return\n    end\n    sync_threads()\n\n    steps = floor(Int, CUDA.log2(convert(Float32, len)))\n    for i = 0:steps\n        stride = 2^i\n        sindex = 2*stride*(index - 1) + 1\n        if sindex + stride <= len\n            shmem[sindex] = op(shmem[sindex], shmem[sindex + stride])\n        end\n        sync_threads()\n    end\n\n    if index == 1\n        sums[blockIdx().x] = shmem[1]\n    end\n    return\nend","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"reduction2_kernel (generic function with 1 method)","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"function reduction2(op, a::CuArray)\n    threadsPerBlock = 1024\n    len = length(a)\n\n    sums = similar(a, cld(len, threadsPerBlock))\n\n    blocks = cld(len, threadsPerBlock)\n    shmem = sizeof(eltype(a))*threadsPerBlock\n    @cuda shmem=shmem threads=threadsPerBlock blocks=blocks reduction2_kernel(op, a, sums)\n\n    # Recursively call reduction for larger arrays\n    if length(sums) > 1\n        return reduction2(op, sums)\n    end\n\n    CUDA.@allowscalar return sums[1]\nend","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"reduction2 (generic function with 1 method)","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"a = CUDA.ones(100_000);\nreduction2(+, a)","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"100000.0f0","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"@time CUDA.@sync reduction1(+, a);\n@time CUDA.@sync reduction2(+, a);\nnothing #hide","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"  0.000451 seconds (73 allocations: 3.406 KiB)\n  0.000281 seconds (72 allocations: 3.031 KiB)\n","category":"page"},{"location":"introduction/04-Reduction.html#Reduction-3-:-Sequential-access","page":"Reduction","title":"Reduction 3 : Sequential access","text":"","category":"section"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"In both the above implementations our memory-access pattern is strided which is difficult to coalesce. We discussed coalesced memory access in the Shared Memory tutorial.","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"TL;DR When consecutive threads access consecutive locations in memory, the GPU combines several transactions into a fewer transactions which is called coalesced memory access. When memory accesses are not consecutive which happens when the locations are non-sequantial, sparse or misaligned the GPU hardware is unable to reduce the number of transactions. Since transactions are serviced sequentially there is a significant performance penalty for non-coalesced access.","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"To make use of sequantial access instead of stride iterating from 1 to length ÷ 2 we can do it the other way around (length ÷ 2:1)","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"NOTE: The algorithm below assumes that the blockDim is a power of two. Transforming it to become friendly with non-power of two can be done as an exercise.","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"function reduction3_kernel(op, a, sums)\n    shmem = @cuDynamicSharedMem(eltype(a), (blockDim().x, ))\n    tid = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n    index = threadIdx().x\n    len = blockDim().x\n\n    # Adjust length for the last block\n    if blockIdx().x == gridDim().x\n        len = mod1(length(a), blockDim().x)\n    end\n\n    if tid <= length(a)\n        @inbounds shmem[index] = a[tid]\n    else\n        return\n    end\n    sync_threads()\n\n    stride = len ÷ 2\n    while stride > 0\n        if index <= stride && index + stride <= len\n            shmem[index] = op(shmem[index], shmem[index + stride])\n        end\n        stride = stride ÷ 2\n        sync_threads()\n    end\n\n    if index == 1\n        @inbounds sums[blockIdx().x] = shmem[1]\n    end\n\n    return\nend","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"reduction3_kernel (generic function with 1 method)","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"function reduction3(op, a::CuArray)\n    threadsPerBlock = 1024\n    len = length(a)\n\n    sums = similar(a, cld(len, threadsPerBlock))\n\n    blocks = cld(len, threadsPerBlock)\n    shmem = sizeof(eltype(a))*threadsPerBlock\n    @cuda shmem=shmem threads=threadsPerBlock blocks=blocks reduction3_kernel(op, a, sums)\n\n    # Recursively call reduction for larger arrays\n    if length(sums) > 1\n        return reduction3(op, sums)[1]\n    end\n\n    CUDA.@allowscalar return sums[1]\n    return sums\nend","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"reduction3 (generic function with 1 method)","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"a = CUDA.ones(1024);\nreduction3(+, a)","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"1024.0f0","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"a = CUDA.ones(Int, 1024 * 1024);\nreduction3(+, a)","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"1048576","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"CUDA.@time reduction2(+, a);\nCUDA.@time reduction3(+, a);\nnothing #hide","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"  0.806616 seconds (771.02 k CPU allocations: 38.272 MiB, 0.88% gc time) (2 GPU allocations: 8.008 KiB, 0.01% gc time of which 46.73% spent allocating)\n  0.000311 seconds (68 CPU allocations: 2.922 KiB) (2 GPU allocations: 8.008 KiB, 7.17% gc time of which 71.49% spent allocating)\n","category":"page"},{"location":"introduction/04-Reduction.html#Reduction-4-:-Warp-Shuffle","page":"Reduction","title":"Reduction 4 : Warp Shuffle","text":"","category":"section"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"A powerful feature in modern GPUs is the ability to communicate within warps with the help of special instructions. Currently we transfer data with the help of shared memory which obviously requires sync_threads() and access to shared memory. Warp shuffle functions allow transferring memory within a warp without the use of shared memory also being much faster and not requiring any explicit barrier. The only drawback is that only the following primitive types are supported: Int32, UInt32, Int64, UInt64, Float32, Float64 and any arbitrary source to destination lane mapping is not permitted.","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"There are four shuffle methods.","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"shfl_sync\nshfl_up_sync\nshfl_down_sync\nshfl_xor_sync","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"@doc shfl_sync","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"shfl_sync(threadmask::UInt32, val, lane::Integer, width::Integer=32)","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"Shuffle a value from a directly indexed lane lane, and synchronize threads according to threadmask.","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"shfl_sync acts as a broadcast transferring a lane's value to all other lane.","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"function broadcast_gpu(lane)\n    id = threadIdx().x\n    val = id\n    mask = typemax(UInt32) # 0xffffffff\n    newval = shfl_sync(mask, val, lane)\n    @cuprint(\"id: \", id, \"\\t value: \", val, \"\\t new value: \", newval, \"\\n\")\n    return\nend\n\n@cuda threads=32 blocks=1 broadcast_gpu(19)","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"shfl_up_sync and shfl_down_sync copy the value from lane = current_lane ± delta. If lane is out of bounds from the warp then","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"@doc shfl_down_sync","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"shfl_down_sync(threadmask::UInt32, val, delta::Integer, width::Integer=32)","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"Shuffle a value from a lane with higher ID relative to caller, and synchronize threads according to threadmask.","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"function shfldown_gpu(delta)\n    id = threadIdx().x\n    val = id\n    mask = typemax(UInt32) # 0xffffffff\n    newval = shfl_down_sync(mask, val, delta)\n    @cuprint(\"id: \", id, \"\\t old value: \", val, \"\\t new value: \", newval, \"\\n\")\n    return\nend\n\n@cuda threads=32 blocks=1 shfldown_gpu(2)\nsynchronize()","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"id: 1\t value: 1\t new value: 19\nid: 2\t value: 2\t new value: 19\nid: 3\t value: 3\t new value: 19\nid: 4\t value: 4\t new value: 19\nid: 5\t value: 5\t new value: 19\nid: 6\t value: 6\t new value: 19\nid: 7\t value: 7\t new value: 19\nid: 8\t value: 8\t new value: 19\nid: 9\t value: 9\t new value: 19\nid: 10\t value: 10\t new value: 19\nid: 11\t value: 11\t new value: 19\nid: 12\t value: 12\t new value: 19\nid: 13\t value: 13\t new value: 19\nid: 14\t value: 14\t new value: 19\nid: 15\t value: 15\t new value: 19\nid: 16\t value: 16\t new value: 19\nid: 17\t value: 17\t new value: 19\nid: 18\t value: 18\t new value: 19\nid: 19\t value: 19\t new value: 19\nid: 20\t value: 20\t new value: 19\nid: 21\t value: 21\t new value: 19\nid: 22\t value: 22\t new value: 19\nid: 23\t value: 23\t new value: 19\nid: 24\t value: 24\t new value: 19\nid: 25\t value: 25\t new value: 19\nid: 26\t value: 26\t new value: 19\nid: 27\t value: 27\t new value: 19\nid: 28\t value: 28\t new value: 19\nid: 29\t value: 29\t new value: 19\nid: 30\t value: 30\t new value: 19\nid: 31\t value: 31\t new value: 19\nid: 32\t value: 32\t new value: 19\nid: 1\t old value: 1\t new value: 3\nid: 2\t old value: 2\t new value: 4\nid: 3\t old value: 3\t new value: 5\nid: 4\t old value: 4\t new value: 6\nid: 5\t old value: 5\t new value: 7\nid: 6\t old value: 6\t new value: 8\nid: 7\t old value: 7\t new value: 9\nid: 8\t old value: 8\t new value: 10\nid: 9\t old value: 9\t new value: 11\nid: 10\t old value: 10\t new value: 12\nid: 11\t old value: 11\t new value: 13\nid: 12\t old value: 12\t new value: 14\nid: 13\t old value: 13\t new value: 15\nid: 14\t old value: 14\t new value: 16\nid: 15\t old value: 15\t new value: 17\nid: 16\t old value: 16\t new value: 18\nid: 17\t old value: 17\t new value: 19\nid: 18\t old value: 18\t new value: 20\nid: 19\t old value: 19\t new value: 21\nid: 20\t old value: 20\t new value: 22\nid: 21\t old value: 21\t new value: 23\nid: 22\t old value: 22\t new value: 24\nid: 23\t old value: 23\t new value: 25\nid: 24\t old value: 24\t new value: 26\nid: 25\t old value: 25\t new value: 27\nid: 26\t old value: 26\t new value: 28\nid: 27\t old value: 27\t new value: 29\nid: 28\t old value: 28\t new value: 30\nid: 29\t old value: 29\t new value: 31\nid: 30\t old value: 30\t new value: 32\nid: 31\t old value: 31\t new value: 31\nid: 32\t old value: 32\t new value: 32\n","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"We can use shfl_down_sync to reduce a warp much faster than shared memory.","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"@inline function reducewarp(op, val, mask = typemax(UInt32))\n    val = op(val, shfl_down_sync(mask, val, 1))\n    val = op(val, shfl_down_sync(mask, val, 2))\n    val = op(val, shfl_down_sync(mask, val, 4))\n    val = op(val, shfl_down_sync(mask, val, 8))\n    val = op(val, shfl_down_sync(mask, val, 16))\n    return val\nend\n\nfunction reduction4_kernel(op, a, sums)\n    tid = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n    lane_id = tid % 32\n    warp_id = cld(tid, 32)\n\n    # exit\n    tid > length(a) && return\n\n    # set essential\n    tid <= length(a) && (val = a[tid])\n\n    val = reducewarp(op, val)\n    lane_id == 1 && (sums[warp_id] = val)\n    return\nend\n\nfunction reduction4(op, a::CuArray)\n    threadsPerBlock = 1024\n    len = length(a)\n\n    sums = similar(a, cld(len, 32))\n\n    blocks = cld(len, threadsPerBlock)\n    shmem = sizeof(eltype(a))*threadsPerBlock\n    @cuda threads=threadsPerBlock blocks=blocks reduction4_kernel(op, a, sums)\n\n    # Recursively call reduction for larger arrays\n    if length(sums) > 1\n        return reduction4(op, sums)\n    end\n\n    CUDA.@allowscalar return sums[1]\n    return sums\nend","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"reduction4 (generic function with 1 method)","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"a = CUDA.ones(Int, 320_000)\nreduction4(+, a)","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"320000","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"There is one big problem with our implementation, the input length is expected to be a multiple of 32. This problem can be solved either by defining a neutral element for op (zero(eltype(a)) for +, one(eltype(a)) for *) however this won't work with xor. Another is to force the last warp's computation via shared memory like earlier examples. We correct this by having only 1 thread work for the last warp; this is a simple solution that is inefficient but when the number of warps is large the performance hit shouldn't be too high.","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"function reduction5_kernel(op, a, sums)\n    tid = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n    lane_id = tid % 32\n    warp_id = cld(tid, 32)\n\n    # exit non essential\n    tid > cld(length(a), 32)*32 && return\n\n    # set essential\n    tid <= length(a) && (val = a[tid])\n\n    # Manage last warp\n    if warp_id*32 > length(a)\n        if lane_id == 1\n            for i=(tid + 1):length(a)\n                val = op(val, a[i])\n            end\n        end\n    else\n        val = reducewarp(op, val)\n    end\n\n    lane_id == 1 && (sums[warp_id] = val)\n    return\nend\n\nfunction reduction5(op, a::CuArray)\n    threadsPerBlock = 1024\n    len = length(a)\n\n    sums = similar(a, cld(len, 32))\n\n    blocks = cld(len, threadsPerBlock)\n    shmem = sizeof(eltype(a))*threadsPerBlock\n    @cuda threads=threadsPerBlock blocks=blocks reduction5_kernel(op, a, sums)\n\n    # Recursively call reduction for larger arrays\n    if length(sums) > 1\n        return reduction5(op, sums)\n    end\n\n    CUDA.@allowscalar return sums[1]\n    return sums\nend","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"reduction5 (generic function with 1 method)","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"a = CUDA.ones(Int, 5_000_000)\nreduction5(+, a)","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"5000000","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"@time CUDA.@sync reduction3(+, a);\n@time CUDA.@sync reduction5(+, a);\nnothing #hide","category":"page"},{"location":"introduction/04-Reduction.html","page":"Reduction","title":"Reduction","text":"  0.001485 seconds (101 allocations: 4.344 KiB)\n  0.000761 seconds (171 allocations: 6.984 KiB)\n","category":"page"}]
}
